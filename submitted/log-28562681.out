Looking in indexes: https://pypi.org/simple, http://webservice
Collecting transformers==4.38.1
  Using cached transformers-4.38.1-py3-none-any.whl.metadata (131 kB)
Collecting filelock (from transformers==4.38.1)
  Using cached filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)
Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.38.1)
  Using cached huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)
Collecting numpy>=1.17 (from transformers==4.38.1)
  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)
Collecting packaging>=20.0 (from transformers==4.38.1)
  Using cached packaging-24.0-py3-none-any.whl.metadata (3.2 kB)
Collecting pyyaml>=5.1 (from transformers==4.38.1)
  Using cached PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
Collecting regex!=2019.12.17 (from transformers==4.38.1)
  Using cached regex-2024.5.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)
Collecting requests (from transformers==4.38.1)
  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)
Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.1)
  Using cached tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
Collecting safetensors>=0.4.1 (from transformers==4.38.1)
  Using cached safetensors-0.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)
Collecting tqdm>=4.27 (from transformers==4.38.1)
  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)
Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.1)
  Using cached fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)
Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.1)
  Using cached typing_extensions-4.12.1-py3-none-any.whl.metadata (3.0 kB)
Collecting charset-normalizer<4,>=2 (from requests->transformers==4.38.1)
  Using cached charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)
Collecting idna<4,>=2.5 (from requests->transformers==4.38.1)
  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)
Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.38.1)
  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)
Collecting certifi>=2017.4.17 (from requests->transformers==4.38.1)
  Using cached certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)
Using cached transformers-4.38.1-py3-none-any.whl (8.5 MB)
Using cached huggingface_hub-0.23.2-py3-none-any.whl (401 kB)
Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)
Using cached packaging-24.0-py3-none-any.whl (53 kB)
Using cached PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (724 kB)
Using cached regex-2024.5.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (788 kB)
Using cached safetensors-0.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)
Using cached tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)
Using cached filelock-3.14.0-py3-none-any.whl (12 kB)
Using cached requests-2.32.3-py3-none-any.whl (64 kB)
Using cached certifi-2024.6.2-py3-none-any.whl (164 kB)
Using cached charset_normalizer-3.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)
Using cached fsspec-2024.6.0-py3-none-any.whl (176 kB)
Using cached idna-3.7-py3-none-any.whl (66 kB)
Using cached typing_extensions-4.12.1-py3-none-any.whl (37 kB)
Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)
Installing collected packages: urllib3, typing-extensions, tqdm, safetensors, regex, pyyaml, packaging, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers
  Attempting uninstall: urllib3
    Found existing installation: urllib3 2.2.1
    Uninstalling urllib3-2.2.1:
      Successfully uninstalled urllib3-2.2.1
  Attempting uninstall: typing-extensions
    Found existing installation: typing_extensions 4.12.1
    Uninstalling typing_extensions-4.12.1:
      Successfully uninstalled typing_extensions-4.12.1
  Attempting uninstall: tqdm
    Found existing installation: tqdm 4.66.4
    Uninstalling tqdm-4.66.4:
      Successfully uninstalled tqdm-4.66.4
  Attempting uninstall: safetensors
    Found existing installation: safetensors 0.4.3
    Uninstalling safetensors-0.4.3:
      Successfully uninstalled safetensors-0.4.3
  Attempting uninstall: regex
    Found existing installation: regex 2024.5.15
    Uninstalling regex-2024.5.15:
      Successfully uninstalled regex-2024.5.15
  Attempting uninstall: pyyaml
    Found existing installation: PyYAML 6.0.1
    Uninstalling PyYAML-6.0.1:
      Successfully uninstalled PyYAML-6.0.1
  Attempting uninstall: packaging
    Found existing installation: packaging 24.0
    Uninstalling packaging-24.0:
      Successfully uninstalled packaging-24.0
  Attempting uninstall: numpy
    Found existing installation: numpy 1.26.4
    Uninstalling numpy-1.26.4:
      Successfully uninstalled numpy-1.26.4
  Attempting uninstall: idna
    Found existing installation: idna 3.7
    Uninstalling idna-3.7:
      Successfully uninstalled idna-3.7
  Attempting uninstall: fsspec
    Found existing installation: fsspec 2024.6.0
    Uninstalling fsspec-2024.6.0:
      Successfully uninstalled fsspec-2024.6.0
  Attempting uninstall: filelock
    Found existing installation: filelock 3.14.0
    Uninstalling filelock-3.14.0:
      Successfully uninstalled filelock-3.14.0
  Attempting uninstall: charset-normalizer
    Found existing installation: charset-normalizer 3.3.2
    Uninstalling charset-normalizer-3.3.2:
      Successfully uninstalled charset-normalizer-3.3.2
  Attempting uninstall: certifi
    Found existing installation: certifi 2024.6.2
    Uninstalling certifi-2024.6.2:
      Successfully uninstalled certifi-2024.6.2
  Attempting uninstall: requests
    Found existing installation: requests 2.32.3
    Uninstalling requests-2.32.3:
      Successfully uninstalled requests-2.32.3
  Attempting uninstall: huggingface-hub
    Found existing installation: huggingface-hub 0.23.2
    Uninstalling huggingface-hub-0.23.2:
      Successfully uninstalled huggingface-hub-0.23.2
  Attempting uninstall: tokenizers
    Found existing installation: tokenizers 0.15.2
    Uninstalling tokenizers-0.15.2:
      Successfully uninstalled tokenizers-0.15.2
  Attempting uninstall: transformers
    Found existing installation: transformers 4.38.1
    Uninstalling transformers-4.38.1:
      Successfully uninstalled transformers-4.38.1
Successfully installed certifi-2024.6.2 charset-normalizer-3.3.2 filelock-3.14.0 fsspec-2024.6.0 huggingface-hub-0.23.2 idna-3.7 numpy-1.26.4 packaging-24.0 pyyaml-6.0.1 regex-2024.5.15 requests-2.32.3 safetensors-0.4.3 tokenizers-0.15.2 tqdm-4.66.4 transformers-4.38.1 typing-extensions-4.12.1 urllib3-2.2.1
M	__pycache__/cache.cpython-312.pyc
M	__pycache__/xevaluator.cpython-312.pyc
M	__pycache__/xhuggingface.cpython-312.pyc
Your branch is up to date with 'origin/yangexp2'.
Already up to date.
/private/home/beidic/.conda/envs/griffin/bin/python
[1m[31mERROR! `huggingface-cli login` uses an outdated login mechanism that is not compatible with the Hugging Face Hub backend anymore. Please use `huggingface-cli login instead.[0m
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-chat-hf,griffin=False,check=False', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-chat-hf,griffin=False,check=False', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-chat-hf,griffin=False,check=False', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-chat-hf,griffin=False,check=False', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-chat-hf,griffin=False,check=False', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-chat-hf,griffin=False,check=False', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-chat-hf,griffin=False,check=False', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-chat-hf,griffin=False,check=False', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
xhf (pretrained=meta-llama/Llama-2-7b-chat-hf,griffin=False,check=False), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1
|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|
|-----|------:|----------------|-----:|-----------|-----:|---|-----:|
|gsm8k|      3|strict-match    |     5|exact_match|0.2403|±  |0.0118|
|     |       |flexible-extract|     5|exact_match|0.2426|±  |0.0118|

