Already on 'yangexp2'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/huggingface-cli", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py", line 51, in main
    service.run()
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/user.py", line 98, in run
    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 111, in login
    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 307, in _login
    raise ValueError("Invalid token passed!")
ValueError: Invalid token passed!
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 7, in <module>
Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    import evaluate as hf_evaluate
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/evaluate/__init__.py", line 29, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 7, in <module>
    import evaluate as hf_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/evaluate/__init__.py", line 29, in <module>
        from .evaluation_suite import EvaluationSuitefrom .evaluation_suite import EvaluationSuite

  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/evaluate/evaluation_suite/__init__.py", line 7, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/evaluate/evaluation_suite/__init__.py", line 7, in <module>
        from datasets import Dataset, DownloadConfig, DownloadMode, load_datasetfrom datasets import Dataset, DownloadConfig, DownloadMode, load_dataset

  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/datasets/__init__.py", line 18, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/datasets/__init__.py", line 18, in <module>
    from .arrow_dataset import Dataset
    from .arrow_dataset import Dataset
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 63, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/datasets/arrow_dataset.py", line 63, in <module>
    from huggingface_hub import (
      File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/__init__.py", line 503, in __getattr__
from huggingface_hub import (
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/__init__.py", line 503, in __getattr__
    submod = importlib.import_module(submod_path)
    submod = importlib.import_module(submod_path)
                   ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/__init__.py", line 90, in import_module
^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/__init__.py", line 90, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
    return _bootstrap._gcd_import(name[level:], package, level)
                ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/hf_api.py", line 45, in <module>
^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/hf_api.py", line 45, in <module>
    import requests
    import requests
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/requests/__init__.py", line 43, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/requests/__init__.py", line 43, in <module>
    import urllib3
    import urllib3
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/urllib3/__init__.py", line 15, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/urllib3/__init__.py", line 15, in <module>
    from ._base_connection import _TYPE_BODY
    from ._base_connection import _TYPE_BODY
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/urllib3/_base_connection.py", line 5, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/urllib3/_base_connection.py", line 5, in <module>
    from .util.connection import _TYPE_SOCKET_OPTIONS
ModuleNotFoundError: No module named 'urllib3.util'
    from .util.connection import _TYPE_SOCKET_OPTIONS
ModuleNotFoundError: No module named 'urllib3.util'
[2024-06-04 06:39:56,515] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 441549 closing signal SIGTERM
[2024-06-04 06:39:56,516] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 441550 closing signal SIGTERM
[2024-06-04 06:39:56,516] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 441551 closing signal SIGTERM
[2024-06-04 06:39:56,516] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 441552 closing signal SIGTERM
[2024-06-04 06:39:56,516] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 441555 closing signal SIGTERM
[2024-06-04 06:39:56,516] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 441556 closing signal SIGTERM
[2024-06-04 06:39:56,635] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 4 (pid: 441553) of binary: /private/home/beidic/.conda/envs/griffin/bin/python
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-06-04_06:39:56
  host      : learnfair5044.h2.fair
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 441554)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_06:39:56
  host      : learnfair5044.h2.fair
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 441553)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 397, in from_name
    return next(cls.discover(name=name))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 102, in require_version
    got_ver = importlib.metadata.version(pkg)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 888, in version
    return distribution(distribution_name).version
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 861, in distribution
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 397, in from_name
    return Distribution.from_name(distribution_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 399, in from_name
    return next(cls.discover(name=name))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration

    raise PackageNotFoundError(name)During handling of the above exception, another exception occurred:


Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 102, in require_version
importlib.metadata.PackageNotFoundError: No package metadata was found for numpy

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
    got_ver = importlib.metadata.version(pkg)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 888, in version
    return distribution(distribution_name).version
  Traceback (most recent call last):
      File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 397, in from_name
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 861, in distribution
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
    return next(cls.discover(name=name))
           ^^^^^^^^^^^^^^^^^^^^^    ^return Distribution.from_name(distribution_name)^
^^^^^^
StopIteration 
  
  During handling of the above exception, another exception occurred:
  
   Traceback (most recent call last):
 ^^^^  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 102, in require_version
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 399, in from_name
    from .evaluator import evaluate, simple_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    got_ver = importlib.metadata.version(pkg)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    ^raise PackageNotFoundError(name)

  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 888, in version
importlib.metadata.PackageNotFoundError: No package metadata was found for numpy

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
    return distribution(distribution_name).version
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 861, in distribution
    from .evaluator import evaluate, simple_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    return Distribution.from_name(distribution_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 399, in from_name
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
    raise PackageNotFoundError(name)
importlib.metadata.PackageNotFoundError: No package metadata was found for numpy

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    from lm_eval.api.model import LM
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
    from lm_eval.api.model import LM
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
    from lm_eval.api.model import LM
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
    import transformers
    import transformers
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 57, in <module>
    from . import dependency_versions_check
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 57, in <module>
    require_version_core(deps[pkg])
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 117, in require_version_core
    require_version_core(deps[pkg])
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 117, in require_version_core
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 397, in from_name
    return require_version(requirement, hint)
    import transformers
        return require_version(requirement, hint) 
     File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
   ^^^^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 104, in require_version
^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 104, in require_version
    return next(cls.discover(name=name))
    from . import dependency_versions_check
          File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 57, in <module>
   ^^^^^^^^^^    ^raise importlib.metadata.PackageNotFoundError(^
^^^^importlib.metadata^.^PackageNotFoundError^^: ^No package metadata was found for The 'numpy>=1.17' distribution was not found and is required by this application. 
Try: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main^
^^^^^^^
StopIteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
      File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 102, in require_version
raise importlib.metadata.PackageNotFoundError(
importlib.metadata.PackageNotFoundError: No package metadata was found for The 'numpy>=1.17' distribution was not found and is required by this application. 
Try: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main
    require_version_core(deps[pkg])
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 117, in require_version_core
    got_ver = importlib.metadata.version(pkg)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 888, in version
    return require_version(requirement, hint)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 104, in require_version
    return distribution(distribution_name).version
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 861, in distribution
    raise importlib.metadata.PackageNotFoundError(
importlib.metadata.PackageNotFoundError: No package metadata was found for The 'numpy>=1.17' distribution was not found and is required by this application. 
Try: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main
    return Distribution.from_name(distribution_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 399, in from_name
    raise PackageNotFoundError(name)
importlib.metadata.PackageNotFoundError: No package metadata was found for numpy

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
    from lm_eval.api.model import LM
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
    import transformers
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 57, in <module>
    require_version_core(deps[pkg])
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 117, in require_version_core
    return require_version(requirement, hint)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 104, in require_version
    raise importlib.metadata.PackageNotFoundError(
importlib.metadata.PackageNotFoundError: No package metadata was found for The 'numpy>=1.17' distribution was not found and is required by this application. 
Try: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 397, in from_name
    return next(cls.discover(name=name))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 102, in require_version
    got_ver = importlib.metadata.version(pkg)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 888, in version
    return distribution(distribution_name).version
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 861, in distribution
    return Distribution.from_name(distribution_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 399, in from_name
    raise PackageNotFoundError(name)
importlib.metadata.PackageNotFoundError: No package metadata was found for numpy

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
    from lm_eval.api.model import LM
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
    import transformers
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 57, in <module>
    require_version_core(deps[pkg])
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 117, in require_version_core
    return require_version(requirement, hint)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 104, in require_version
    raise importlib.metadata.PackageNotFoundError(
importlib.metadata.PackageNotFoundError: No package metadata was found for The 'numpy>=1.17' distribution was not found and is required by this application. 
Try: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 397, in from_name
    return next(cls.discover(name=name))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 102, in require_version
    got_ver = importlib.metadata.version(pkg)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 888, in version
    return distribution(distribution_name).version
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 861, in distribution
    return Distribution.from_name(distribution_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 399, in from_name
    raise PackageNotFoundError(name)
importlib.metadata.PackageNotFoundError: No package metadata was found for numpy

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
    from lm_eval.api.model import LM
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
    import transformers
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 57, in <module>
    require_version_core(deps[pkg])
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 117, in require_version_core
    return require_version(requirement, hint)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 104, in require_version
    raise importlib.metadata.PackageNotFoundError(
importlib.metadata.PackageNotFoundError: No package metadata was found for The 'numpy>=1.17' distribution was not found and is required by this application. 
Try: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 397, in from_name
    return next(cls.discover(name=name))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 102, in require_version
    got_ver = importlib.metadata.version(pkg)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 888, in version
    return distribution(distribution_name).version
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 861, in distribution
    return Distribution.from_name(distribution_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 399, in from_name
    raise PackageNotFoundError(name)
importlib.metadata.PackageNotFoundError: No package metadata was found for numpy

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
    from lm_eval.api.model import LM
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
    import transformers
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 57, in <module>
    require_version_core(deps[pkg])
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 117, in require_version_core
    return require_version(requirement, hint)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 104, in require_version
    raise importlib.metadata.PackageNotFoundError(
importlib.metadata.PackageNotFoundError: No package metadata was found for The 'numpy>=1.17' distribution was not found and is required by this application. 
Try: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 397, in from_name
    return next(cls.discover(name=name))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 102, in require_version
    got_ver = importlib.metadata.version(pkg)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 888, in version
    return distribution(distribution_name).version
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 861, in distribution
    return Distribution.from_name(distribution_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/importlib/metadata/__init__.py", line 399, in from_name
    raise PackageNotFoundError(name)
importlib.metadata.PackageNotFoundError: No package metadata was found for numpy

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
    from lm_eval.api.model import LM
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
    import transformers
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 57, in <module>
    require_version_core(deps[pkg])
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 117, in require_version_core
    return require_version(requirement, hint)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/utils/versions.py", line 104, in require_version
    raise importlib.metadata.PackageNotFoundError(
importlib.metadata.PackageNotFoundError: No package metadata was found for The 'numpy>=1.17' distribution was not found and is required by this application. 
Try: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main
[2024-06-04 06:40:11,528] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 441746) of binary: /private/home/beidic/.conda/envs/griffin/bin/python
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-06-04_06:40:11
  host      : learnfair5044.h2.fair
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 441747)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-06-04_06:40:11
  host      : learnfair5044.h2.fair
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 441748)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-06-04_06:40:11
  host      : learnfair5044.h2.fair
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 441749)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-06-04_06:40:11
  host      : learnfair5044.h2.fair
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 441750)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-06-04_06:40:11
  host      : learnfair5044.h2.fair
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 441751)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-06-04_06:40:11
  host      : learnfair5044.h2.fair
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 441752)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-06-04_06:40:11
  host      : learnfair5044.h2.fair
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 441753)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_06:40:11
  host      : learnfair5044.h2.fair
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 441746)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
    from lm_eval import utils
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
    from .evaluator import evaluate, simple_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    import lm_eval.api.metrics
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
        from lm_eval.api.model import LMfrom lm_eval.api.model import LM

  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
    import transformers
    import transformers
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
      File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 16, in <module>
from . import dependency_versions_check
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 16, in <module>
    from .utils.versions import require_version, require_version_core
    from .utils.versions import require_version, require_version_core
ModuleNotFoundError: No module named 'transformers.utils'
ModuleNotFoundError: No module named 'transformers.utils'
Traceback (most recent call last):
Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
    from lm_eval import utils
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
        from .evaluator import evaluate, simple_evaluatefrom .evaluator import evaluate, simple_evaluate

  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
    from lm_eval.api.model import LM
    from lm_eval.api.model import LM
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
    from lm_eval.api.model import LM
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
    import transformers
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
    import transformers
    import transformers
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
    from . import dependency_versions_check  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 16, in <module>

  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 16, in <module>
    from . import dependency_versions_check
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 16, in <module>
    from .utils.versions import require_version, require_version_core
    from .utils.versions import require_version, require_version_core
ModuleNotFoundError    : from .utils.versions import require_version, require_version_coreNo module named 'transformers.utils'

ModuleNotFoundError: No module named 'transformers.utils'
ModuleNotFoundError: No module named 'transformers.utils'
Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
    from lm_eval.api.model import LM
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
    import transformers
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 16, in <module>
    from .utils.versions import require_version, require_version_core
ModuleNotFoundError: No module named 'transformers.utils'
Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
    from lm_eval.api.model import LM
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
    import transformers
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 16, in <module>
    from .utils.versions import require_version, require_version_core
ModuleNotFoundError: No module named 'transformers.utils'
Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 13, in <module>
    from lm_eval import utils
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/evaluator.py", line 11, in <module>
    import lm_eval.api.metrics
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/registry.py", line 6, in <module>
    from lm_eval.api.model import LM
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/api/model.py", line 8, in <module>
    import transformers
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/dependency_versions_check.py", line 16, in <module>
    from .utils.versions import require_version, require_version_core
ModuleNotFoundError: No module named 'transformers.utils'
[2024-06-04 06:40:27,388] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 441976) of binary: /private/home/beidic/.conda/envs/griffin/bin/python
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-06-04_06:40:27
  host      : learnfair5044.h2.fair
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 441977)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-06-04_06:40:27
  host      : learnfair5044.h2.fair
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 441978)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-06-04_06:40:27
  host      : learnfair5044.h2.fair
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 441979)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-06-04_06:40:27
  host      : learnfair5044.h2.fair
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 441980)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-06-04_06:40:27
  host      : learnfair5044.h2.fair
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 441981)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-06-04_06:40:27
  host      : learnfair5044.h2.fair
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 441982)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-06-04_06:40:27
  host      : learnfair5044.h2.fair
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 441983)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_06:40:27
  host      : learnfair5044.h2.fair
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 441976)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:06:40:41,001 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:40:41,002 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:40:41,002 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:40:41,187 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:40:41,198 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:40:41,791 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:40:42,200 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:40:46,919 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:40:46,921 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:40:46,927 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:40:46,927 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:40:47,112 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:40:47,113 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:40:47,117 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:40:47,118 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:40:47,324 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:40:47,326 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:40:47,331 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:40:47,331 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:40:47,463 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:40:47,899 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:40:47,900 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:40:47,904 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:40:47,904 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:40:48,011 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:40:48,012 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:40:48,016 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:40:48,016 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:06:40:52,174 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:40:52,176 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:40:52,182 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:40:52,182 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:06:40:54,222 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:40:54,225 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:40:54,234 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:40:54,234 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:06:40:59,497 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:40:59,499 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:40:59,504 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:40:59,504 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|       | 1/4 [00:16<00:50, 16.96s/it]Loading checkpoint shards:  25%|       | 1/4 [00:30<01:31, 30.63s/it]Loading checkpoint shards:  25%|       | 1/4 [00:23<01:11, 23.84s/it]Loading checkpoint shards:  25%|       | 1/4 [00:30<01:31, 30.61s/it]Loading checkpoint shards:  25%|       | 1/4 [00:30<01:30, 30.27s/it]Loading checkpoint shards:  25%|       | 1/4 [00:30<01:31, 30.63s/it]Loading checkpoint shards:  25%|       | 1/4 [00:30<01:30, 30.30s/it]Loading checkpoint shards:  25%|       | 1/4 [00:26<01:18, 26.09s/it]Loading checkpoint shards:  50%|     | 2/4 [00:58<00:57, 28.93s/it]Loading checkpoint shards:  50%|     | 2/4 [00:59<00:58, 29.39s/it]Loading checkpoint shards:  50%|     | 2/4 [00:59<00:59, 29.60s/it]Loading checkpoint shards:  50%|     | 2/4 [00:54<00:55, 27.50s/it]Loading checkpoint shards:  50%|     | 2/4 [00:59<00:58, 29.46s/it]Loading checkpoint shards:  50%|     | 2/4 [00:52<00:53, 26.82s/it]Loading checkpoint shards:  50%|     | 2/4 [00:59<00:59, 29.62s/it]Loading checkpoint shards:  50%|     | 2/4 [00:47<00:49, 24.72s/it]Loading checkpoint shards:  75%|  | 3/4 [01:25<00:27, 27.98s/it]Loading checkpoint shards:  75%|  | 3/4 [01:26<00:28, 28.54s/it]Loading checkpoint shards:  75%|  | 3/4 [01:26<00:28, 28.44s/it]Loading checkpoint shards:  75%|  | 3/4 [01:26<00:28, 28.68s/it]Loading checkpoint shards:  75%|  | 3/4 [01:21<00:27, 27.43s/it]Loading checkpoint shards:  75%|  | 3/4 [01:26<00:28, 28.53s/it]Loading checkpoint shards:  75%|  | 3/4 [01:14<00:25, 25.71s/it]Loading checkpoint shards:  75%|  | 3/4 [01:20<00:27, 27.26s/it]Loading checkpoint shards: 100%|| 4/4 [01:31<00:00, 19.20s/it]Loading checkpoint shards: 100%|| 4/4 [01:31<00:00, 22.91s/it]
Loading checkpoint shards: 100%|| 4/4 [01:18<00:00, 17.53s/it]Loading checkpoint shards: 100%|| 4/4 [01:18<00:00, 19.74s/it]
Loading checkpoint shards: 100%|| 4/4 [01:31<00:00, 19.26s/it]Loading checkpoint shards: 100%|| 4/4 [01:31<00:00, 22.88s/it]
Loading checkpoint shards: 100%|| 4/4 [01:31<00:00, 19.29s/it]Loading checkpoint shards: 100%|| 4/4 [01:31<00:00, 19.37s/it]Loading checkpoint shards: 100%|| 4/4 [01:31<00:00, 22.89s/it]Loading checkpoint shards: 100%|| 4/4 [01:31<00:00, 22.97s/it]

Loading checkpoint shards: 100%|| 4/4 [01:31<00:00, 19.39s/it]Loading checkpoint shards: 100%|| 4/4 [01:31<00:00, 22.97s/it]
Loading checkpoint shards: 100%|| 4/4 [01:27<00:00, 18.64s/it]Loading checkpoint shards: 100%|| 4/4 [01:27<00:00, 21.77s/it]
Loading checkpoint shards: 100%|| 4/4 [01:25<00:00, 18.52s/it]Loading checkpoint shards: 100%|| 4/4 [01:25<00:00, 21.41s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:43:02,953 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:02,956 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:03,394 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|        | 14/82 [00:00<00:00, 132.05it/s] 34%|      | 28/82 [00:00<00:00, 132.95it/s] 51%|     | 42/82 [00:00<00:00, 133.32it/s] 68%|   | 56/82 [00:00<00:00, 133.53it/s] 85%| | 70/82 [00:00<00:00, 133.49it/s]100%|| 82/82 [00:00<00:00, 133.25it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:43:22,603 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:22,605 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:22,905 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:43:22,982 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:22,984 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 17%|        | 14/82 [00:00<00:00, 137.41it/s] 34%|      | 28/82 [00:00<00:00, 128.64it/s]2024-06-04:06:43:23,193 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 51%|     | 42/82 [00:00<00:00, 132.27it/s] 24%|       | 20/82 [00:00<00:00, 195.83it/s] 68%|   | 56/82 [00:00<00:00, 134.12it/s] 49%|     | 40/82 [00:00<00:00, 197.26it/s] 85%| | 70/82 [00:00<00:00, 135.07it/s] 73%|  | 60/82 [00:00<00:00, 197.63it/s]100%|| 82/82 [00:00<00:00, 134.39it/s]
 98%|| 80/82 [00:00<00:00, 197.76it/s]100%|| 82/82 [00:00<00:00, 197.52it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:43:30,889 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:30,891 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:31,067 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 23%|       | 19/83 [00:00<00:00, 187.34it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:43:31,237 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:31,239 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 46%|     | 38/83 [00:00<00:00, 184.26it/s] 70%|   | 58/83 [00:00<00:00, 187.97it/s]2024-06-04:06:43:31,447 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 95%|| 79/83 [00:00<00:00, 193.53it/s]100%|| 83/83 [00:00<00:00, 191.70it/s]
 25%|       | 21/83 [00:00<00:00, 205.94it/s] 51%|     | 42/83 [00:00<00:00, 207.37it/s] 76%|  | 63/83 [00:00<00:00, 207.83it/s]100%|| 83/83 [00:00<00:00, 207.87it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:43:37,524 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:37,527 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:37,922 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 14%|        | 12/83 [00:00<00:00, 119.36it/s] 30%|       | 25/83 [00:00<00:00, 120.20it/s] 46%|     | 38/83 [00:00<00:00, 120.43it/s] 61%|   | 51/83 [00:00<00:00, 120.26it/s] 77%|  | 64/83 [00:00<00:00, 120.47it/s] 93%|| 77/83 [00:00<00:00, 120.65it/s]100%|| 83/83 [00:00<00:00, 120.46it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:44:12,850 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:44:12,852 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:44:13,250 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 16%|        | 13/82 [00:00<00:00, 123.35it/s] 32%|      | 26/82 [00:00<00:00, 124.61it/s] 48%|     | 39/82 [00:00<00:00, 124.85it/s] 63%|   | 52/82 [00:00<00:00, 125.13it/s] 79%|  | 65/82 [00:00<00:00, 125.16it/s] 95%|| 78/82 [00:00<00:00, 125.09it/s]100%|| 82/82 [00:00<00:00, 124.94it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:06:44:39,762 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:44:39,849 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:44:39,852 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:44:40,352 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]  8%|         | 7/83 [00:00<00:01, 66.20it/s] 17%|        | 14/83 [00:00<00:01, 66.75it/s] 25%|       | 21/83 [00:00<00:00, 67.32it/s] 34%|      | 28/83 [00:00<00:00, 67.18it/s] 42%|     | 35/83 [00:00<00:00, 67.28it/s] 51%|     | 42/83 [00:00<00:00, 67.18it/s] 59%|    | 49/83 [00:00<00:00, 67.12it/s] 67%|   | 56/83 [00:00<00:00, 67.21it/s] 76%|  | 63/83 [00:00<00:00, 67.18it/s] 84%| | 70/83 [00:01<00:00, 67.10it/s] 93%|| 77/83 [00:01<00:00, 67.11it/s]100%|| 83/83 [00:01<00:00, 67.13it/s]
2024-06-04:06:44:52,365 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:44:52,365 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:44:52,365 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:44:52,365 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:44:52,365 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:44:52,365 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:44:52,366 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:44:52,366 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:18<24:44, 18.10s/it]Running generate_until requests:   2%|         | 2/83 [00:35<24:00, 17.79s/it]Running generate_until requests:   4%|         | 3/83 [00:44<18:06, 13.59s/it]Running generate_until requests:   5%|         | 4/83 [00:50<13:57, 10.61s/it]Running generate_until requests:   6%|         | 5/83 [00:58<12:36,  9.70s/it]Running generate_until requests:   7%|         | 6/83 [01:08<12:31,  9.76s/it]Running generate_until requests:   8%|         | 7/83 [01:18<12:21,  9.76s/it]Running generate_until requests:  10%|         | 8/83 [01:25<11:28,  9.18s/it]Running generate_until requests:  11%|         | 9/83 [01:36<11:51,  9.62s/it]Running generate_until requests:  12%|        | 10/83 [01:43<10:34,  8.69s/it]Running generate_until requests:  13%|        | 11/83 [01:53<10:54,  9.09s/it]Running generate_until requests:  14%|        | 12/83 [02:07<12:49, 10.84s/it]Running generate_until requests:  16%|        | 13/83 [02:13<10:36,  9.09s/it]Running generate_until requests:  17%|        | 14/83 [02:19<09:41,  8.43s/it]Running generate_until requests:  18%|        | 15/83 [02:28<09:31,  8.40s/it]Running generate_until requests:  19%|        | 16/83 [02:48<13:14, 11.86s/it]Running generate_until requests:  20%|        | 17/83 [02:58<12:24, 11.28s/it]Running generate_until requests:  22%|       | 18/83 [03:06<11:13, 10.36s/it]Running generate_until requests:  23%|       | 19/83 [03:17<11:20, 10.63s/it]Running generate_until requests:  24%|       | 20/83 [03:31<12:10, 11.60s/it]Running generate_until requests:  25%|       | 21/83 [03:37<10:20, 10.00s/it]Running generate_until requests:  27%|       | 22/83 [03:45<09:36,  9.45s/it]Running generate_until requests:  28%|       | 23/83 [03:57<10:10, 10.18s/it]Running generate_until requests:  29%|       | 24/83 [04:06<09:27,  9.62s/it]Running generate_until requests:  30%|       | 25/83 [04:13<08:43,  9.02s/it]Running generate_until requests:  31%|      | 26/83 [04:21<08:17,  8.73s/it]Running generate_until requests:  33%|      | 27/83 [04:29<07:52,  8.44s/it]Running generate_until requests:  34%|      | 28/83 [04:36<07:24,  8.08s/it]Running generate_until requests:  35%|      | 29/83 [04:47<07:57,  8.84s/it]Running generate_until requests:  36%|      | 30/83 [04:55<07:42,  8.72s/it]Running generate_until requests:  37%|      | 31/83 [05:06<08:04,  9.32s/it]Running generate_until requests:  39%|      | 32/83 [05:18<08:39, 10.19s/it]Running generate_until requests:  40%|      | 33/83 [05:23<07:08,  8.58s/it]Running generate_until requests:  41%|      | 34/83 [05:36<07:57,  9.75s/it]Running generate_until requests:  42%|     | 35/83 [05:43<07:13,  9.02s/it]Running generate_until requests:  43%|     | 36/83 [05:53<07:26,  9.50s/it]Running generate_until requests:  45%|     | 37/83 [06:02<07:02,  9.18s/it]Running generate_until requests:  46%|     | 38/83 [06:14<07:25,  9.90s/it]Running generate_until requests:  47%|     | 39/83 [06:18<06:05,  8.30s/it]Running generate_until requests:  48%|     | 40/83 [06:26<05:54,  8.24s/it]Running generate_until requests:  49%|     | 41/83 [06:35<05:54,  8.43s/it]Running generate_until requests:  51%|     | 42/83 [06:44<05:55,  8.67s/it]Running generate_until requests:  52%|    | 43/83 [06:54<05:58,  8.96s/it]Running generate_until requests:  53%|    | 44/83 [07:01<05:28,  8.42s/it]Running generate_until requests:  54%|    | 45/83 [07:08<05:01,  7.94s/it]Running generate_until requests:  55%|    | 46/83 [07:15<04:44,  7.68s/it]Running generate_until requests:  57%|    | 47/83 [07:24<04:56,  8.23s/it]Running generate_until requests:  58%|    | 48/83 [07:34<05:06,  8.75s/it]Running generate_until requests:  59%|    | 49/83 [07:40<04:23,  7.76s/it]Running generate_until requests:  60%|    | 50/83 [07:46<04:04,  7.40s/it]Running generate_until requests:  61%|   | 51/83 [07:57<04:27,  8.37s/it]Running generate_until requests:  63%|   | 52/83 [08:02<03:45,  7.28s/it]Running generate_until requests:  64%|   | 53/83 [08:11<03:56,  7.90s/it]Running generate_until requests:  65%|   | 54/83 [08:18<03:37,  7.49s/it]Running generate_until requests:  66%|   | 55/83 [08:28<03:53,  8.33s/it]Running generate_until requests:  67%|   | 56/83 [08:40<04:11,  9.30s/it]Running generate_until requests:  69%|   | 57/83 [09:08<06:34, 15.17s/it]Running generate_until requests:  70%|   | 58/83 [09:19<05:43, 13.73s/it]Running generate_until requests:  71%|   | 59/83 [09:25<04:38, 11.60s/it]Running generate_until requests:  72%|  | 60/83 [09:32<03:51, 10.06s/it]Running generate_until requests:  73%|  | 61/83 [09:42<03:43, 10.16s/it]Running generate_until requests:  75%|  | 62/83 [09:52<03:28,  9.93s/it]Running generate_until requests:  76%|  | 63/83 [09:57<02:51,  8.58s/it]Running generate_until requests:  77%|  | 64/83 [10:08<02:55,  9.21s/it]Running generate_until requests:  78%|  | 65/83 [10:15<02:32,  8.48s/it]Running generate_until requests:  80%|  | 66/83 [10:25<02:31,  8.92s/it]Running generate_until requests:  81%|  | 67/83 [10:29<02:01,  7.60s/it]Running generate_until requests:  82%| | 68/83 [10:36<01:51,  7.43s/it]Running generate_until requests:  83%| | 69/83 [10:50<02:12,  9.44s/it]Running generate_until requests:  84%| | 70/83 [10:57<01:52,  8.68s/it]Running generate_until requests:  86%| | 71/83 [11:02<01:28,  7.40s/it]Running generate_until requests:  87%| | 72/83 [11:08<01:19,  7.20s/it]Running generate_until requests:  88%| | 73/83 [11:15<01:10,  7.04s/it]Running generate_until requests:  89%| | 74/83 [11:22<01:02,  6.92s/it]Running generate_until requests:  90%| | 75/83 [11:35<01:11,  8.97s/it]Running generate_until requests:  92%|| 76/83 [11:39<00:52,  7.52s/it]Running generate_until requests:  93%|| 77/83 [11:47<00:45,  7.50s/it]Running generate_until requests:  94%|| 78/83 [11:54<00:37,  7.47s/it]Running generate_until requests:  95%|| 79/83 [12:06<00:34,  8.73s/it]Running generate_until requests:  96%|| 80/83 [12:12<00:24,  8.05s/it]Running generate_until requests:  98%|| 81/83 [12:22<00:16,  8.38s/it]Running generate_until requests:  99%|| 82/83 [12:27<00:07,  7.44s/it]Running generate_until requests: 100%|| 83/83 [12:35<00:00,  7.61s/it]Running generate_until requests: 100%|| 83/83 [12:35<00:00,  9.10s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:07:08:46,411 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:08:46,412 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:08:46,439 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:08:46,444 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:08:46,456 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:08:46,538 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:08:47,733 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:08:47,975 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:08:53,390 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:08:53,392 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:08:53,396 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:08:53,396 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:08:53,412 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:08:53,413 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:08:53,413 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:08:53,414 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:08:53,417 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:08:53,417 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:08:53,418 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:08:53,418 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:08:53,422 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:08:53,423 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:08:53,427 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:08:53,427 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:08:53,452 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:08:53,452 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:08:53,453 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:08:53,454 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:08:53,458 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:08:53,458 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:08:53,458 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:08:53,459 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:07:08:57,640 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:08:57,642 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:08:57,648 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:08:57,648 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  25%|       | 1/4 [00:03<00:10,  3.42s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.21s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.16s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.25s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.25s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.16s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:07:08:59,500 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:08:59,502 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:08:59,508 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:08:59,508 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.33s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.27s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.20s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.30s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.30s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.23s/it]Loading checkpoint shards:  25%|       | 1/4 [00:02<00:08,  2.81s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.22s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.28s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.19s/it]Loading checkpoint shards:  50%|     | 2/4 [00:05<00:05,  2.74s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.28s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.28s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.17s/it]Loading checkpoint shards:  25%|       | 1/4 [00:04<00:12,  4.20s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.32s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.69s/it]
Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.29s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.64s/it]
Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.24s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.59s/it]
Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.19s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.56s/it]
Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.31s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.31s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.66s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.66s/it]

Loading checkpoint shards:  75%|  | 3/4 [00:07<00:02,  2.46s/it]Loading checkpoint shards: 100%|| 4/4 [00:08<00:00,  1.72s/it]Loading checkpoint shards: 100%|| 4/4 [00:08<00:00,  2.05s/it]
Loading checkpoint shards:  50%|     | 2/4 [00:08<00:08,  4.47s/it]Loading checkpoint shards:  75%|  | 3/4 [00:13<00:04,  4.53s/it]Loading checkpoint shards: 100%|| 4/4 [00:14<00:00,  3.26s/it]Loading checkpoint shards: 100%|| 4/4 [00:14<00:00,  3.70s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:07:10:19,468 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:10:19,678 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:10:19,680 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:10:19,981 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|        | 14/83 [00:00<00:00, 137.20it/s] 34%|      | 28/83 [00:00<00:00, 138.23it/s] 51%|     | 42/83 [00:00<00:00, 138.64it/s] 67%|   | 56/83 [00:00<00:00, 137.29it/s] 84%| | 70/83 [00:00<00:00, 137.86it/s]100%|| 83/83 [00:00<00:00, 137.56it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:10:23,986 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:10:23,988 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:07:10:24,265 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:10:24,364 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:10:24,367 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 17%|        | 14/83 [00:00<00:00, 137.81it/s] 35%|      | 29/83 [00:00<00:00, 141.54it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 53%|    | 44/83 [00:00<00:00, 140.06it/s]2024-06-04:07:10:24,667 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:10:24,678 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:10:24,680 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
  0%|          | 0/83 [00:00<?, ?it/s] 71%|   | 59/83 [00:00<00:00, 135.85it/s] 17%|        | 14/83 [00:00<00:00, 132.32it/s] 88%| | 73/83 [00:00<00:00, 133.40it/s] 34%|      | 28/83 [00:00<00:00, 134.00it/s]100%|| 83/83 [00:00<00:00, 136.87it/s]
2024-06-04:07:10:24,924 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 51%|     | 42/83 [00:00<00:00, 131.37it/s] 16%|        | 13/82 [00:00<00:00, 127.35it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:10:25,054 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:10:25,057 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 67%|   | 56/83 [00:00<00:00, 130.58it/s] 32%|      | 26/82 [00:00<00:00, 128.73it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 84%| | 70/83 [00:00<00:00, 130.56it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:10:25,246 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:10:25,248 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 49%|     | 40/82 [00:00<00:00, 129.46it/s]100%|| 83/83 [00:00<00:00, 130.99it/s]
 65%|   | 53/82 [00:00<00:00, 128.52it/s]2024-06-04:07:10:25,427 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
 80%|  | 66/82 [00:00<00:00, 128.15it/s]  0%|          | 0/82 [00:00<?, ?it/s] 96%|| 79/82 [00:00<00:00, 126.21it/s] 13%|        | 11/82 [00:00<00:00, 103.26it/s]2024-06-04:07:10:25,581 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
100%|| 82/82 [00:00<00:00, 127.28it/s]
  0%|          | 0/82 [00:00<?, ?it/s] 27%|       | 22/82 [00:00<00:00, 104.26it/s] 13%|        | 11/82 [00:00<00:00, 101.52it/s] 40%|      | 33/82 [00:00<00:00, 104.35it/s] 27%|       | 22/82 [00:00<00:00, 103.60it/s] 54%|    | 44/82 [00:00<00:00, 104.48it/s] 40%|      | 33/82 [00:00<00:00, 103.91it/s] 67%|   | 55/82 [00:00<00:00, 103.99it/s] 54%|    | 44/82 [00:00<00:00, 104.86it/s] 80%|  | 66/82 [00:00<00:00, 104.62it/s] 67%|   | 55/82 [00:00<00:00, 105.08it/s] 94%|| 77/82 [00:00<00:00, 104.63it/s]100%|| 82/82 [00:00<00:00, 104.82it/s]
 80%|  | 66/82 [00:00<00:00, 105.40it/s] 94%|| 77/82 [00:00<00:00, 105.10it/s]100%|| 82/82 [00:00<00:00, 104.80it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:10:48,685 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:10:48,687 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:10:49,068 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 16%|        | 13/82 [00:00<00:00, 121.25it/s] 32%|      | 26/82 [00:00<00:00, 122.03it/s] 48%|     | 39/82 [00:00<00:00, 122.18it/s] 63%|   | 52/82 [00:00<00:00, 122.30it/s] 79%|  | 65/82 [00:00<00:00, 121.94it/s] 95%|| 78/82 [00:00<00:00, 121.88it/s]100%|| 82/82 [00:00<00:00, 121.80it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:11:28,268 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:11:28,271 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:11:28,783 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|        | 11/83 [00:00<00:00, 105.22it/s] 27%|       | 22/83 [00:00<00:00, 105.18it/s] 41%|      | 34/83 [00:00<00:00, 109.45it/s] 54%|    | 45/83 [00:00<00:00, 77.23it/s]  65%|   | 54/83 [00:00<00:00, 49.68it/s] 73%|  | 61/83 [00:01<00:00, 40.22it/s] 81%|  | 67/83 [00:01<00:00, 42.06it/s] 88%| | 73/83 [00:01<00:00, 44.67it/s] 95%|| 79/83 [00:01<00:00, 44.12it/s]100%|| 83/83 [00:01<00:00, 52.31it/s]
2024-06-04:07:11:42,069 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:11:42,069 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:11:42,069 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:11:42,069 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:11:42,069 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:11:42,070 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:11:42,070 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:11:42,070 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:14<19:11, 14.05s/it]Running generate_until requests:   2%|         | 2/83 [00:29<19:59, 14.81s/it]Running generate_until requests:   4%|         | 3/83 [00:38<16:08, 12.11s/it]Running generate_until requests:   5%|         | 4/83 [00:44<12:43,  9.67s/it]Running generate_until requests:   6%|         | 5/83 [00:53<12:25,  9.56s/it]Running generate_until requests:   7%|         | 6/83 [01:00<11:15,  8.77s/it]Running generate_until requests:   8%|         | 7/83 [01:11<11:58,  9.46s/it]Running generate_until requests:  10%|         | 8/83 [01:19<11:21,  9.09s/it]Running generate_until requests:  11%|         | 9/83 [01:31<12:17,  9.97s/it]Running generate_until requests:  12%|        | 10/83 [01:39<11:07,  9.14s/it]Running generate_until requests:  13%|        | 11/83 [01:48<10:51,  9.05s/it]Running generate_until requests:  14%|        | 12/83 [01:58<11:19,  9.57s/it]Running generate_until requests:  16%|        | 13/83 [02:04<09:57,  8.54s/it]Running generate_until requests:  17%|        | 14/83 [02:13<09:47,  8.51s/it]Running generate_until requests:  18%|        | 15/83 [02:32<13:19, 11.76s/it]Running generate_until requests:  19%|        | 16/83 [02:50<15:00, 13.44s/it]Running generate_until requests:  20%|        | 17/83 [02:58<13:09, 11.96s/it]Running generate_until requests:  22%|       | 18/83 [03:06<11:47, 10.88s/it]Running generate_until requests:  23%|       | 19/83 [03:16<11:18, 10.60s/it]Running generate_until requests:  24%|       | 20/83 [03:29<11:40, 11.12s/it]Running generate_until requests:  25%|       | 21/83 [03:37<10:40, 10.33s/it]Running generate_until requests:  27%|       | 22/83 [03:44<09:33,  9.40s/it]Running generate_until requests:  28%|       | 23/83 [03:57<10:15, 10.26s/it]Running generate_until requests:  29%|       | 24/83 [04:05<09:27,  9.62s/it]Running generate_until requests:  30%|       | 25/83 [04:13<08:50,  9.14s/it]Running generate_until requests:  31%|      | 26/83 [04:21<08:27,  8.91s/it]Running generate_until requests:  33%|      | 27/83 [04:29<08:04,  8.65s/it]Running generate_until requests:  34%|      | 28/83 [04:36<07:23,  8.06s/it]Running generate_until requests:  35%|      | 29/83 [04:45<07:33,  8.41s/it]Running generate_until requests:  36%|      | 30/83 [04:55<07:40,  8.70s/it]Running generate_until requests:  37%|      | 31/83 [05:01<06:53,  7.95s/it]Running generate_until requests:  39%|      | 32/83 [05:09<06:44,  7.93s/it]Running generate_until requests:  40%|      | 33/83 [05:13<05:40,  6.80s/it]Running generate_until requests:  41%|      | 34/83 [05:29<07:50,  9.60s/it]Running generate_until requests:  42%|     | 35/83 [05:37<07:20,  9.17s/it]Running generate_until requests:  43%|     | 36/83 [05:47<07:16,  9.28s/it]Running generate_until requests:  45%|     | 37/83 [05:57<07:27,  9.73s/it]Running generate_until requests:  46%|     | 38/83 [06:09<07:44, 10.32s/it]Running generate_until requests:  47%|     | 39/83 [06:14<06:19,  8.62s/it]Running generate_until requests:  48%|     | 40/83 [06:21<05:48,  8.10s/it]Running generate_until requests:  49%|     | 41/83 [06:28<05:24,  7.73s/it]Running generate_until requests:  51%|     | 42/83 [06:39<06:04,  8.90s/it]Running generate_until requests:  52%|    | 43/83 [06:49<06:02,  9.07s/it]Running generate_until requests:  53%|    | 44/83 [06:55<05:18,  8.17s/it]Running generate_until requests:  54%|    | 45/83 [07:02<04:58,  7.87s/it]Running generate_until requests:  55%|    | 46/83 [07:09<04:45,  7.71s/it]Running generate_until requests:  57%|    | 47/83 [07:16<04:31,  7.54s/it]Running generate_until requests:  58%|    | 48/83 [07:25<04:33,  7.81s/it]Running generate_until requests:  59%|    | 49/83 [07:30<04:02,  7.14s/it]Running generate_until requests:  60%|    | 50/83 [07:41<04:30,  8.20s/it]Running generate_until requests:  61%|   | 51/83 [07:49<04:23,  8.24s/it]Running generate_until requests:  63%|   | 52/83 [07:55<03:54,  7.58s/it]Running generate_until requests:  64%|   | 53/83 [08:02<03:41,  7.39s/it]Running generate_until requests:  65%|   | 54/83 [08:08<03:22,  6.97s/it]Running generate_until requests:  66%|   | 55/83 [08:18<03:41,  7.90s/it]Running generate_until requests:  67%|   | 56/83 [08:36<04:48, 10.68s/it]Running generate_until requests:  69%|   | 57/83 [08:44<04:20, 10.03s/it]Running generate_until requests:  70%|   | 58/83 [08:52<03:56,  9.47s/it]Running generate_until requests:  71%|   | 59/83 [08:58<03:22,  8.45s/it]Running generate_until requests:  72%|  | 60/83 [09:05<03:01,  7.90s/it]Running generate_until requests:  73%|  | 61/83 [09:14<03:04,  8.38s/it]Running generate_until requests:  75%|  | 62/83 [09:21<02:47,  7.97s/it]Running generate_until requests:  76%|  | 63/83 [09:28<02:31,  7.57s/it]Running generate_until requests:  77%|  | 64/83 [09:53<04:00, 12.67s/it]Running generate_until requests:  78%|  | 65/83 [09:59<03:14, 10.80s/it]Running generate_until requests:  80%|  | 66/83 [10:09<02:58, 10.51s/it]Running generate_until requests:  81%|  | 67/83 [10:15<02:26,  9.14s/it]Running generate_until requests:  82%| | 68/83 [10:22<02:08,  8.57s/it]Running generate_until requests:  83%| | 69/83 [10:33<02:08,  9.21s/it]Running generate_until requests:  84%| | 70/83 [10:44<02:06,  9.70s/it]Running generate_until requests:  86%| | 71/83 [10:48<01:37,  8.15s/it]Running generate_until requests:  87%| | 72/83 [10:55<01:25,  7.81s/it]Running generate_until requests:  88%| | 73/83 [11:02<01:15,  7.54s/it]Running generate_until requests:  89%| | 74/83 [11:08<01:03,  7.01s/it]Running generate_until requests:  90%| | 75/83 [11:22<01:12,  9.08s/it]Running generate_until requests:  92%|| 76/83 [11:31<01:03,  9.11s/it]Running generate_until requests:  93%|| 77/83 [11:38<00:50,  8.43s/it]Running generate_until requests:  94%|| 78/83 [11:44<00:38,  7.76s/it]Running generate_until requests:  95%|| 79/83 [11:57<00:37,  9.34s/it]Running generate_until requests:  96%|| 80/83 [12:03<00:24,  8.18s/it]Running generate_until requests:  98%|| 81/83 [12:12<00:17,  8.56s/it]Running generate_until requests:  99%|| 82/83 [12:17<00:07,  7.61s/it]Running generate_until requests: 100%|| 83/83 [12:23<00:00,  7.01s/it]Running generate_until requests: 100%|| 83/83 [12:23<00:00,  8.96s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:07:35:47,655 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:35:47,771 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:35:47,906 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:35:47,916 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:35:47,979 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:35:48,412 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:35:48,591 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:35:49,386 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:35:54,624 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:35:54,626 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:35:54,630 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:35:54,630 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:35:54,728 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:35:54,729 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:35:54,735 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:35:54,735 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:35:54,770 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:35:54,771 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:35:54,776 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:35:54,776 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:35:54,788 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:35:54,789 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:35:54,793 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:35:54,793 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:35:54,873 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:35:54,874 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:35:54,881 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:35:54,881 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:35:55,377 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:35:55,378 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:35:55,382 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:35:55,382 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:07:35:58,151 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:35:58,154 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:35:58,163 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:35:58,164 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.14s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.16s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.30s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.27s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.09s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.09s/it]2024-06-04:07:36:01,352 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:36:01,353 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:36:01,358 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:36:01,358 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.27s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.27s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.30s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.26s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.16s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.16s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.01s/it]Loading checkpoint shards:  25%|       | 1/4 [00:05<00:17,  5.79s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.17s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.22s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.22s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.25s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.13s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.15s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.28s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.25s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.64s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.61s/it]

Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.29s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.64s/it]
Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.30s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.64s/it]
Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.21s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.55s/it]
Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.22s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.56s/it]
Loading checkpoint shards:  50%|     | 2/4 [00:05<00:05,  2.67s/it]Loading checkpoint shards:  75%|  | 3/4 [00:07<00:02,  2.35s/it]Loading checkpoint shards: 100%|| 4/4 [00:08<00:00,  1.65s/it]Loading checkpoint shards: 100%|| 4/4 [00:08<00:00,  2.00s/it]
Loading checkpoint shards:  50%|     | 2/4 [00:10<00:10,  5.32s/it]Loading checkpoint shards:  75%|  | 3/4 [00:15<00:04,  4.99s/it]Loading checkpoint shards: 100%|| 4/4 [00:16<00:00,  3.54s/it]Loading checkpoint shards: 100%|| 4/4 [00:16<00:00,  4.17s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:37:25,799 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:37:25,801 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:37:25,820 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:37:25,822 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:37:25,861 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:37:25,863 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:37:26,024 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:37:26,027 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:37:26,041 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 16%|        | 13/82 [00:00<00:00, 126.95it/s]2024-06-04:07:37:26,173 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
2024-06-04:07:37:26,182 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]  0%|          | 0/82 [00:00<?, ?it/s] 32%|      | 26/82 [00:00<00:00, 127.90it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:37:26,276 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:37:26,278 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 16%|        | 13/82 [00:00<00:00, 127.44it/s] 16%|        | 13/82 [00:00<00:00, 127.75it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:07:37:26,338 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 48%|     | 39/82 [00:00<00:00, 128.20it/s] 33%|      | 27/82 [00:00<00:00, 129.70it/s] 33%|      | 27/82 [00:00<00:00, 130.02it/s] 17%|        | 14/82 [00:00<00:00, 133.52it/s]2024-06-04:07:37:26,467 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
 65%|   | 53/82 [00:00<00:00, 130.72it/s]  0%|          | 0/83 [00:00<?, ?it/s] 50%|     | 41/82 [00:00<00:00, 130.51it/s] 50%|     | 41/82 [00:00<00:00, 130.72it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:37:26,565 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:37:26,568 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 34%|      | 28/82 [00:00<00:00, 134.28it/s] 82%| | 67/82 [00:00<00:00, 132.03it/s] 23%|       | 19/83 [00:00<00:00, 189.21it/s] 67%|   | 55/82 [00:00<00:00, 130.86it/s] 67%|   | 55/82 [00:00<00:00, 130.99it/s] 51%|     | 42/82 [00:00<00:00, 134.45it/s] 99%|| 81/82 [00:00<00:00, 132.63it/s]100%|| 82/82 [00:00<00:00, 131.16it/s]
 46%|     | 38/83 [00:00<00:00, 151.87it/s] 84%| | 69/82 [00:00<00:00, 131.09it/s] 84%| | 69/82 [00:00<00:00, 131.07it/s] 74%|  | 61/82 [00:00<00:00, 153.91it/s]100%|| 82/82 [00:00<00:00, 130.74it/s]
100%|| 82/82 [00:00<00:00, 131.30it/s]
 65%|   | 54/83 [00:00<00:00, 141.83it/s]100%|| 82/82 [00:00<00:00, 171.88it/s]100%|| 82/82 [00:00<00:00, 158.00it/s]
2024-06-04:07:37:26,886 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 83%| | 69/83 [00:00<00:00, 140.56it/s] 18%|        | 15/83 [00:00<00:00, 141.25it/s]100%|| 83/83 [00:00<00:00, 144.57it/s]
 41%|      | 34/83 [00:00<00:00, 166.17it/s] 67%|   | 56/83 [00:00<00:00, 187.05it/s] 94%|| 78/83 [00:00<00:00, 197.08it/s]100%|| 83/83 [00:00<00:00, 188.52it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:38:02,559 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:38:02,561 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:38:02,960 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 16%|        | 13/83 [00:00<00:00, 124.07it/s] 31%|      | 26/83 [00:00<00:00, 124.99it/s] 47%|     | 39/83 [00:00<00:00, 122.69it/s] 63%|   | 52/83 [00:00<00:00, 123.13it/s] 78%|  | 65/83 [00:00<00:00, 123.47it/s] 94%|| 78/83 [00:00<00:00, 123.81it/s]100%|| 83/83 [00:00<00:00, 123.67it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:07:38:07,777 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:38:08,089 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:38:08,093 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:38:08,478 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 16%|        | 13/83 [00:00<00:00, 127.20it/s] 31%|      | 26/83 [00:00<00:00, 128.05it/s] 47%|     | 39/83 [00:00<00:00, 128.12it/s] 63%|   | 52/83 [00:00<00:00, 127.90it/s] 78%|  | 65/83 [00:00<00:00, 127.87it/s] 94%|| 78/83 [00:00<00:00, 128.10it/s]100%|| 83/83 [00:00<00:00, 127.98it/s]
2024-06-04:07:38:20,548 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:38:20,548 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:38:20,548 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:38:20,548 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:38:20,548 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:38:20,548 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:38:20,549 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:38:20,551 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:22<31:05, 22.75s/it]Running generate_until requests:   2%|         | 2/83 [00:53<36:55, 27.35s/it]Running generate_until requests:   4%|         | 3/83 [01:08<29:20, 22.00s/it]Running generate_until requests:   5%|         | 4/83 [01:18<22:24, 17.01s/it]Running generate_until requests:   6%|         | 5/83 [01:35<22:01, 16.95s/it]Running generate_until requests:   7%|         | 6/83 [01:46<19:27, 15.16s/it]Running generate_until requests:   8%|         | 7/83 [02:04<20:23, 16.10s/it]Running generate_until requests:  10%|         | 8/83 [02:19<19:31, 15.62s/it]Running generate_until requests:  11%|         | 9/83 [02:38<20:34, 16.68s/it]Running generate_until requests:  12%|        | 10/83 [02:48<17:54, 14.72s/it]Running generate_until requests:  13%|        | 11/83 [03:06<18:53, 15.74s/it]Running generate_until requests:  14%|        | 12/83 [03:26<20:05, 16.98s/it]Running generate_until requests:  16%|        | 13/83 [03:32<15:56, 13.67s/it]Running generate_until requests:  17%|        | 14/83 [03:45<15:33, 13.52s/it]Running generate_until requests:  18%|        | 15/83 [04:12<19:41, 17.38s/it]Running generate_until requests:  19%|        | 16/83 [04:37<22:12, 19.89s/it]Running generate_until requests:  20%|        | 17/83 [04:52<20:03, 18.24s/it]Running generate_until requests:  22%|       | 18/83 [05:07<18:38, 17.21s/it]Running generate_until requests:  23%|       | 19/83 [05:24<18:24, 17.25s/it]Running generate_until requests:  24%|       | 20/83 [05:47<19:50, 18.89s/it]Running generate_until requests:  25%|       | 21/83 [05:56<16:37, 16.09s/it]Running generate_until requests:  27%|       | 22/83 [06:10<15:36, 15.35s/it]Running generate_until requests:  28%|       | 23/83 [06:26<15:28, 15.47s/it]Running generate_until requests:  29%|       | 24/83 [06:39<14:29, 14.74s/it]Running generate_until requests:  30%|       | 25/83 [06:51<13:39, 14.13s/it]Running generate_until requests:  31%|      | 26/83 [07:05<13:12, 13.91s/it]Running generate_until requests:  33%|      | 27/83 [07:20<13:28, 14.44s/it]Running generate_until requests:  34%|      | 28/83 [07:33<12:37, 13.76s/it]Running generate_until requests:  35%|      | 29/83 [07:52<13:47, 15.33s/it]Running generate_until requests:  36%|      | 30/83 [08:09<13:59, 15.84s/it]Running generate_until requests:  37%|      | 31/83 [08:19<12:22, 14.28s/it]Running generate_until requests:  39%|      | 32/83 [08:36<12:47, 15.05s/it]Running generate_until requests:  40%|      | 33/83 [08:45<10:59, 13.19s/it]Running generate_until requests:  41%|      | 34/83 [09:09<13:26, 16.46s/it]Running generate_until requests:  42%|     | 35/83 [09:23<12:36, 15.77s/it]Running generate_until requests:  43%|     | 36/83 [09:37<11:47, 15.06s/it]Running generate_until requests:  45%|     | 37/83 [09:55<12:23, 16.17s/it]Running generate_until requests:  46%|     | 38/83 [10:18<13:31, 18.04s/it]Running generate_until requests:  47%|     | 39/83 [10:28<11:27, 15.64s/it]Running generate_until requests:  48%|     | 40/83 [10:51<12:54, 18.00s/it]Running generate_until requests:  49%|     | 41/83 [11:01<10:49, 15.47s/it]Running generate_until requests:  51%|     | 42/83 [11:18<10:57, 16.03s/it]Running generate_until requests:  52%|    | 43/83 [11:35<10:54, 16.35s/it]Running generate_until requests:  53%|    | 44/83 [11:46<09:30, 14.63s/it]Running generate_until requests:  54%|    | 45/83 [11:59<09:00, 14.22s/it]Running generate_until requests:  55%|    | 46/83 [12:11<08:17, 13.44s/it]Running generate_until requests:  57%|    | 47/83 [12:22<07:36, 12.68s/it]Running generate_until requests:  58%|    | 48/83 [12:35<07:34, 12.97s/it]Running generate_until requests:  59%|    | 49/83 [12:42<06:18, 11.14s/it]Running generate_until requests:  60%|    | 50/83 [12:56<06:36, 12.02s/it]Running generate_until requests:  61%|   | 51/83 [13:11<06:48, 12.75s/it]Running generate_until requests:  63%|   | 52/83 [13:18<05:38, 10.93s/it]Running generate_until requests:  64%|   | 53/83 [13:27<05:18, 10.62s/it]Running generate_until requests:  65%|   | 54/83 [13:35<04:45,  9.85s/it]Running generate_until requests:  66%|   | 55/83 [13:46<04:42, 10.09s/it]Running generate_until requests:  67%|   | 56/83 [14:08<06:09, 13.68s/it]Running generate_until requests:  69%|   | 57/83 [14:24<06:12, 14.33s/it]Running generate_until requests:  70%|   | 58/83 [14:36<05:39, 13.59s/it]Running generate_until requests:  71%|   | 59/83 [14:45<04:51, 12.13s/it]Running generate_until requests:  72%|  | 60/83 [14:53<04:12, 10.97s/it]Running generate_until requests:  73%|  | 61/83 [15:05<04:09, 11.35s/it]Running generate_until requests:  75%|  | 62/83 [15:15<03:48, 10.90s/it]Running generate_until requests:  76%|  | 63/83 [15:24<03:29, 10.49s/it]Running generate_until requests:  77%|  | 64/83 [15:39<03:44, 11.81s/it]Running generate_until requests:  78%|  | 65/83 [15:49<03:19, 11.06s/it]Running generate_until requests:  80%|  | 66/83 [15:56<02:49,  9.99s/it]Running generate_until requests:  81%|  | 67/83 [16:04<02:28,  9.28s/it]Running generate_until requests:  82%| | 68/83 [16:14<02:23,  9.59s/it]Running generate_until requests:  83%| | 69/83 [16:33<02:53, 12.41s/it]Running generate_until requests:  84%| | 70/83 [16:43<02:31, 11.66s/it]Running generate_until requests:  86%| | 71/83 [16:49<02:01, 10.10s/it]Running generate_until requests:  87%| | 72/83 [16:55<01:37,  8.85s/it]Running generate_until requests:  88%| | 73/83 [17:06<01:33,  9.32s/it]Running generate_until requests:  89%| | 74/83 [17:17<01:29,  9.97s/it]Running generate_until requests:  90%| | 75/83 [17:35<01:37, 12.16s/it]Running generate_until requests:  92%|| 76/83 [17:49<01:29, 12.78s/it]Running generate_until requests:  93%|| 77/83 [17:58<01:10, 11.72s/it]Running generate_until requests:  94%|| 78/83 [18:07<00:54, 10.91s/it]Running generate_until requests:  95%|| 79/83 [18:20<00:46, 11.61s/it]Running generate_until requests:  96%|| 80/83 [18:28<00:31, 10.46s/it]Running generate_until requests:  98%|| 81/83 [18:42<00:22, 11.49s/it]Running generate_until requests:  99%|| 82/83 [18:50<00:10, 10.36s/it]Running generate_until requests: 100%|| 83/83 [19:01<00:00, 10.77s/it]Running generate_until requests: 100%|| 83/83 [19:01<00:00, 13.76s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:07:58:07,501 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:58:07,501 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:58:07,516 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:58:07,568 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:58:08,086 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:58:08,558 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:58:08,892 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:58:09,264 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:58:13,263 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:58:13,265 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:58:13,269 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:58:13,270 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:58:13,415 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:58:13,416 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:58:13,421 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:58:13,421 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:58:13,747 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:58:13,749 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:58:13,753 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:58:13,753 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:58:14,003 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:58:14,004 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:58:14,009 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:58:14,009 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:58:14,420 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:58:14,421 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:58:14,425 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:58:14,425 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|       | 1/4 [00:02<00:06,  2.20s/it]Loading checkpoint shards:  25%|       | 1/4 [00:02<00:08,  2.99s/it]Loading checkpoint shards:  25%|       | 1/4 [00:02<00:08,  2.91s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.04s/it]2024-06-04:07:58:18,677 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:58:18,679 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:58:18,685 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:58:18,685 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.01s/it]2024-06-04:07:58:19,231 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:58:19,232 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:58:19,237 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:58:19,237 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  50%|     | 2/4 [00:04<00:04,  2.23s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.06s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.02s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.11s/it]Loading checkpoint shards:  75%|  | 3/4 [00:06<00:02,  2.30s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.13s/it]Loading checkpoint shards: 100%|| 4/4 [00:07<00:00,  1.67s/it]Loading checkpoint shards: 100%|| 4/4 [00:07<00:00,  1.89s/it]
Loading checkpoint shards:  25%|       | 1/4 [00:02<00:07,  2.56s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.30s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.01s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.03s/it]2024-06-04:07:58:24,554 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:58:24,559 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:58:24,575 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:58:24,576 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.06s/it]Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.13s/it]Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.46s/it]
Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.15s/it]Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.46s/it]
Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.02s/it]Loading checkpoint shards:  50%|     | 2/4 [00:04<00:04,  2.19s/it]Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.11s/it]Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.47s/it]
Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.08s/it]Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.44s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|  | 3/4 [00:06<00:01,  1.97s/it]Loading checkpoint shards: 100%|| 4/4 [00:06<00:00,  1.40s/it]Loading checkpoint shards: 100%|| 4/4 [00:06<00:00,  1.68s/it]
Loading checkpoint shards:  50%|     | 2/4 [00:07<00:08,  4.10s/it]Loading checkpoint shards:  25%|       | 1/4 [00:02<00:08,  2.73s/it]Loading checkpoint shards:  50%|     | 2/4 [00:05<00:05,  2.54s/it]Loading checkpoint shards:  75%|  | 3/4 [00:07<00:02,  2.42s/it]Loading checkpoint shards: 100%|| 4/4 [00:08<00:00,  1.72s/it]Loading checkpoint shards: 100%|| 4/4 [00:08<00:00,  2.01s/it]
Loading checkpoint shards:  75%|  | 3/4 [00:14<00:05,  5.33s/it]Loading checkpoint shards: 100%|| 4/4 [00:16<00:00,  3.89s/it]Loading checkpoint shards: 100%|| 4/4 [00:16<00:00,  4.11s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:59:05,273 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:59:05,276 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:59:05,570 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|        | 14/83 [00:00<00:00, 136.33it/s] 34%|      | 28/83 [00:00<00:00, 137.51it/s] 51%|     | 42/83 [00:00<00:00, 137.82it/s] 67%|   | 56/83 [00:00<00:00, 137.83it/s] 84%| | 70/83 [00:00<00:00, 133.33it/s]100%|| 83/83 [00:00<00:00, 135.20it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:59:26,102 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:59:26,104 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:59:26,196 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:59:26,198 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:59:26,307 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:07:59:26,368 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|       | 20/82 [00:00<00:00, 195.52it/s] 24%|       | 20/83 [00:00<00:00, 198.54it/s] 49%|     | 40/82 [00:00<00:00, 194.60it/s] 48%|     | 40/83 [00:00<00:00, 197.77it/s] 73%|  | 60/82 [00:00<00:00, 195.55it/s] 72%|  | 60/83 [00:00<00:00, 198.63it/s] 98%|| 80/82 [00:00<00:00, 196.35it/s]100%|| 82/82 [00:00<00:00, 195.93it/s]
 98%|| 81/83 [00:00<00:00, 200.54it/s]100%|| 83/83 [00:00<00:00, 199.86it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:59:35,463 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:59:35,465 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:59:35,625 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|       | 21/83 [00:00<00:00, 206.47it/s] 51%|     | 42/83 [00:00<00:00, 206.29it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 76%|  | 63/83 [00:00<00:00, 206.77it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:59:35,977 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:59:35,979 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
100%|| 83/83 [00:00<00:00, 206.56it/s]
2024-06-04:07:59:36,173 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|       | 21/82 [00:00<00:00, 208.38it/s] 51%|     | 42/82 [00:00<00:00, 208.44it/s] 77%|  | 63/82 [00:00<00:00, 208.76it/s]100%|| 82/82 [00:00<00:00, 208.62it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:07:59:52,818 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:59:52,892 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:59:52,894 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:59:53,289 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 14%|        | 12/83 [00:00<00:00, 117.03it/s] 29%|       | 24/83 [00:00<00:00, 118.02it/s] 43%|     | 36/83 [00:00<00:00, 118.04it/s] 58%|    | 48/83 [00:00<00:00, 118.41it/s] 72%|  | 60/83 [00:00<00:00, 118.55it/s] 87%| | 72/83 [00:00<00:00, 118.65it/s]100%|| 83/83 [00:00<00:00, 118.44it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:00:38,509 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:00:38,511 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:00:39,176 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s]  5%|         | 4/82 [00:00<00:02, 35.32it/s] 12%|        | 10/82 [00:00<00:01, 48.89it/s] 26%|       | 21/82 [00:00<00:00, 73.62it/s] 35%|      | 29/82 [00:00<00:00, 73.07it/s] 48%|     | 39/82 [00:00<00:00, 82.24it/s] 61%|    | 50/82 [00:00<00:00, 89.35it/s] 74%|  | 61/82 [00:00<00:00, 93.64it/s] 88%| | 72/82 [00:00<00:00, 96.80it/s]100%|| 82/82 [00:00<00:00, 86.44it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:01:33,300 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:01:33,303 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:01:33,794 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|        | 11/82 [00:00<00:00, 104.73it/s] 27%|       | 22/82 [00:00<00:00, 105.44it/s] 40%|      | 33/82 [00:00<00:00, 105.89it/s] 54%|    | 44/82 [00:00<00:00, 106.08it/s] 67%|   | 55/82 [00:00<00:00, 106.00it/s] 80%|  | 66/82 [00:00<00:00, 105.53it/s] 94%|| 77/82 [00:00<00:00, 105.77it/s]100%|| 82/82 [00:00<00:00, 105.74it/s]
2024-06-04:08:01:45,656 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:01:45,656 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:01:45,656 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:01:45,656 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:01:45,656 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:01:45,657 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:01:45,657 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:08:01:45,657 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:16<21:55, 16.04s/it]Running generate_until requests:   2%|         | 2/83 [00:34<23:17, 17.25s/it]Running generate_until requests:   4%|         | 3/83 [00:42<17:40, 13.25s/it]Running generate_until requests:   5%|         | 4/83 [00:47<13:15, 10.07s/it]Running generate_until requests:   6%|         | 5/83 [00:56<12:37,  9.71s/it]Running generate_until requests:   7%|         | 6/83 [01:03<11:04,  8.62s/it]Running generate_until requests:   8%|         | 7/83 [01:12<11:16,  8.90s/it]Running generate_until requests:  10%|         | 8/83 [01:20<10:42,  8.57s/it]Running generate_until requests:  11%|         | 9/83 [01:32<11:38,  9.44s/it]Running generate_until requests:  12%|        | 10/83 [01:38<10:30,  8.64s/it]Running generate_until requests:  13%|        | 11/83 [01:47<10:13,  8.51s/it]Running generate_until requests:  14%|        | 12/83 [01:59<11:29,  9.70s/it]Running generate_until requests:  16%|        | 13/83 [02:03<09:10,  7.87s/it]Running generate_until requests:  17%|        | 14/83 [02:10<08:48,  7.65s/it]Running generate_until requests:  18%|        | 15/83 [02:17<08:33,  7.56s/it]Running generate_until requests:  19%|        | 16/83 [02:30<10:16,  9.20s/it]Running generate_until requests:  20%|        | 17/83 [02:38<09:39,  8.78s/it]Running generate_until requests:  22%|       | 18/83 [02:46<09:17,  8.58s/it]Running generate_until requests:  23%|       | 19/83 [02:56<09:25,  8.84s/it]Running generate_until requests:  24%|       | 20/83 [03:09<10:34, 10.07s/it]Running generate_until requests:  25%|       | 21/83 [03:14<08:56,  8.65s/it]Running generate_until requests:  27%|       | 22/83 [03:21<08:20,  8.20s/it]Running generate_until requests:  28%|       | 23/83 [03:29<08:12,  8.20s/it]Running generate_until requests:  29%|       | 24/83 [03:36<07:43,  7.86s/it]Running generate_until requests:  30%|       | 25/83 [03:45<07:44,  8.01s/it]Running generate_until requests:  31%|      | 26/83 [03:52<07:20,  7.73s/it]Running generate_until requests:  33%|      | 27/83 [04:04<08:32,  9.14s/it]Running generate_until requests:  34%|      | 28/83 [04:11<07:41,  8.39s/it]Running generate_until requests:  35%|      | 29/83 [04:20<07:44,  8.60s/it]Running generate_until requests:  36%|      | 30/83 [04:29<07:48,  8.85s/it]Running generate_until requests:  37%|      | 31/83 [04:37<07:19,  8.46s/it]Running generate_until requests:  39%|      | 32/83 [04:46<07:15,  8.54s/it]Running generate_until requests:  40%|      | 33/83 [04:51<06:15,  7.51s/it]Running generate_until requests:  41%|      | 34/83 [05:02<07:02,  8.62s/it]Running generate_until requests:  42%|     | 35/83 [05:10<06:49,  8.52s/it]Running generate_until requests:  43%|     | 36/83 [05:17<06:17,  8.03s/it]Running generate_until requests:  45%|     | 37/83 [05:25<06:02,  7.88s/it]Running generate_until requests:  46%|     | 38/83 [05:36<06:43,  8.97s/it]Running generate_until requests:  47%|     | 39/83 [05:42<05:48,  7.91s/it]Running generate_until requests:  48%|     | 40/83 [05:52<06:12,  8.67s/it]Running generate_until requests:  49%|     | 41/83 [05:57<05:21,  7.66s/it]Running generate_until requests:  51%|     | 42/83 [06:05<05:15,  7.70s/it]Running generate_until requests:  52%|    | 43/83 [06:14<05:22,  8.07s/it]Running generate_until requests:  53%|    | 44/83 [06:20<04:45,  7.31s/it]Running generate_until requests:  54%|    | 45/83 [06:27<04:33,  7.20s/it]Running generate_until requests:  55%|    | 46/83 [06:32<04:06,  6.67s/it]Running generate_until requests:  57%|    | 47/83 [06:38<03:50,  6.41s/it]Running generate_until requests:  58%|    | 48/83 [06:46<04:07,  7.06s/it]Running generate_until requests:  59%|    | 49/83 [06:51<03:38,  6.43s/it]Running generate_until requests:  60%|    | 50/83 [07:00<03:55,  7.13s/it]Running generate_until requests:  61%|   | 51/83 [07:07<03:50,  7.20s/it]Running generate_until requests:  63%|   | 52/83 [07:28<05:43, 11.09s/it]Running generate_until requests:  64%|   | 53/83 [07:37<05:17, 10.58s/it]Running generate_until requests:  65%|   | 54/83 [07:42<04:22,  9.05s/it]Running generate_until requests:  66%|   | 55/83 [07:49<03:55,  8.42s/it]Running generate_until requests:  67%|   | 56/83 [08:02<04:17,  9.56s/it]Running generate_until requests:  69%|   | 57/83 [08:13<04:25, 10.21s/it]Running generate_until requests:  70%|   | 58/83 [08:21<03:57,  9.52s/it]Running generate_until requests:  71%|   | 59/83 [08:27<03:21,  8.40s/it]Running generate_until requests:  72%|  | 60/83 [08:33<02:53,  7.56s/it]Running generate_until requests:  73%|  | 61/83 [08:41<02:54,  7.92s/it]Running generate_until requests:  75%|  | 62/83 [08:47<02:28,  7.09s/it]Running generate_until requests:  76%|  | 63/83 [08:53<02:16,  6.81s/it]Running generate_until requests:  77%|  | 64/83 [09:01<02:16,  7.16s/it]Running generate_until requests:  78%|  | 65/83 [09:07<02:03,  6.88s/it]Running generate_until requests:  80%|  | 66/83 [09:12<01:49,  6.42s/it]Running generate_until requests:  81%|  | 67/83 [09:18<01:38,  6.19s/it]Running generate_until requests:  82%| | 68/83 [09:24<01:30,  6.05s/it]Running generate_until requests:  83%| | 69/83 [09:35<01:47,  7.65s/it]Running generate_until requests:  84%| | 70/83 [09:44<01:42,  7.90s/it]Running generate_until requests:  86%| | 71/83 [09:48<01:22,  6.88s/it]Running generate_until requests:  87%| | 72/83 [09:53<01:09,  6.35s/it]Running generate_until requests:  88%| | 73/83 [10:00<01:03,  6.35s/it]Running generate_until requests:  89%| | 74/83 [10:07<01:00,  6.69s/it]Running generate_until requests:  90%| | 75/83 [10:17<01:01,  7.63s/it]Running generate_until requests:  92%|| 76/83 [10:23<00:50,  7.17s/it]Running generate_until requests:  93%|| 77/83 [10:28<00:39,  6.64s/it]Running generate_until requests:  94%|| 78/83 [10:34<00:32,  6.44s/it]Running generate_until requests:  95%|| 79/83 [10:42<00:26,  6.69s/it]Running generate_until requests:  96%|| 80/83 [10:48<00:19,  6.65s/it]Running generate_until requests:  98%|| 81/83 [10:56<00:14,  7.14s/it]Running generate_until requests:  99%|| 82/83 [11:02<00:06,  6.54s/it]Running generate_until requests: 100%|| 83/83 [11:07<00:00,  6.20s/it]Running generate_until requests: 100%|| 83/83 [11:07<00:00,  8.04s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:08:23:08,512 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:23:08,532 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:23:08,963 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:23:09,152 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:23:09,233 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:23:11,281 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:23:12,022 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:23:12,835 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:23:13,463 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:23:13,464 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:23:13,469 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:23:13,469 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:23:14,353 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:23:14,354 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:23:14,358 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:23:14,358 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:23:14,491 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:23:14,492 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:23:14,497 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:23:14,497 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:23:14,787 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:23:14,788 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:23:14,792 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:23:14,792 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|       | 1/4 [00:02<00:07,  2.38s/it]Loading checkpoint shards:  25%|       | 1/4 [00:02<00:08,  2.80s/it]Loading checkpoint shards:  25%|       | 1/4 [00:02<00:07,  2.54s/it]Loading checkpoint shards:  25%|       | 1/4 [00:02<00:08,  2.99s/it]Loading checkpoint shards:  50%|     | 2/4 [00:04<00:04,  2.29s/it]Loading checkpoint shards:  50%|     | 2/4 [00:04<00:04,  2.28s/it]Loading checkpoint shards:  50%|     | 2/4 [00:05<00:05,  2.89s/it]2024-06-04:08:23:21,670 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:23:21,672 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:23:21,680 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:23:21,681 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:23:22,071 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:23:22,073 INFO     [main.py:378] Selected Tasks: ['gsm8k']
Loading checkpoint shards:  75%|  | 3/4 [00:06<00:02,  2.20s/it]2024-06-04:08:23:22,083 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:23:22,083 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  50%|     | 2/4 [00:05<00:05,  2.95s/it]Loading checkpoint shards:  75%|  | 3/4 [00:06<00:02,  2.25s/it]Loading checkpoint shards: 100%|| 4/4 [00:07<00:00,  1.57s/it]Loading checkpoint shards: 100%|| 4/4 [00:07<00:00,  1.83s/it]
Loading checkpoint shards: 100%|| 4/4 [00:07<00:00,  1.61s/it]Loading checkpoint shards: 100%|| 4/4 [00:07<00:00,  1.87s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:08:23:23,832 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:23:23,833 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:23:23,838 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:23:23,838 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  75%|  | 3/4 [00:08<00:02,  2.88s/it]2024-06-04:08:23:24,445 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:23:24,450 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:23:24,468 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:23:24,469 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.06s/it]Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.36s/it]
Loading checkpoint shards:  75%|  | 3/4 [00:08<00:02,  2.84s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.04s/it]Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.36s/it]
Loading checkpoint shards:  25%|       | 1/4 [00:02<00:08,  2.74s/it]Loading checkpoint shards:  25%|       | 1/4 [00:02<00:08,  2.82s/it]Loading checkpoint shards:  25%|       | 1/4 [00:02<00:06,  2.15s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|     | 2/4 [00:05<00:04,  2.50s/it]Loading checkpoint shards:  50%|     | 2/4 [00:05<00:04,  2.49s/it]Loading checkpoint shards:  50%|     | 2/4 [00:04<00:04,  2.08s/it]Loading checkpoint shards:  75%|  | 3/4 [00:07<00:02,  2.43s/it]Loading checkpoint shards:  75%|  | 3/4 [00:07<00:02,  2.47s/it]Loading checkpoint shards: 100%|| 4/4 [00:08<00:00,  1.78s/it]Loading checkpoint shards: 100%|| 4/4 [00:08<00:00,  2.06s/it]
Loading checkpoint shards: 100%|| 4/4 [00:08<00:00,  1.79s/it]Loading checkpoint shards: 100%|| 4/4 [00:08<00:00,  2.06s/it]
Loading checkpoint shards:  75%|  | 3/4 [00:07<00:02,  2.66s/it]Loading checkpoint shards:  25%|       | 1/4 [00:04<00:14,  4.78s/it]Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.38s/it]Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.37s/it]
Loading checkpoint shards:  50%|     | 2/4 [00:07<00:07,  3.65s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.01s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.09s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.64s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:24:09,827 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:24:09,829 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:24:10,044 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 24%|       | 20/82 [00:00<00:00, 195.82it/s] 49%|     | 40/82 [00:00<00:00, 189.94it/s] 73%|  | 60/82 [00:00<00:00, 159.48it/s] 98%|| 80/82 [00:00<00:00, 172.80it/s]100%|| 82/82 [00:00<00:00, 174.43it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:24:15,469 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:24:15,471 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:24:15,763 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|        | 14/83 [00:00<00:00, 130.41it/s] 34%|      | 28/83 [00:00<00:00, 131.52it/s] 51%|     | 42/83 [00:00<00:00, 132.24it/s] 67%|   | 56/83 [00:00<00:00, 133.78it/s] 84%| | 70/83 [00:00<00:00, 135.30it/s]100%|| 83/83 [00:00<00:00, 134.64it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:08:24:27,497 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:24:27,566 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:24:27,568 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:24:27,745 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|       | 20/83 [00:00<00:00, 197.02it/s] 48%|     | 40/83 [00:00<00:00, 197.72it/s] 72%|  | 60/83 [00:00<00:00, 197.54it/s] 96%|| 80/83 [00:00<00:00, 197.75it/s]100%|| 83/83 [00:00<00:00, 197.65it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:24:30,287 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:24:30,289 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:24:30,730 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|        | 11/82 [00:00<00:00, 104.02it/s] 27%|       | 22/82 [00:00<00:00, 104.08it/s] 40%|      | 33/82 [00:00<00:00, 104.68it/s] 54%|    | 44/82 [00:00<00:00, 104.72it/s] 67%|   | 55/82 [00:00<00:00, 104.79it/s] 80%|  | 66/82 [00:00<00:00, 104.85it/s] 94%|| 77/82 [00:00<00:00, 104.82it/s]100%|| 82/82 [00:00<00:00, 104.69it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:25:02,804 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:25:02,807 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:25:03,273 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 10%|         | 8/83 [00:00<00:00, 76.97it/s] 19%|        | 16/83 [00:00<00:00, 73.38it/s] 30%|       | 25/83 [00:00<00:00, 80.20it/s] 42%|     | 35/83 [00:00<00:00, 85.72it/s] 53%|    | 44/83 [00:00<00:00, 83.80it/s] 65%|   | 54/83 [00:00<00:00, 88.27it/s] 76%|  | 63/83 [00:00<00:00, 83.52it/s] 87%| | 72/83 [00:00<00:00, 79.40it/s] 98%|| 81/83 [00:01<00:00, 78.45it/s]100%|| 83/83 [00:01<00:00, 80.78it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:25:04,932 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:25:04,935 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:25:05,729 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]  7%|         | 6/82 [00:00<00:01, 53.55it/s] 15%|        | 12/82 [00:00<00:01, 54.63it/s] 22%|       | 18/82 [00:00<00:01, 54.85it/s] 29%|       | 24/82 [00:00<00:01, 54.87it/s] 37%|      | 30/82 [00:00<00:00, 54.96it/s] 44%|     | 36/82 [00:00<00:00, 54.94it/s] 51%|     | 42/82 [00:00<00:00, 54.91it/s] 59%|    | 48/82 [00:00<00:00, 53.27it/s] 66%|   | 54/82 [00:00<00:00, 53.19it/s] 73%|  | 60/82 [00:01<00:00, 53.20it/s] 80%|  | 66/82 [00:01<00:00, 54.27it/s] 88%| | 72/82 [00:01<00:00, 54.81it/s] 95%|| 78/82 [00:01<00:00, 54.67it/s]100%|| 82/82 [00:01<00:00, 54.38it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:25:33,192 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:25:33,195 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:25:33,746 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s]  9%|         | 7/82 [00:00<00:01, 69.68it/s] 18%|        | 15/82 [00:00<00:00, 72.44it/s] 28%|       | 23/82 [00:00<00:00, 74.65it/s] 38%|      | 31/82 [00:00<00:00, 71.21it/s] 52%|    | 43/82 [00:00<00:00, 85.59it/s] 63%|   | 52/82 [00:00<00:00, 83.61it/s] 78%|  | 64/82 [00:00<00:00, 94.61it/s] 93%|| 76/82 [00:00<00:00, 100.57it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
100%|| 82/82 [00:00<00:00, 90.18it/s] 
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:25:34,799 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:25:34,802 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:25:35,281 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|        | 11/83 [00:00<00:00, 102.82it/s] 27%|       | 22/83 [00:00<00:00, 104.79it/s] 40%|      | 33/83 [00:00<00:00, 103.56it/s] 53%|    | 44/83 [00:00<00:00, 104.70it/s] 66%|   | 55/83 [00:00<00:00, 89.47it/s]  80%|  | 66/83 [00:00<00:00, 93.82it/s] 93%|| 77/83 [00:00<00:00, 96.60it/s]100%|| 83/83 [00:00<00:00, 97.77it/s]
2024-06-04:08:25:46,770 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:25:46,770 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:25:46,770 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:25:46,770 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:25:46,770 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:25:46,770 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:08:25:46,772 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:25:46,773 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:12<16:30, 12.08s/it]Running generate_until requests:   2%|         | 2/83 [00:25<17:01, 12.61s/it]Running generate_until requests:   4%|         | 3/83 [00:32<13:27, 10.09s/it]Running generate_until requests:   5%|         | 4/83 [00:36<10:28,  7.96s/it]Running generate_until requests:   6%|         | 5/83 [00:43<09:39,  7.43s/it]Running generate_until requests:   7%|         | 6/83 [00:49<08:46,  6.84s/it]Running generate_until requests:   8%|         | 7/83 [00:56<09:06,  7.19s/it]Running generate_until requests:  10%|         | 8/83 [01:03<08:45,  7.01s/it]Running generate_until requests:  11%|         | 9/83 [01:12<09:26,  7.66s/it]Running generate_until requests:  12%|        | 10/83 [01:17<08:25,  6.93s/it]Running generate_until requests:  13%|        | 11/83 [01:24<08:19,  6.94s/it]Running generate_until requests:  14%|        | 12/83 [01:36<09:48,  8.29s/it]Running generate_until requests:  16%|        | 13/83 [01:42<08:47,  7.54s/it]Running generate_until requests:  17%|        | 14/83 [01:48<08:08,  7.08s/it]Running generate_until requests:  18%|        | 15/83 [02:00<09:59,  8.82s/it]Running generate_until requests:  19%|        | 16/83 [02:13<10:58,  9.83s/it]Running generate_until requests:  20%|        | 17/83 [02:19<09:45,  8.87s/it]Running generate_until requests:  22%|       | 18/83 [02:26<08:56,  8.25s/it]Running generate_until requests:  23%|       | 19/83 [02:33<08:23,  7.87s/it]Running generate_until requests:  24%|       | 20/83 [02:44<09:13,  8.79s/it]Running generate_until requests:  25%|       | 21/83 [02:49<07:51,  7.60s/it]Running generate_until requests:  27%|       | 22/83 [02:55<07:13,  7.11s/it]Running generate_until requests:  28%|       | 23/83 [03:03<07:17,  7.30s/it]Running generate_until requests:  29%|       | 24/83 [03:08<06:45,  6.88s/it]Running generate_until requests:  30%|       | 25/83 [03:15<06:37,  6.85s/it]Running generate_until requests:  31%|      | 26/83 [03:21<06:13,  6.55s/it]Running generate_until requests:  33%|      | 27/83 [03:28<06:10,  6.62s/it]Running generate_until requests:  34%|      | 28/83 [03:33<05:46,  6.31s/it]Running generate_until requests:  35%|      | 29/83 [03:40<05:41,  6.32s/it]Running generate_until requests:  36%|      | 30/83 [03:48<05:58,  6.77s/it]Running generate_until requests:  37%|      | 31/83 [03:53<05:34,  6.44s/it]Running generate_until requests:  39%|      | 32/83 [03:59<05:17,  6.22s/it]Running generate_until requests:  40%|      | 33/83 [04:03<04:43,  5.67s/it]Running generate_until requests:  41%|      | 34/83 [04:14<05:56,  7.27s/it]Running generate_until requests:  42%|     | 35/83 [04:21<05:43,  7.15s/it]Running generate_until requests:  43%|     | 36/83 [04:29<05:43,  7.31s/it]Running generate_until requests:  45%|     | 37/83 [04:36<05:29,  7.15s/it]Running generate_until requests:  46%|     | 38/83 [04:47<06:18,  8.41s/it]Running generate_until requests:  47%|     | 39/83 [04:52<05:20,  7.29s/it]Running generate_until requests:  48%|     | 40/83 [05:01<05:35,  7.79s/it]Running generate_until requests:  49%|     | 41/83 [05:05<04:46,  6.83s/it]Running generate_until requests:  51%|     | 42/83 [05:13<04:55,  7.21s/it]Running generate_until requests:  52%|    | 43/83 [05:21<04:51,  7.29s/it]Running generate_until requests:  53%|    | 44/83 [05:26<04:13,  6.50s/it]Running generate_until requests:  54%|    | 45/83 [05:31<03:59,  6.30s/it]Running generate_until requests:  55%|    | 46/83 [05:37<03:48,  6.17s/it]Running generate_until requests:  57%|    | 47/83 [05:42<03:28,  5.80s/it]Running generate_until requests:  58%|    | 48/83 [05:49<03:37,  6.21s/it]Running generate_until requests:  59%|    | 49/83 [05:54<03:14,  5.72s/it]Running generate_until requests:  60%|    | 50/83 [06:02<03:31,  6.39s/it]Running generate_until requests:  61%|   | 51/83 [06:08<03:17,  6.18s/it]Running generate_until requests:  63%|   | 52/83 [06:11<02:49,  5.48s/it]Running generate_until requests:  64%|   | 53/83 [06:19<03:04,  6.15s/it]Running generate_until requests:  65%|   | 54/83 [06:24<02:44,  5.66s/it]Running generate_until requests:  66%|   | 55/83 [06:30<02:42,  5.79s/it]Running generate_until requests:  67%|   | 56/83 [06:41<03:19,  7.40s/it]Running generate_until requests:  69%|   | 57/83 [06:49<03:19,  7.67s/it]Running generate_until requests:  70%|   | 58/83 [06:55<03:01,  7.26s/it]Running generate_until requests:  71%|   | 59/83 [07:00<02:37,  6.57s/it]Running generate_until requests:  72%|  | 60/83 [07:05<02:17,  5.99s/it]Running generate_until requests:  73%|  | 61/83 [07:12<02:17,  6.24s/it]Running generate_until requests:  75%|  | 62/83 [07:17<02:04,  5.94s/it]Running generate_until requests:  76%|  | 63/83 [07:22<01:51,  5.58s/it]Running generate_until requests:  77%|  | 64/83 [07:31<02:03,  6.51s/it]Running generate_until requests:  78%|  | 65/83 [07:36<01:50,  6.14s/it]Running generate_until requests:  80%|  | 66/83 [07:40<01:34,  5.57s/it]Running generate_until requests:  81%|  | 67/83 [07:44<01:20,  5.03s/it]Running generate_until requests:  82%| | 68/83 [07:50<01:19,  5.32s/it]Running generate_until requests:  83%| | 69/83 [08:00<01:33,  6.65s/it]Running generate_until requests:  84%| | 70/83 [08:07<01:29,  6.86s/it]Running generate_until requests:  86%| | 71/83 [08:11<01:10,  5.91s/it]Running generate_until requests:  87%| | 72/83 [08:15<00:58,  5.31s/it]Running generate_until requests:  88%| | 73/83 [08:20<00:54,  5.41s/it]Running generate_until requests:  89%| | 74/83 [08:26<00:50,  5.61s/it]Running generate_until requests:  90%| | 75/83 [08:35<00:52,  6.52s/it]Running generate_until requests:  92%|| 76/83 [08:39<00:40,  5.86s/it]Running generate_until requests:  93%|| 77/83 [08:45<00:34,  5.71s/it]Running generate_until requests:  94%|| 78/83 [08:50<00:27,  5.51s/it]Running generate_until requests:  95%|| 79/83 [08:56<00:23,  5.80s/it]Running generate_until requests:  96%|| 80/83 [09:02<00:17,  5.68s/it]Running generate_until requests:  98%|| 81/83 [09:09<00:12,  6.12s/it]Running generate_until requests:  99%|| 82/83 [09:14<00:05,  5.78s/it]Running generate_until requests: 100%|| 83/83 [09:18<00:00,  5.38s/it]Running generate_until requests: 100%|| 83/83 [09:18<00:00,  6.73s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:08:42:36,004 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:42:36,004 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:42:36,024 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:42:36,045 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:42:36,132 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:42:36,466 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:42:36,659 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:42:37,953 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:42:42,818 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:42:42,819 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:42:42,824 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:42:42,824 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:42:42,870 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:42:42,871 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:42:42,876 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:42:42,876 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:42:42,885 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:42:42,886 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:42:42,890 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:42:42,890 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:42:42,973 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:42:42,974 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:42:42,979 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:42:42,979 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:42:43,127 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:42:43,129 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:42:43,134 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:42:43,135 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:42:43,440 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:42:43,441 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:42:43,445 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:42:43,445 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:08:42:47,056 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:42:47,057 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:42:47,064 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:42:47,064 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.16s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.26s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.18s/it]2024-06-04:08:42:48,252 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:42:48,254 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:42:48,259 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:42:48,259 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.19s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.13s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.23s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.22s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.27s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.25s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.17s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.25s/it]Loading checkpoint shards:  50%|     | 2/4 [00:06<00:06,  3.23s/it]Loading checkpoint shards:  25%|       | 1/4 [00:03<00:09,  3.12s/it]Loading checkpoint shards:  25%|       | 1/4 [00:05<00:16,  5.42s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.21s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.23s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.20s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.13s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.23s/it]Loading checkpoint shards:  75%|  | 3/4 [00:09<00:03,  3.19s/it]Loading checkpoint shards:  50%|     | 2/4 [00:05<00:05,  2.75s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.27s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.61s/it]
Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.25s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.60s/it]
Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.29s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.64s/it]
Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.21s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.55s/it]
Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.23s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.60s/it]
Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.20s/it]Loading checkpoint shards: 100%|| 4/4 [00:10<00:00,  2.57s/it]
Loading checkpoint shards:  75%|  | 3/4 [00:07<00:02,  2.40s/it]Loading checkpoint shards: 100%|| 4/4 [00:08<00:00,  1.68s/it]Loading checkpoint shards: 100%|| 4/4 [00:08<00:00,  2.04s/it]
Loading checkpoint shards:  50%|     | 2/4 [00:10<00:10,  5.17s/it]Loading checkpoint shards:  75%|  | 3/4 [00:15<00:04,  4.95s/it]Loading checkpoint shards: 100%|| 4/4 [00:16<00:00,  3.52s/it]Loading checkpoint shards: 100%|| 4/4 [00:16<00:00,  4.11s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:44:13,115 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:44:13,117 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:44:13,449 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 18%|        | 15/82 [00:00<00:00, 140.25it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:44:13,653 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:44:13,655 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 37%|      | 30/82 [00:00<00:00, 141.59it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:44:13,766 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:44:13,768 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 55%|    | 45/82 [00:00<00:00, 139.84it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:08:44:13,874 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:44:13,888 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:44:13,890 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:44:13,897 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:44:13,899 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 76%|  | 62/82 [00:00<00:00, 148.87it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 17%|        | 14/82 [00:00<00:00, 138.92it/s]2024-06-04:08:44:14,012 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 94%|| 77/82 [00:00<00:00, 141.48it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:44:14,041 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:44:14,043 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
100%|| 82/82 [00:00<00:00, 141.55it/s]
 34%|      | 28/82 [00:00<00:00, 133.00it/s] 17%|        | 14/83 [00:00<00:00, 139.17it/s]2024-06-04:08:44:14,175 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 51%|     | 42/82 [00:00<00:00, 131.31it/s]2024-06-04:08:44:14,219 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
 34%|      | 28/83 [00:00<00:00, 132.56it/s]  0%|          | 0/83 [00:00<?, ?it/s] 24%|       | 20/83 [00:00<00:00, 195.51it/s] 68%|   | 56/82 [00:00<00:00, 131.75it/s] 51%|     | 42/83 [00:00<00:00, 130.78it/s] 17%|        | 14/83 [00:00<00:00, 133.98it/s]2024-06-04:08:44:14,350 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 48%|     | 40/83 [00:00<00:00, 194.25it/s] 85%| | 70/82 [00:00<00:00, 132.87it/s] 67%|   | 56/83 [00:00<00:00, 130.74it/s] 34%|      | 28/83 [00:00<00:00, 134.56it/s] 17%|        | 14/82 [00:00<00:00, 130.94it/s] 72%|  | 60/83 [00:00<00:00, 195.72it/s]100%|| 82/82 [00:00<00:00, 133.16it/s]
 54%|    | 45/83 [00:00<00:00, 148.05it/s] 84%| | 70/83 [00:00<00:00, 130.98it/s] 34%|      | 28/82 [00:00<00:00, 131.19it/s] 96%|| 80/83 [00:00<00:00, 196.38it/s]100%|| 83/83 [00:00<00:00, 195.95it/s]
100%|| 83/83 [00:00<00:00, 131.91it/s]
 80%|  | 66/83 [00:00<00:00, 169.88it/s] 54%|    | 44/82 [00:00<00:00, 143.36it/s]100%|| 83/83 [00:00<00:00, 167.31it/s]
 80%|  | 66/82 [00:00<00:00, 169.92it/s]100%|| 82/82 [00:00<00:00, 165.93it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:08:44:49,474 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:44:49,549 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:44:49,552 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:44:49,919 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 16%|        | 13/83 [00:00<00:00, 126.60it/s] 31%|      | 26/83 [00:00<00:00, 127.52it/s] 47%|     | 39/83 [00:00<00:00, 127.61it/s] 63%|   | 52/83 [00:00<00:00, 127.57it/s] 78%|  | 65/83 [00:00<00:00, 127.31it/s] 94%|| 78/83 [00:00<00:00, 127.26it/s]100%|| 83/83 [00:00<00:00, 127.35it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:44:52,671 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:44:52,673 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:44:53,198 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 16%|        | 13/82 [00:00<00:00, 119.98it/s] 32%|      | 26/82 [00:00<00:00, 120.87it/s] 48%|     | 39/82 [00:00<00:00, 120.36it/s] 63%|   | 52/82 [00:00<00:00, 120.65it/s] 79%|  | 65/82 [00:00<00:00, 120.87it/s] 95%|| 78/82 [00:00<00:00, 121.08it/s]100%|| 82/82 [00:00<00:00, 120.85it/s]
2024-06-04:08:45:04,539 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:45:04,539 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:45:04,539 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:45:04,539 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:45:04,539 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:45:04,539 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:45:04,539 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:45:04,540 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 465, in <module>
    cli_evaluate()
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 384, in cli_evaluate
    results = xevaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/yang/GRIFFIN2/xevaluator.py", line 262, in simple_evaluate
    results = evaluate(
              ^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/yang/GRIFFIN2/xevaluator.py", line 407, in evaluate
    resps = getattr(lm, reqtype)(cloned_reqs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/yang/GRIFFIN2/xhuggingface.py", line 1187, in generate_until
    cont = self._model_generate(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/yang/GRIFFIN2/xhuggingface.py", line 747, in _model_generate
    outputs = self.model.generate(
              ^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/generation/utils.py", line 1544, in generate
    return self.greedy_search(
           ^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/yang/GRIFFIN2/llama10.py", line 1270, in greedy_search
    outputs = self(
              ^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/yang/GRIFFIN2/llama10.py", line 952, in forward
    logits = logits.float()
             ^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 602.00 MiB. GPU 1 has a total capacity of 31.74 GiB of which 541.38 MiB is free. Including non-PyTorch memory, this process has 31.21 GiB memory in use. Of the allocated memory 24.89 GiB is allocated by PyTorch, and 5.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-06-04 08:45:18,481] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 671882 closing signal SIGTERM
[2024-06-04 08:45:18,481] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 671884 closing signal SIGTERM
[2024-06-04 08:45:18,481] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 671885 closing signal SIGTERM
[2024-06-04 08:45:18,482] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 671886 closing signal SIGTERM
[2024-06-04 08:45:18,482] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 671887 closing signal SIGTERM
[2024-06-04 08:45:18,482] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 671888 closing signal SIGTERM
[2024-06-04 08:45:18,483] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 671889 closing signal SIGTERM
[2024-06-04 08:45:19,940] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 671883) of binary: /private/home/beidic/.conda/envs/griffin/bin/python
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_08:45:18
  host      : learnfair5044.h2.fair
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 671883)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
