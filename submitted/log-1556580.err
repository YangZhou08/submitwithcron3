Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.24s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.69s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  6.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.54s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.82s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.68s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.73s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:20<00:20, 20.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.88s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 11.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.37s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 11.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.22s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 11.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.57s/it]
Map (num_proc=16):   0%|          | 0/44540 [00:00<?, ? examples/s]Map (num_proc=16):   2%|▏         | 1000/44540 [00:03<02:26, 296.34 examples/s]Map (num_proc=16):   7%|▋         | 3000/44540 [00:03<00:38, 1091.08 examples/s]Map (num_proc=16):  11%|█         | 5000/44540 [00:03<00:20, 1901.10 examples/s]Map (num_proc=16):  13%|█▎        | 6000/44540 [00:03<00:16, 2319.64 examples/s]Map (num_proc=16):   0%|          | 0/44540 [00:00<?, ? examples/s]Map (num_proc=16):  16%|█▌        | 7000/44540 [00:04<00:18, 2082.26 examples/s]Map (num_proc=16):  20%|██        | 9000/44540 [00:04<00:10, 3294.38 examples/s]Map (num_proc=16):  25%|██▍       | 11000/44540 [00:04<00:07, 4661.70 examples/s]Map (num_proc=16):  27%|██▋       | 12000/44540 [00:05<00:08, 3851.66 examples/s]Map (num_proc=16):  29%|██▉       | 13000/44540 [00:05<00:10, 2899.84 examples/s]Map (num_proc=16):  31%|███▏      | 14000/44540 [00:06<00:10, 2902.47 examples/s]Map (num_proc=16):  34%|███▎      | 15000/44540 [00:06<00:09, 3151.67 examples/s]Map (num_proc=16):  36%|███▌      | 16000/44540 [00:06<00:07, 3761.76 examples/s]Map (num_proc=16):  38%|███▊      | 17000/44540 [00:06<00:06, 4474.54 examples/s]Map (num_proc=16):  45%|████▍     | 20000/44540 [00:07<00:03, 6599.19 examples/s]Map (num_proc=16):  47%|████▋     | 21000/44540 [00:07<00:03, 6585.23 examples/s]Map (num_proc=16):  49%|████▉     | 22000/44540 [00:07<00:03, 5843.45 examples/s]Map (num_proc=16):  52%|█████▏    | 23000/44540 [00:07<00:05, 4187.23 examples/s]Map (num_proc=16):  54%|█████▍    | 24000/44540 [00:07<00:04, 4879.42 examples/s]Map (num_proc=16):  58%|█████▊    | 26000/44540 [00:08<00:02, 7110.66 examples/s]Map (num_proc=16):  62%|██████▏   | 27784/44540 [00:08<00:02, 6241.45 examples/s]Map (num_proc=16):  65%|██████▍   | 28784/44540 [00:08<00:02, 6189.71 examples/s]Map (num_proc=16):  68%|██████▊   | 30351/44540 [00:08<00:02, 6387.44 examples/s]Map (num_proc=16):  70%|██████▉   | 31134/44540 [00:08<00:02, 6088.93 examples/s]Map (num_proc=16):  72%|███████▏  | 31918/44540 [00:09<00:03, 3999.67 examples/s]Map (num_proc=16):  77%|███████▋  | 34486/44540 [00:09<00:01, 6485.93 examples/s]Map (num_proc=16):  81%|████████  | 36053/44540 [00:09<00:01, 5819.13 examples/s]Map (num_proc=16):  83%|████████▎ | 36837/44540 [00:10<00:01, 5372.94 examples/s]Map (num_proc=16):   2%|▏         | 1000/44540 [00:05<04:21, 166.82 examples/s]Map (num_proc=16):   4%|▍         | 2000/44540 [00:06<01:49, 388.19 examples/s]Map (num_proc=16):  84%|████████▍ | 37621/44540 [00:10<00:01, 3770.63 examples/s]Map (num_proc=16):  11%|█         | 5000/44540 [00:06<00:35, 1127.33 examples/s]Map (num_proc=16):  13%|█▎        | 6000/44540 [00:07<00:29, 1310.17 examples/s]Map (num_proc=16):  88%|████████▊ | 39189/44540 [00:11<00:02, 2332.17 examples/s]Map (num_proc=16):  18%|█▊        | 8000/44540 [00:07<00:18, 2019.42 examples/s]Map (num_proc=16):  22%|██▏       | 10000/44540 [00:07<00:11, 2933.32 examples/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Map (num_proc=16):  29%|██▉       | 13000/44540 [00:08<00:09, 3449.29 examples/s]Map (num_proc=16):  36%|███▌      | 16000/44540 [00:08<00:06, 4429.82 examples/s]Map (num_proc=16):  38%|███▊      | 17000/44540 [00:09<00:06, 4213.27 examples/s]Map (num_proc=16):  40%|████      | 18000/44540 [00:09<00:08, 2983.55 examples/s]Map (num_proc=16):  47%|████▋     | 20783/44540 [00:10<00:05, 4038.15 examples/s]Map (num_proc=16):  54%|█████▍    | 24134/44540 [00:10<00:03, 6061.76 examples/s]Map (num_proc=16):  56%|█████▋    | 25134/44540 [00:10<00:03, 6295.50 examples/s]Map (num_proc=16):  60%|██████    | 26917/44540 [00:10<00:02, 5953.41 examples/s]Map (num_proc=16):  63%|██████▎   | 27917/44540 [00:11<00:03, 5093.13 examples/s]Map (num_proc=16):  64%|██████▍   | 28701/44540 [00:11<00:03, 4794.69 examples/s]Map (num_proc=16):  68%|██████▊   | 30485/44540 [00:11<00:02, 6334.34 examples/s]Map (num_proc=16):  73%|███████▎  | 32485/44540 [00:11<00:01, 7971.92 examples/s]Map (num_proc=16):  77%|███████▋  | 34268/44540 [00:11<00:01, 6339.08 examples/s]Map (num_proc=16):  90%|█████████ | 40189/44540 [00:16<00:07, 621.54 examples/s] Map (num_proc=16):  82%|████████▏ | 36620/44540 [00:12<00:01, 4841.05 examples/s]Map (num_proc=16):  95%|█████████▍| 42189/44540 [00:17<00:02, 1022.18 examples/s]Map (num_proc=16):  84%|████████▍ | 37404/44540 [00:13<00:01, 3944.80 examples/s]Map (num_proc=16):  86%|████████▌ | 38404/44540 [00:15<00:04, 1312.30 examples/s]Map (num_proc=16):  88%|████████▊ | 39404/44540 [00:16<00:03, 1361.84 examples/s]Map (num_proc=16):  96%|█████████▋| 42973/44540 [00:20<00:02, 605.34 examples/s] Map (num_proc=16):   0%|          | 0/44537 [00:00<?, ? examples/s]Map (num_proc=16):  91%|█████████ | 40404/44540 [00:16<00:02, 1634.61 examples/s]Map (num_proc=16):  98%|█████████▊| 43757/44540 [00:20<00:01, 734.48 examples/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Map (num_proc=16):  93%|█████████▎| 41404/44540 [00:16<00:01, 2019.89 examples/s]Map (num_proc=16): 100%|██████████| 44540/44540 [00:21<00:00, 866.17 examples/s]Map (num_proc=16): 100%|██████████| 44540/44540 [00:21<00:00, 2076.75 examples/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.63s/it]Map (num_proc=16):   2%|▏         | 1000/44537 [00:02<01:47, 406.14 examples/s]Map (num_proc=16):   4%|▍         | 2000/44537 [00:02<00:47, 903.07 examples/s]Map (num_proc=16):  95%|█████████▍| 42188/44540 [00:19<00:02, 908.21 examples/s] Map (num_proc=16):   7%|▋         | 3000/44537 [00:02<00:29, 1401.41 examples/s]Map (num_proc=16):  11%|█         | 5000/44537 [00:02<00:13, 2881.71 examples/s]Map (num_proc=16):  13%|█▎        | 6000/44537 [00:03<00:11, 3361.71 examples/s]Map (num_proc=16):  96%|█████████▋| 42972/44540 [00:19<00:01, 976.49 examples/s]Map (num_proc=16):  16%|█▌        | 7000/44537 [00:03<00:09, 3791.90 examples/s]Map (num_proc=16):  98%|█████████▊| 43756/44540 [00:20<00:00, 1206.33 examples/s]Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/bigmodeldatasetgeneration_savenone.py", line 241, in <module>
    large_model = LlamaForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", cache_dir = dir_models).to(torch.bfloat16).to(torch_device) 
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/modeling_utils.py", line 2303, in to
    return super().to(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in to
Map (num_proc=16):  20%|██        | 9000/44537 [00:03<00:06, 5204.39 examples/s]    return self._apply(convert)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 79.33 GiB of which 76.19 MiB is free. Process 445205 has 1.07 GiB memory in use. Process 445206 has 1.07 GiB memory in use. Including non-PyTorch memory, this process has 9.57 GiB memory in use. Process 445207 has 13.68 GiB memory in use. Process 445210 has 13.68 GiB memory in use. Process 445212 has 13.68 GiB memory in use. Process 445209 has 12.78 GiB memory in use. Process 445208 has 13.68 GiB memory in use. Of the allocated memory 9.11 GiB is allocated by PyTorch, and 49.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/bigmodeldatasetgeneration_savenone.py", line 241, in <module>
    large_model = LlamaForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf", cache_dir = dir_models).to(torch.bfloat16).to(torch_device) 
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/modeling_utils.py", line 2303, in to
    return super().to(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 79.33 GiB of which 42.19 MiB is free. Process 445205 has 1.07 GiB memory in use. Process 445206 has 1.07 GiB memory in use. Process 445211 has 9.57 GiB memory in use. Process 445207 has 13.68 GiB memory in use. Process 445210 has 13.68 GiB memory in use. Process 445212 has 13.68 GiB memory in use. Including non-PyTorch memory, this process has 12.81 GiB memory in use. Process 445208 has 13.68 GiB memory in use. Of the allocated memory 12.36 GiB is allocated by PyTorch, and 49.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Map (num_proc=16):  25%|██▍       | 11000/44537 [00:03<00:06, 5301.07 examples/s]Map (num_proc=16): 100%|██████████| 44540/44540 [00:20<00:00, 2174.52 examples/s]
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/bigmodeldatasetgeneration_savenone.py", line 323, in <module>
    large_outputs = large_model.generate(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/generation/utils.py", line 1619, in generate
    and torch.sum(inputs_tensor[:, -1] == generation_config.pad_token_id) > 0
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Map (num_proc=16):  31%|███▏      | 14000/44537 [00:04<00:04, 7021.65 examples/s]Map (num_proc=16):  34%|███▎      | 15000/44537 [00:04<00:04, 6879.09 examples/s]Map (num_proc=16):  36%|███▌      | 16000/44537 [00:04<00:04, 6548.94 examples/s]Map (num_proc=16):  38%|███▊      | 17000/44537 [00:05<00:06, 4288.06 examples/s]Map (num_proc=16):  40%|████      | 18000/44537 [00:05<00:06, 4204.74 examples/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Map (num_proc=16):  45%|████▍     | 20000/44537 [00:05<00:05, 4755.05 examples/s]Map (num_proc=16):  52%|█████▏    | 23000/44537 [00:05<00:03, 6477.61 examples/s]Map (num_proc=16):  56%|█████▌    | 25000/44537 [00:06<00:03, 6070.55 examples/s]Map (num_proc=16):  61%|██████    | 27000/44537 [00:06<00:02, 6898.47 examples/s]Map (num_proc=16):  65%|██████▍   | 28784/44537 [00:06<00:01, 8207.58 examples/s]Map (num_proc=16):  69%|██████▉   | 30784/44537 [00:06<00:01, 8547.74 examples/s]Map (num_proc=16):  73%|███████▎  | 32568/44537 [00:06<00:01, 9410.50 examples/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.65s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.94s/it]
Map (num_proc=16):  77%|███████▋  | 34351/44537 [00:07<00:01, 7182.05 examples/s]Map (num_proc=16):  81%|████████  | 35918/44537 [00:07<00:01, 7795.20 examples/s]Map (num_proc=16):  84%|████████▍ | 37485/44537 [00:07<00:00, 7798.13 examples/s]Map (num_proc=16):  89%|████████▉ | 39836/44537 [00:07<00:00, 9309.76 examples/s]Map (num_proc=16):  93%|█████████▎| 41403/44537 [00:08<00:00, 7610.53 examples/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Map (num_proc=16):  96%|█████████▋| 42971/44537 [00:08<00:00, 7501.33 examples/s]Map (num_proc=16): 100%|██████████| 44537/44537 [00:08<00:00, 6984.81 examples/s]Map (num_proc=16): 100%|██████████| 44537/44537 [00:08<00:00, 5080.38 examples/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.20s/it]Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/bigmodeldatasetgeneration_savenone.py", line 323, in <module>
    large_outputs = large_model.generate(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/generation/utils.py", line 1801, in generate
    return self.sample(
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/generation/utils.py", line 2918, in sample
    outputs = self(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 1492, in forward
    outputs = self.model(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 1131, in forward
    layer_outputs = decoder_layer(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 857, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 555, in forward
    value_states = torch.cat([past_key_value[1], value_states], dim=2)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 158.00 MiB. GPU 0 has a total capacty of 79.33 GiB of which 98.31 MiB is free. Process 445205 has 1.07 GiB memory in use. Process 445206 has 1.07 GiB memory in use. Including non-PyTorch memory, this process has 34.09 GiB memory in use. Process 445210 has 29.29 GiB memory in use. Process 445212 has 13.68 GiB memory in use. Of the allocated memory 32.20 GiB is allocated by PyTorch, and 1.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/bigmodeldatasetgeneration_savenone.py", line 323, in <module>
    large_outputs = large_model.generate(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/generation/utils.py", line 1801, in generate
    return self.sample(
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/generation/utils.py", line 2918, in sample
    outputs = self(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 1492, in forward
    outputs = self.model(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 1131, in forward
    layer_outputs = decoder_layer(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 857, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 555, in forward
    value_states = torch.cat([past_key_value[1], value_states], dim=2)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacty of 79.33 GiB of which 92.31 MiB is free. Process 445205 has 1.07 GiB memory in use. Process 445206 has 1.07 GiB memory in use. Process 445207 has 34.09 GiB memory in use. Including non-PyTorch memory, this process has 29.29 GiB memory in use. Process 445212 has 13.68 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/bigmodeldatasetgeneration_savenone.py", line 323, in <module>
    large_outputs = large_model.generate(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/generation/utils.py", line 1801, in generate
    return self.sample(
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/generation/utils.py", line 2918, in sample
    outputs = self(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 1492, in forward
    outputs = self.model(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 1088, in forward
    inputs_embeds = self.embed_tokens(input_ids)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/functional.py", line 2233, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 79.33 GiB of which 16.31 MiB is free. Process 445205 has 1.07 GiB memory in use. Process 445206 has 1.07 GiB memory in use. Process 445207 has 34.09 GiB memory in use. Process 445210 has 29.29 GiB memory in use. Including non-PyTorch memory, this process has 13.76 GiB memory in use. Of the allocated memory 13.23 GiB is allocated by PyTorch, and 48.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  5.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.62s/it]
Map (num_proc=16):   0%|          | 0/44540 [00:00<?, ? examples/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Map (num_proc=16):   2%|▏         | 1000/44540 [00:01<00:59, 733.79 examples/s]Map (num_proc=16):   4%|▍         | 2000/44540 [00:01<00:27, 1537.32 examples/s]Map (num_proc=16):   9%|▉         | 4000/44540 [00:01<00:14, 2791.30 examples/s]Map (num_proc=16):  11%|█         | 5000/44540 [00:02<00:11, 3361.79 examples/s]Map (num_proc=16):  22%|██▏       | 10000/44540 [00:02<00:03, 8954.30 examples/s]Map (num_proc=16):  34%|███▎      | 15000/44540 [00:02<00:02, 14510.98 examples/s]Map (num_proc=16):  40%|████      | 18000/44540 [00:02<00:02, 8867.03 examples/s] Map (num_proc=16):  45%|████▍     | 20000/44540 [00:03<00:03, 7122.49 examples/s]Map (num_proc=16):  49%|████▉     | 21784/44540 [00:03<00:03, 7295.93 examples/s]Map (num_proc=16):  53%|█████▎    | 23784/44540 [00:03<00:03, 6909.98 examples/s]Map (num_proc=16):  60%|█████▉    | 26568/44540 [00:04<00:01, 9134.82 examples/s]Map (num_proc=16):  66%|██████▋   | 29568/44540 [00:04<00:01, 11725.40 examples/s]Map (num_proc=16):  73%|███████▎  | 32352/44540 [00:04<00:00, 13201.45 examples/s]Map (num_proc=16):  79%|███████▉  | 35136/44540 [00:04<00:00, 11904.68 examples/s]Map (num_proc=16):  82%|████████▏ | 36704/44540 [00:05<00:00, 8301.66 examples/s] Map (num_proc=16):  88%|████████▊ | 39054/44540 [00:05<00:00, 8658.64 examples/s]Map (num_proc=16):  91%|█████████ | 40622/44540 [00:05<00:00, 9349.20 examples/s]Map (num_proc=16): 100%|██████████| 44540/44540 [00:05<00:00, 9461.61 examples/s]Map (num_proc=16): 100%|██████████| 44540/44540 [00:05<00:00, 7490.85 examples/s]
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/bigmodeldatasetgeneration_savenone.py", line 323, in <module>
    large_outputs = large_model.generate(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/generation/utils.py", line 1801, in generate
    return self.sample(
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/generation/utils.py", line 2918, in sample
    outputs = self(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 1492, in forward
    outputs = self.model(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 1131, in forward
    layer_outputs = decoder_layer(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 857, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 554, in forward
    key_states = torch.cat([past_key_value[0], key_states], dim=2)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 210.00 MiB. GPU 0 has a total capacty of 79.33 GiB of which 24.44 MiB is free. Including non-PyTorch memory, this process has 41.81 GiB memory in use. Process 445206 has 35.33 GiB memory in use. Of the allocated memory 35.78 GiB is allocated by PyTorch, and 5.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/bigmodeldatasetgeneration_savenone.py", line 323, in <module>
    large_outputs = large_model.generate(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/generation/utils.py", line 1801, in generate
    return self.sample(
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/generation/utils.py", line 2918, in sample
    outputs = self(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 1492, in forward
    outputs = self.model(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 1131, in forward
    layer_outputs = decoder_layer(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 857, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 555, in forward
    value_states = torch.cat([past_key_value[1], value_states], dim=2)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 516.00 MiB. GPU 0 has a total capacty of 79.33 GiB of which 319.81 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 77.21 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
