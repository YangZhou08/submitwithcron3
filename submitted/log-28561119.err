ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
datasets 2.19.1 requires fsspec[http]<=2024.3.1,>=2023.1.0, but you have fsspec 2024.5.0 which is incompatible.
Already on 'yangexp2'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/huggingface-cli", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py", line 51, in main
    service.run()
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/user.py", line 98, in run
    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 111, in login
    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 307, in _login
    raise ValueError("Invalid token passed!")
ValueError: Invalid token passed!
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-03:23:59:27,148 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:23:59:27,148 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:23:59:27,149 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:23:59:27,149 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:23:59:27,150 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:23:59:27,150 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:23:59:27,183 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:23:59:27,510 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:23:59:36,460 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:23:59:36,460 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:23:59:36,460 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:23:59:36,460 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:23:59:36,460 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:23:59:36,460 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:23:59:36,474 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:23:59:36,474 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:23:59:36,474 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:23:59:36,474 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:23:59:36,474 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:23:59:36,474 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:23:59:36,474 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'griffin': False, 'check': False}
2024-06-03:23:59:36,474 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'griffin': False, 'check': False}
2024-06-03:23:59:36,474 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'griffin': False, 'check': False}
2024-06-03:23:59:36,474 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'griffin': False, 'check': False}
2024-06-03:23:59:36,474 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'griffin': False, 'check': False}
2024-06-03:23:59:36,474 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'griffin': False, 'check': False}
2024-06-03:23:59:37,464 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:23:59:37,472 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:23:59:37,473 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'griffin': False, 'check': False}
2024-06-03:23:59:37,902 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:23:59:37,910 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:23:59:37,910 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'griffin': False, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [01:02<02:04, 62.49s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [01:04<02:09, 64.71s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [01:04<02:08, 64.25s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [01:04<02:08, 64.34s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [01:04<02:08, 64.36s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [01:03<02:07, 63.54s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [01:04<02:09, 64.58s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [01:03<02:07, 63.89s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:03<01:01, 61.43s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:04<01:01, 61.96s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:05<01:02, 62.65s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:04<01:01, 61.96s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:05<01:02, 62.59s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:05<01:02, 62.38s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:05<01:02, 62.33s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [02:05<01:02, 62.39s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:43<00:00, 51.23s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:43<00:00, 54.43s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:43<00:00, 51.47s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:43<00:00, 54.46s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:44<00:00, 51.86s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:44<00:00, 54.76s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:44<00:00, 51.94s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:44<00:00, 54.79s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:43<00:00, 51.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:43<00:00, 54.54s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:44<00:00, 51.85s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:44<00:00, 54.96s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:44<00:00, 51.74s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:44<00:00, 54.81s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [02:45<00:00, 52.32s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:45<00:00, 55.25s/it]
[2024-06-04 00:02:24,328] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 105614 closing signal SIGTERM
[2024-06-04 00:02:24,328] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 105615 closing signal SIGTERM
[2024-06-04 00:02:24,328] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 105617 closing signal SIGTERM
[2024-06-04 00:02:24,328] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 105618 closing signal SIGTERM
[2024-06-04 00:02:24,329] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 105619 closing signal SIGTERM
[2024-06-04 00:02:24,329] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 105620 closing signal SIGTERM
[2024-06-04 00:02:24,329] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 105621 closing signal SIGTERM
[2024-06-04 00:02:25,409] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -7) local_rank: 2 (pid: 105616) of binary: /private/home/beidic/.conda/envs/griffin/bin/python
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
main.py FAILED
------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_00:02:24
  host      : learnfair5044.h2.fair
  rank      : 2 (local_rank: 2)
  exitcode  : -7 (pid: 105616)
  error_file: <N/A>
  traceback : Signal 7 (SIGBUS) received by PID 105616
======================================================
/var/spool/slurm//job28561119/slurm_script: line 64: 105591 Bus error               (core dumped) accelerate launch --main_process_port 29510 --num_processes 8 --num_machines 1 main.py --model xhf --model_args pretrained=meta-llama/Llama-2-13b-hf,griffin=False,check=False --tasks gsm8k --batch_size 1
