Already on 'yangexp2'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/huggingface-cli", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py", line 51, in main
    service.run()
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/user.py", line 98, in run
    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 111, in login
    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 307, in _login
    raise ValueError("Invalid token passed!")
ValueError: Invalid token passed!
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:00:00:15,628 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:00:00:15,628 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:00:00:15,701 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:00:00:15,729 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:00:00:15,842 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:00:00:15,908 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:00:00:15,998 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:00:00:16,111 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:00:00:20,869 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:00:00:20,874 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:00:00:20,874 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-06-04:00:00:20,956 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:00:00:20,958 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:00:00:20,963 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:00:00:20,963 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:00:00:20,963 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-06-04:00:00:20,963 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-06-04:00:00:21,787 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:00:00:21,791 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:00:00:21,791 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]2024-06-04:00:00:22,520 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:00:00:22,526 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:00:00:22,526 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-06-04:00:00:22,527 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:00:00:22,536 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:00:00:22,536 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-06-04:00:00:22,887 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:00:00:22,891 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:00:00:22,891 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:00:00:23,041 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:00:00:23,045 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:00:00:23,045 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Llama-2-13b-hf', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:06,  3.45s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:06,  3.46s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:09,  4.85s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:09,  4.74s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:09,  4.72s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:09,  4.80s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:08,  4.49s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:09,  4.61s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:06<00:03,  3.38s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:06<00:03,  3.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.75s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.92s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.78s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.95s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:09<00:04,  4.56s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:09<00:04,  4.53s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:09<00:04,  4.67s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:08<00:04,  4.41s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:09<00:04,  4.69s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.34s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.70s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:09<00:04,  4.56s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.80s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  3.79s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.05s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  3.80s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.04s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.68s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.88s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.69s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.93s/it]
[2024-06-04 00:01:29,689] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 122666 closing signal SIGTERM
[2024-06-04 00:01:29,689] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 122667 closing signal SIGTERM
[2024-06-04 00:01:29,689] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 122668 closing signal SIGTERM
[2024-06-04 00:01:29,690] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 122670 closing signal SIGTERM
[2024-06-04 00:01:29,690] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 122671 closing signal SIGTERM
[2024-06-04 00:01:29,690] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 122672 closing signal SIGTERM
[2024-06-04 00:01:29,690] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 122673 closing signal SIGTERM
[2024-06-04 00:01:30,466] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -7) local_rank: 3 (pid: 122669) of binary: /private/home/beidic/.conda/envs/griffin/bin/python
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
main.py FAILED
------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_00:01:29
  host      : learnfair7671.h2.fair
  rank      : 3 (local_rank: 3)
  exitcode  : -7 (pid: 122669)
  error_file: <N/A>
  traceback : Signal 7 (SIGBUS) received by PID 122669
======================================================
/var/spool/slurm//job28561116/slurm_script: line 63: 122642 Bus error               (core dumped) accelerate launch --main_process_port 29510 --num_processes 8 --num_machines 1 main.py --model xhf --model_args pretrained=meta-llama/Llama-2-13b-hf,cats=True,check=True,kernel_size=16,spr=0.5,thr=0.1 --tasks gsm8k --batch_size 1
