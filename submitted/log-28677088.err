Already on 'yangexp2'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/huggingface-cli", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py", line 51, in main
    service.run()
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/user.py", line 98, in run
    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 111, in login
    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 307, in _login
    raise ValueError("Invalid token passed!")
ValueError: Invalid token passed!
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-06:11:35:18,972 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:18,972 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:18,972 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:18,973 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:18,973 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:19,482 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:19,506 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:19,673 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:26,641 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:26,642 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:26,644 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:26,646 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:26,653 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:26,654 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-06:11:35:26,654 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:26,654 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-06:11:35:26,699 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:26,700 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:26,704 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:26,704 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-06:11:35:26,831 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:26,832 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:26,835 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:26,835 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-06:11:35:26,904 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:26,905 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:26,913 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:26,913 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.05}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-06:11:35:30,736 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:30,738 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:30,748 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:30,748 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-06:11:35:31,093 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:31,094 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:31,100 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:31,100 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-06:11:35:31,101 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:31,102 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:31,106 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:31,106 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.05}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:26, 28.70s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:29<01:29, 29.68s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:30<01:30, 30.05s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:30<01:30, 30.08s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:29<01:29, 29.72s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:18, 26.14s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:17, 25.67s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:17, 25.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:58<00:58, 29.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:00<01:00, 30.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:59<00:59, 29.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:00<01:00, 30.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:59<00:59, 29.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:55<00:56, 28.17s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:56<00:56, 28.45s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:55<00:56, 28.19s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:29<00:29, 29.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:30<00:30, 30.16s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:30<00:30, 30.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:30<00:30, 30.34s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:30<00:30, 30.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:25<00:29, 29.14s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:26<00:29, 29.27s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:26<00:29, 29.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:35<00:00, 20.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:35<00:00, 23.87s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 20.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 24.09s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 20.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 24.09s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 20.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 24.11s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 20.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 24.00s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 19.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 22.92s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 20.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 23.07s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 20.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 22.99s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:37:56,887 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:37:56,892 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:37:57,255 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 18%|█▊        | 15/82 [00:00<00:00, 147.37it/s] 37%|███▋      | 30/82 [00:00<00:00, 148.14it/s] 55%|█████▍    | 45/82 [00:00<00:00, 147.84it/s] 73%|███████▎  | 60/82 [00:00<00:00, 147.49it/s] 91%|█████████▏| 75/82 [00:00<00:00, 147.28it/s]100%|██████████| 82/82 [00:00<00:00, 147.45it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:38:02,524 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:02,526 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:02,853 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 108.74it/s] 27%|██▋       | 22/83 [00:00<00:00, 109.28it/s] 41%|████      | 34/83 [00:00<00:00, 109.78it/s] 54%|█████▍    | 45/83 [00:00<00:00, 109.25it/s] 67%|██████▋   | 56/83 [00:00<00:00, 108.73it/s] 81%|████████  | 67/83 [00:00<00:00, 108.91it/s] 94%|█████████▍| 78/83 [00:00<00:00, 108.81it/s]100%|██████████| 83/83 [00:00<00:00, 108.97it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:11:38:12,477 INFO     [xhuggingface.py:315] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:38:12,552 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:12,555 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:12,907 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 145.76it/s] 37%|███▋      | 31/83 [00:00<00:00, 148.63it/s] 57%|█████▋    | 47/83 [00:00<00:00, 149.32it/s] 76%|███████▌  | 63/83 [00:00<00:00, 149.74it/s] 95%|█████████▌| 79/83 [00:00<00:00, 150.24it/s]100%|██████████| 83/83 [00:00<00:00, 149.59it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:38:17,756 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:17,758 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:18,098 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 104.07it/s] 27%|██▋       | 22/82 [00:00<00:00, 104.02it/s] 40%|████      | 33/82 [00:00<00:00, 103.88it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:38:18,530 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:18,533 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 54%|█████▎    | 44/82 [00:00<00:00, 103.96it/s] 67%|██████▋   | 55/82 [00:00<00:00, 103.87it/s] 80%|████████  | 66/82 [00:00<00:00, 103.85it/s] 94%|█████████▍| 77/82 [00:00<00:00, 103.71it/s]2024-06-06:11:38:18,895 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
100%|██████████| 82/82 [00:00<00:00, 103.76it/s]
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 101.11it/s] 27%|██▋       | 22/82 [00:00<00:00, 87.39it/s]  40%|████      | 33/82 [00:00<00:00, 93.89it/s] 54%|█████▎    | 44/82 [00:00<00:00, 98.05it/s] 67%|██████▋   | 55/82 [00:00<00:00, 100.45it/s] 80%|████████  | 66/82 [00:00<00:00, 101.69it/s] 94%|█████████▍| 77/82 [00:00<00:00, 102.42it/s]100%|██████████| 82/82 [00:00<00:00, 99.77it/s] 
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:38:20,906 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:20,908 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:21,319 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 11%|█         | 9/83 [00:00<00:00, 83.57it/s] 22%|██▏       | 18/83 [00:00<00:00, 84.49it/s] 33%|███▎      | 27/83 [00:00<00:00, 84.76it/s] 43%|████▎     | 36/83 [00:00<00:00, 84.81it/s] 54%|█████▍    | 45/83 [00:00<00:00, 84.59it/s] 65%|██████▌   | 54/83 [00:00<00:00, 84.68it/s] 76%|███████▌  | 63/83 [00:00<00:00, 84.79it/s] 87%|████████▋ | 72/83 [00:00<00:00, 84.74it/s] 98%|█████████▊| 81/83 [00:00<00:00, 84.59it/s]100%|██████████| 83/83 [00:00<00:00, 84.58it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:38:30,368 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:30,370 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:11:38:30,699 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:38:30,770 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:30,773 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 16%|█▌        | 13/82 [00:00<00:00, 124.98it/s] 32%|███▏      | 26/82 [00:00<00:00, 106.53it/s] 45%|████▌     | 37/82 [00:00<00:00, 102.58it/s]2024-06-06:11:38:31,168 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
 59%|█████▊    | 48/82 [00:00<00:00, 101.17it/s]  0%|          | 0/83 [00:00<?, ?it/s] 72%|███████▏  | 59/82 [00:00<00:00, 100.50it/s] 12%|█▏        | 10/83 [00:00<00:00, 99.99it/s] 24%|██▍       | 20/83 [00:00<00:00, 99.41it/s] 85%|████████▌ | 70/82 [00:00<00:00, 99.94it/s]  36%|███▌      | 30/83 [00:00<00:00, 99.57it/s] 99%|█████████▉| 81/82 [00:00<00:00, 100.00it/s]100%|██████████| 82/82 [00:00<00:00, 101.87it/s]
 48%|████▊     | 40/83 [00:00<00:00, 99.18it/s] 60%|██████    | 50/83 [00:00<00:00, 97.91it/s] 72%|███████▏  | 60/83 [00:00<00:00, 96.90it/s] 84%|████████▍ | 70/83 [00:00<00:00, 94.58it/s] 98%|█████████▊| 81/83 [00:00<00:00, 98.87it/s]100%|██████████| 83/83 [00:00<00:00, 98.37it/s]
2024-06-06:11:38:35,280 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:11:38:35,280 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:11:38:35,280 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:11:38:35,280 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:11:38:35,280 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:11:38:35,280 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:11:38:35,280 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-06:11:38:35,281 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:11<15:45, 11.53s/it]Running generate_until requests:   2%|▏         | 2/83 [00:24<16:50, 12.47s/it]Running generate_until requests:   4%|▎         | 3/83 [00:32<13:52, 10.41s/it]Running generate_until requests:   5%|▍         | 4/83 [00:37<10:57,  8.33s/it]Running generate_until requests:   6%|▌         | 5/83 [00:46<10:53,  8.38s/it]Running generate_until requests:   7%|▋         | 6/83 [00:52<09:47,  7.63s/it]Running generate_until requests:   8%|▊         | 7/83 [01:01<10:14,  8.09s/it]Running generate_until requests:  10%|▉         | 8/83 [01:08<09:48,  7.84s/it]Running generate_until requests:  11%|█         | 9/83 [01:18<10:21,  8.40s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:24<09:15,  7.62s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:31<09:03,  7.55s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:42<10:15,  8.67s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:47<08:46,  7.53s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:54<08:32,  7.43s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:06<09:53,  8.72s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:21<11:48, 10.58s/it]Running generate_until requests:  20%|██        | 17/83 [02:31<11:32, 10.50s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:41<11:01, 10.18s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:51<10:53, 10.21s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:04<11:30, 10.96s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:11<10:08,  9.82s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:19<09:28,  9.32s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:28<09:18,  9.31s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:36<08:46,  8.92s/it]Running generate_until requests:  30%|███       | 25/83 [03:44<08:13,  8.51s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:53<08:06,  8.54s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:05<09:01,  9.66s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:13<08:25,  9.19s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:21<07:54,  8.78s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:34<08:57, 10.15s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:43<08:31,  9.84s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:53<08:15,  9.71s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:58<07:04,  8.50s/it]Running generate_until requests:  41%|████      | 34/83 [05:12<08:11, 10.03s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:19<07:26,  9.30s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:26<06:34,  8.39s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:34<06:23,  8.34s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:50<07:54, 10.55s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:56<06:53,  9.41s/it]Running generate_until requests:  48%|████▊     | 40/83 [06:04<06:18,  8.81s/it]Running generate_until requests:  49%|████▉     | 41/83 [06:10<05:34,  7.97s/it]Running generate_until requests:  51%|█████     | 42/83 [06:18<05:29,  8.02s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:29<05:52,  8.80s/it]Running generate_until requests:  53%|█████▎    | 44/83 [06:35<05:16,  8.11s/it]Running generate_until requests:  54%|█████▍    | 45/83 [06:43<05:05,  8.03s/it]Running generate_until requests:  55%|█████▌    | 46/83 [06:51<04:59,  8.09s/it]Running generate_until requests:  57%|█████▋    | 47/83 [06:57<04:30,  7.50s/it]Running generate_until requests:  58%|█████▊    | 48/83 [07:07<04:43,  8.09s/it]Running generate_until requests:  59%|█████▉    | 49/83 [07:12<04:06,  7.24s/it]Running generate_until requests:  60%|██████    | 50/83 [07:22<04:29,  8.16s/it]Running generate_until requests:  61%|██████▏   | 51/83 [07:31<04:25,  8.29s/it]Running generate_until requests:  63%|██████▎   | 52/83 [07:38<04:10,  8.07s/it]Running generate_until requests:  64%|██████▍   | 53/83 [07:49<04:25,  8.84s/it]Running generate_until requests:  65%|██████▌   | 54/83 [07:55<03:50,  7.95s/it]Running generate_until requests:  66%|██████▋   | 55/83 [08:01<03:29,  7.49s/it]Running generate_until requests:  67%|██████▋   | 56/83 [08:20<04:53, 10.87s/it]Running generate_until requests:  69%|██████▊   | 57/83 [08:31<04:42, 10.88s/it]Running generate_until requests:  70%|██████▉   | 58/83 [08:40<04:20, 10.42s/it]Running generate_until requests:  71%|███████   | 59/83 [08:47<03:44,  9.33s/it]Running generate_until requests:  72%|███████▏  | 60/83 [08:54<03:13,  8.42s/it]Running generate_until requests:  73%|███████▎  | 61/83 [09:04<03:15,  8.90s/it]Running generate_until requests:  75%|███████▍  | 62/83 [09:11<02:56,  8.42s/it]Running generate_until requests:  76%|███████▌  | 63/83 [09:18<02:40,  8.04s/it]Running generate_until requests:  77%|███████▋  | 64/83 [09:27<02:36,  8.24s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:34<02:23,  7.96s/it]Running generate_until requests:  80%|███████▉  | 66/83 [09:41<02:10,  7.66s/it]Running generate_until requests:  81%|████████  | 67/83 [09:46<01:47,  6.73s/it]Running generate_until requests:  82%|████████▏ | 68/83 [09:52<01:41,  6.75s/it]Running generate_until requests:  83%|████████▎ | 69/83 [10:03<01:51,  7.95s/it]Running generate_until requests:  84%|████████▍ | 70/83 [10:13<01:52,  8.67s/it]Running generate_until requests:  86%|████████▌ | 71/83 [10:20<01:34,  7.89s/it]Running generate_until requests:  87%|████████▋ | 72/83 [10:28<01:27,  7.95s/it]Running generate_until requests:  88%|████████▊ | 73/83 [10:35<01:17,  7.79s/it]Running generate_until requests:  89%|████████▉ | 74/83 [10:43<01:09,  7.70s/it]Running generate_until requests:  90%|█████████ | 75/83 [10:55<01:12,  9.06s/it]Running generate_until requests:  92%|█████████▏| 76/83 [11:04<01:03,  9.09s/it]Running generate_until requests:  93%|█████████▎| 77/83 [11:10<00:48,  8.16s/it]Running generate_until requests:  94%|█████████▍| 78/83 [11:17<00:38,  7.78s/it]Running generate_until requests:  95%|█████████▌| 79/83 [11:28<00:35,  8.85s/it]Running generate_until requests:  96%|█████████▋| 80/83 [11:36<00:25,  8.50s/it]Running generate_until requests:  98%|█████████▊| 81/83 [11:45<00:17,  8.85s/it]Running generate_until requests:  99%|█████████▉| 82/83 [11:51<00:07,  7.99s/it]Running generate_until requests: 100%|██████████| 83/83 [11:58<00:00,  7.51s/it]Running generate_until requests: 100%|██████████| 83/83 [11:58<00:00,  8.65s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-06:12:05:08,559 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:05:08,560 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:05:08,574 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:05:08,619 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:05:08,732 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:05:09,230 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:05:09,266 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:05:09,554 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:05:15,197 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:05:15,197 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:05:15,201 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:05:15,201 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-06:12:05:15,302 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:05:15,303 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:05:15,307 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:05:15,308 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-06:12:05:15,683 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:05:15,684 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:05:15,688 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:05:15,688 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-06:12:05:15,828 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:05:15,829 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:05:15,834 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:05:15,835 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-06:12:05:15,929 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:05:15,930 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:05:15,937 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:05:15,937 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.33s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.04s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.78s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.19s/it]2024-06-06:12:05:19,540 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:05:19,541 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:05:19,546 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:05:19,546 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-06:12:05:19,834 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:05:19,837 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:05:19,846 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:05:19,846 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-06:12:05:20,057 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:05:20,058 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:05:20,062 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:05:20,062 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.82s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.27s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.04s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.32s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.83s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.62s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.81s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.16s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.83s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.44s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.14s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.95s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.24s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.23s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.87s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.41s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.39s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:12:06:15,619 INFO     [xhuggingface.py:315] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:06:15,698 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:15,699 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:15,936 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 148.37it/s] 36%|███▌      | 30/83 [00:00<00:00, 149.09it/s] 54%|█████▍    | 45/83 [00:00<00:00, 149.06it/s] 72%|███████▏  | 60/83 [00:00<00:00, 149.18it/s] 90%|█████████ | 75/83 [00:00<00:00, 149.09it/s]100%|██████████| 83/83 [00:00<00:00, 149.12it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:06:20,300 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:20,302 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:20,640 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 104.02it/s] 30%|███       | 25/82 [00:00<00:00, 121.48it/s] 49%|████▉     | 40/82 [00:00<00:00, 133.59it/s] 66%|██████▌   | 54/82 [00:00<00:00, 135.29it/s] 83%|████████▎ | 68/82 [00:00<00:00, 136.37it/s]100%|██████████| 82/82 [00:00<00:00, 136.98it/s]100%|██████████| 82/82 [00:00<00:00, 133.21it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:06:49,073 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:49,075 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:49,396 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 107.86it/s] 30%|███       | 25/83 [00:00<00:00, 121.75it/s] 46%|████▌     | 38/83 [00:00<00:00, 113.30it/s] 60%|██████    | 50/83 [00:00<00:00, 108.50it/s] 73%|███████▎  | 61/83 [00:00<00:00, 108.42it/s] 87%|████████▋ | 72/83 [00:00<00:00, 108.68it/s]100%|██████████| 83/83 [00:00<00:00, 110.22it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:06:50,517 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:50,519 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:06:50,852 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:50,854 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:50,865 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 102.34it/s] 27%|██▋       | 22/82 [00:00<00:00, 102.81it/s]2024-06-06:12:06:51,105 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 40%|████      | 33/82 [00:00<00:00, 103.26it/s] 17%|█▋        | 14/83 [00:00<00:00, 131.41it/s] 54%|█████▎    | 44/82 [00:00<00:00, 103.38it/s] 34%|███▎      | 28/83 [00:00<00:00, 131.92it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 67%|██████▋   | 55/82 [00:00<00:00, 103.12it/s] 51%|█████     | 42/83 [00:00<00:00, 133.45it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:06:51,515 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:51,518 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 80%|████████  | 66/82 [00:00<00:00, 103.41it/s] 67%|██████▋   | 56/83 [00:00<00:00, 131.52it/s] 94%|█████████▍| 77/82 [00:00<00:00, 103.32it/s]100%|██████████| 82/82 [00:00<00:00, 103.26it/s]
 84%|████████▍ | 70/83 [00:00<00:00, 117.07it/s] 99%|█████████▉| 82/83 [00:00<00:00, 109.77it/s]100%|██████████| 83/83 [00:00<00:00, 118.08it/s]
2024-06-06:12:06:51,922 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 130.86it/s] 34%|███▍      | 28/82 [00:00<00:00, 131.90it/s] 51%|█████     | 42/82 [00:00<00:00, 131.69it/s] 68%|██████▊   | 56/82 [00:00<00:00, 131.98it/s] 85%|████████▌ | 70/82 [00:00<00:00, 131.98it/s]100%|██████████| 82/82 [00:00<00:00, 132.03it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:06:54,247 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:54,250 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:54,570 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 109.13it/s] 27%|██▋       | 22/82 [00:00<00:00, 107.78it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:06:54,888 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:54,892 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 44%|████▍     | 36/82 [00:00<00:00, 122.13it/s] 60%|█████▉    | 49/82 [00:00<00:00, 114.49it/s] 74%|███████▍  | 61/82 [00:00<00:00, 109.06it/s] 88%|████████▊ | 72/82 [00:00<00:00, 106.11it/s]100%|██████████| 82/82 [00:00<00:00, 108.28it/s]
2024-06-06:12:06:55,374 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 10%|▉         | 8/83 [00:00<00:01, 74.79it/s] 19%|█▉        | 16/83 [00:00<00:00, 68.98it/s] 29%|██▉       | 24/83 [00:00<00:00, 71.62it/s] 39%|███▊      | 32/83 [00:00<00:00, 70.49it/s] 52%|█████▏    | 43/83 [00:00<00:00, 80.44it/s] 63%|██████▎   | 52/83 [00:00<00:00, 76.80it/s] 72%|███████▏  | 60/83 [00:00<00:00, 71.37it/s] 82%|████████▏ | 68/83 [00:00<00:00, 67.99it/s] 90%|█████████ | 75/83 [00:01<00:00, 65.54it/s] 99%|█████████▉| 82/83 [00:01<00:00, 65.88it/s]100%|██████████| 83/83 [00:01<00:00, 69.51it/s]
2024-06-06:12:07:01,087 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:07:01,087 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:07:01,087 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:07:01,087 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:07:01,087 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:07:01,087 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:07:01,088 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-06:12:07:01,105 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:13<18:48, 13.77s/it]Running generate_until requests:   2%|▏         | 2/83 [00:26<18:03, 13.38s/it]Running generate_until requests:   4%|▎         | 3/83 [00:34<14:30, 10.89s/it]Running generate_until requests:   5%|▍         | 4/83 [00:39<11:18,  8.59s/it]Running generate_until requests:   6%|▌         | 5/83 [00:48<11:05,  8.53s/it]Running generate_until requests:   7%|▋         | 6/83 [00:54<09:55,  7.73s/it]Running generate_until requests:   8%|▊         | 7/83 [01:03<10:17,  8.12s/it]Running generate_until requests:  10%|▉         | 8/83 [01:10<09:50,  7.87s/it]Running generate_until requests:  11%|█         | 9/83 [01:20<10:22,  8.41s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:26<09:16,  7.62s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:33<09:00,  7.51s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:44<10:01,  8.47s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:48<08:27,  7.25s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:55<08:19,  7.23s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:07<09:42,  8.56s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:19<10:46,  9.65s/it]Running generate_until requests:  20%|██        | 17/83 [02:28<10:14,  9.30s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:35<09:29,  8.77s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:44<09:23,  8.80s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:57<10:44, 10.23s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:05<09:35,  9.28s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:11<08:37,  8.48s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:19<08:12,  8.20s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:25<07:33,  7.68s/it]Running generate_until requests:  30%|███       | 25/83 [03:31<06:57,  7.19s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:38<06:44,  7.10s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:48<07:21,  7.88s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:55<06:57,  7.59s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:01<06:28,  7.19s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:12<07:14,  8.20s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:19<07:00,  8.09s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:28<07:07,  8.38s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:34<06:15,  7.52s/it]Running generate_until requests:  41%|████      | 34/83 [04:46<07:20,  8.99s/it]Running generate_until requests:  42%|████▏     | 35/83 [04:53<06:30,  8.15s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:59<06:04,  7.76s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:07<05:51,  7.65s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:19<06:46,  9.04s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:25<05:57,  8.13s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:32<05:39,  7.89s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:39<05:10,  7.39s/it]Running generate_until requests:  51%|█████     | 42/83 [05:47<05:09,  7.55s/it]Running generate_until requests:  52%|█████▏    | 43/83 [05:57<05:34,  8.37s/it]Running generate_until requests:  53%|█████▎    | 44/83 [06:03<05:04,  7.80s/it]Running generate_until requests:  54%|█████▍    | 45/83 [06:11<04:58,  7.84s/it]Running generate_until requests:  55%|█████▌    | 46/83 [06:19<04:52,  7.90s/it]Running generate_until requests:  57%|█████▋    | 47/83 [06:25<04:25,  7.38s/it]Running generate_until requests:  58%|█████▊    | 48/83 [06:35<04:41,  8.05s/it]Running generate_until requests:  59%|█████▉    | 49/83 [06:40<04:04,  7.20s/it]Running generate_until requests:  60%|██████    | 50/83 [06:51<04:29,  8.16s/it]Running generate_until requests:  61%|██████▏   | 51/83 [06:59<04:25,  8.29s/it]Running generate_until requests:  63%|██████▎   | 52/83 [07:06<04:00,  7.77s/it]Running generate_until requests:  64%|██████▍   | 53/83 [07:16<04:16,  8.55s/it]Running generate_until requests:  65%|██████▌   | 54/83 [07:22<03:46,  7.79s/it]Running generate_until requests:  66%|██████▋   | 55/83 [07:29<03:25,  7.36s/it]Running generate_until requests:  67%|██████▋   | 56/83 [07:47<04:50, 10.77s/it]Running generate_until requests:  69%|██████▊   | 57/83 [07:59<04:44, 10.94s/it]Running generate_until requests:  70%|██████▉   | 58/83 [08:08<04:21, 10.44s/it]Running generate_until requests:  71%|███████   | 59/83 [08:15<03:43,  9.32s/it]Running generate_until requests:  72%|███████▏  | 60/83 [08:21<03:13,  8.39s/it]Running generate_until requests:  73%|███████▎  | 61/83 [08:31<03:15,  8.89s/it]Running generate_until requests:  75%|███████▍  | 62/83 [08:38<02:55,  8.35s/it]Running generate_until requests:  76%|███████▌  | 63/83 [08:45<02:39,  8.00s/it]Running generate_until requests:  77%|███████▋  | 64/83 [08:54<02:36,  8.21s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:01<02:22,  7.90s/it]Running generate_until requests:  80%|███████▉  | 66/83 [09:09<02:16,  8.03s/it]Running generate_until requests:  81%|████████  | 67/83 [09:15<01:56,  7.29s/it]Running generate_until requests:  82%|████████▏ | 68/83 [09:22<01:46,  7.07s/it]Running generate_until requests:  83%|████████▎ | 69/83 [09:32<01:53,  8.10s/it]Running generate_until requests:  84%|████████▍ | 70/83 [09:42<01:54,  8.79s/it]Running generate_until requests:  86%|████████▌ | 71/83 [09:49<01:35,  7.98s/it]Running generate_until requests:  87%|████████▋ | 72/83 [09:57<01:27,  7.99s/it]Running generate_until requests:  88%|████████▊ | 73/83 [10:04<01:17,  7.73s/it]Running generate_until requests:  89%|████████▉ | 74/83 [10:11<01:08,  7.62s/it]Running generate_until requests:  90%|█████████ | 75/83 [10:23<01:11,  8.93s/it]Running generate_until requests:  92%|█████████▏| 76/83 [10:32<01:03,  9.02s/it]Running generate_until requests:  93%|█████████▎| 77/83 [10:38<00:48,  8.06s/it]Running generate_until requests:  94%|█████████▍| 78/83 [10:45<00:38,  7.68s/it]Running generate_until requests:  95%|█████████▌| 79/83 [10:56<00:34,  8.71s/it]Running generate_until requests:  96%|█████████▋| 80/83 [11:04<00:25,  8.38s/it]Running generate_until requests:  98%|█████████▊| 81/83 [11:13<00:17,  8.73s/it]Running generate_until requests:  99%|█████████▉| 82/83 [11:19<00:07,  7.85s/it]Running generate_until requests: 100%|██████████| 83/83 [11:25<00:00,  7.46s/it]Running generate_until requests: 100%|██████████| 83/83 [11:25<00:00,  8.26s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-06:12:33:33,419 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:33:33,419 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:33:33,419 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:33:33,965 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:33:33,996 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:33:34,626 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:33:35,762 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:33:35,768 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:33:39,778 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:33:39,780 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:33:39,784 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:33:39,784 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-06:12:33:40,244 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:33:40,245 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:33:40,249 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:33:40,250 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-06:12:33:40,281 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:33:40,282 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:33:40,287 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:33:40,287 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-06:12:33:40,696 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:33:40,697 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:33:40,701 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:33:40,701 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.15}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:12:33:41,617 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:33:41,619 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:33:41,624 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:33:41,624 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.15}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.08s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.26s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.21s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.35s/it]2024-06-06:12:33:45,147 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:33:45,149 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:33:45,153 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:33:45,153 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.15}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.24s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.19s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.56s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.62s/it]2024-06-06:12:33:47,261 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:33:47,266 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:33:47,276 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:33:47,277 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.15}
Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.01s/it]2024-06-06:12:33:47,615 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:33:47,616 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:33:47,622 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:33:47,622 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.15}
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.19s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.22s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.81s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.78s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.53s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.01s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.11s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.76s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.05s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.39s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.18s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.33s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:34:43,723 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:43,725 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:43,951 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 18%|█▊        | 15/82 [00:00<00:00, 148.84it/s] 37%|███▋      | 30/82 [00:00<00:00, 149.19it/s] 55%|█████▍    | 45/82 [00:00<00:00, 149.10it/s] 73%|███████▎  | 60/82 [00:00<00:00, 149.14it/s] 91%|█████████▏| 75/82 [00:00<00:00, 148.99it/s]100%|██████████| 82/82 [00:00<00:00, 149.04it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:34:45,421 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:45,423 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:45,648 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 149.52it/s] 37%|███▋      | 31/83 [00:00<00:00, 150.47it/s] 57%|█████▋    | 47/83 [00:00<00:00, 150.61it/s] 76%|███████▌  | 63/83 [00:00<00:00, 150.74it/s] 95%|█████████▌| 79/83 [00:00<00:00, 150.82it/s]100%|██████████| 83/83 [00:00<00:00, 150.61it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:35:00,399 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:35:00,401 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:35:00,612 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 20%|█▉        | 16/82 [00:00<00:00, 150.62it/s] 39%|███▉      | 32/82 [00:00<00:00, 151.02it/s] 59%|█████▊    | 48/82 [00:00<00:00, 150.60it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 78%|███████▊  | 64/82 [00:00<00:00, 151.40it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:35:01,132 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:35:01,134 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 98%|█████████▊| 80/82 [00:00<00:00, 152.76it/s]100%|██████████| 82/82 [00:00<00:00, 152.05it/s]
2024-06-06:12:35:01,382 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 19%|█▉        | 16/83 [00:00<00:00, 155.45it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 39%|███▊      | 32/83 [00:00<00:00, 156.14it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:35:01,672 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:35:01,674 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 58%|█████▊    | 48/83 [00:00<00:00, 152.21it/s] 77%|███████▋  | 64/83 [00:00<00:00, 126.43it/s] 94%|█████████▍| 78/83 [00:00<00:00, 116.79it/s]2024-06-06:12:35:02,039 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
100%|██████████| 83/83 [00:00<00:00, 127.92it/s]
  0%|          | 0/82 [00:00<?, ?it/s] 18%|█▊        | 15/82 [00:00<00:00, 147.66it/s] 38%|███▊      | 31/82 [00:00<00:00, 149.57it/s] 57%|█████▋    | 47/82 [00:00<00:00, 150.11it/s] 77%|███████▋  | 63/82 [00:00<00:00, 148.17it/s] 96%|█████████▋| 79/82 [00:00<00:00, 150.06it/s]100%|██████████| 82/82 [00:00<00:00, 149.66it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:12:35:07,739 INFO     [xhuggingface.py:315] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:35:07,808 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:35:07,810 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:35:08,013 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 19%|█▉        | 16/83 [00:00<00:00, 158.24it/s] 39%|███▊      | 32/83 [00:00<00:00, 158.09it/s] 58%|█████▊    | 48/83 [00:00<00:00, 158.30it/s] 77%|███████▋  | 64/83 [00:00<00:00, 158.39it/s] 96%|█████████▋| 80/83 [00:00<00:00, 158.72it/s]100%|██████████| 83/83 [00:00<00:00, 158.54it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:35:20,350 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:35:20,353 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:35:20,665 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 14%|█▍        | 12/83 [00:00<00:00, 112.85it/s] 29%|██▉       | 24/83 [00:00<00:00, 113.07it/s] 43%|████▎     | 36/83 [00:00<00:00, 113.22it/s] 58%|█████▊    | 48/83 [00:00<00:00, 113.30it/s] 72%|███████▏  | 60/83 [00:00<00:00, 113.31it/s] 87%|████████▋ | 72/83 [00:00<00:00, 113.41it/s]100%|██████████| 83/83 [00:00<00:00, 113.35it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:35:22,996 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:35:22,999 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:35:23,370 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 12%|█▏        | 10/82 [00:00<00:00, 91.18it/s] 24%|██▍       | 20/82 [00:00<00:00, 95.41it/s] 38%|███▊      | 31/82 [00:00<00:00, 101.37it/s] 51%|█████     | 42/82 [00:00<00:00, 104.28it/s] 65%|██████▍   | 53/82 [00:00<00:00, 105.96it/s] 78%|███████▊  | 64/82 [00:00<00:00, 106.74it/s] 91%|█████████▏| 75/82 [00:00<00:00, 107.45it/s]100%|██████████| 82/82 [00:00<00:00, 103.75it/s]
2024-06-06:12:35:29,743 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:35:29,743 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:35:29,743 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:35:29,743 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:35:29,743 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:35:29,743 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:35:29,745 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:35:29,753 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:12<16:30, 12.08s/it]Running generate_until requests:   2%|▏         | 2/83 [00:28<19:44, 14.62s/it]Running generate_until requests:   4%|▎         | 3/83 [00:38<16:25, 12.32s/it]Running generate_until requests:   5%|▍         | 4/83 [00:44<12:59,  9.86s/it]Running generate_until requests:   6%|▌         | 5/83 [00:54<12:58,  9.98s/it]Running generate_until requests:   7%|▋         | 6/83 [01:01<11:35,  9.03s/it]Running generate_until requests:   8%|▊         | 7/83 [01:12<12:15,  9.68s/it]Running generate_until requests:  10%|▉         | 8/83 [01:21<11:45,  9.41s/it]Running generate_until requests:  11%|█         | 9/83 [01:32<12:25, 10.08s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:40<11:08,  9.16s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:48<10:52,  9.06s/it]Running generate_until requests:  14%|█▍        | 12/83 [02:02<12:13, 10.33s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:06<10:01,  8.59s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:15<09:54,  8.62s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:35<13:40, 12.06s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:49<14:10, 12.70s/it]Running generate_until requests:  20%|██        | 17/83 [02:59<13:02, 11.85s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:08<11:55, 11.01s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:19<11:38, 10.92s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:32<12:19, 11.75s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:40<10:43, 10.38s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:48<09:51,  9.70s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:57<09:37,  9.62s/it]Running generate_until requests:  29%|██▉       | 24/83 [04:05<08:57,  9.11s/it]Running generate_until requests:  30%|███       | 25/83 [04:13<08:19,  8.62s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:21<08:12,  8.64s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:33<08:56,  9.58s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:41<08:22,  9.14s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:49<07:51,  8.74s/it]Running generate_until requests:  36%|███▌      | 30/83 [05:02<08:54, 10.09s/it]Running generate_until requests:  37%|███▋      | 31/83 [05:11<08:30,  9.82s/it]Running generate_until requests:  39%|███▊      | 32/83 [05:21<08:15,  9.71s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:26<06:57,  8.36s/it]Running generate_until requests:  41%|████      | 34/83 [05:37<07:26,  9.12s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:44<06:46,  8.47s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:51<06:20,  8.10s/it]Running generate_until requests:  45%|████▍     | 37/83 [06:00<06:29,  8.47s/it]Running generate_until requests:  46%|████▌     | 38/83 [06:16<07:57, 10.62s/it]Running generate_until requests:  47%|████▋     | 39/83 [06:23<06:55,  9.44s/it]Running generate_until requests:  48%|████▊     | 40/83 [06:30<06:20,  8.84s/it]Running generate_until requests:  49%|████▉     | 41/83 [06:36<05:38,  8.06s/it]Running generate_until requests:  51%|█████     | 42/83 [06:44<05:27,  7.98s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:55<05:46,  8.67s/it]Running generate_until requests:  53%|█████▎    | 44/83 [07:01<05:12,  8.01s/it]Running generate_until requests:  54%|█████▍    | 45/83 [07:09<05:02,  7.97s/it]Running generate_until requests:  55%|█████▌    | 46/83 [07:17<04:54,  7.95s/it]Running generate_until requests:  57%|█████▋    | 47/83 [07:23<04:25,  7.36s/it]Running generate_until requests:  58%|█████▊    | 48/83 [07:33<04:50,  8.30s/it]Running generate_until requests:  59%|█████▉    | 49/83 [07:38<04:11,  7.38s/it]Running generate_until requests:  60%|██████    | 50/83 [07:51<04:53,  8.90s/it]Running generate_until requests:  61%|██████▏   | 51/83 [08:00<04:44,  8.88s/it]Running generate_until requests:  63%|██████▎   | 52/83 [08:07<04:23,  8.49s/it]Running generate_until requests:  64%|██████▍   | 53/83 [08:17<04:27,  8.93s/it]Running generate_until requests:  65%|██████▌   | 54/83 [08:23<03:52,  8.02s/it]Running generate_until requests:  66%|██████▋   | 55/83 [08:30<03:31,  7.55s/it]Running generate_until requests:  67%|██████▋   | 56/83 [08:48<04:52, 10.82s/it]Running generate_until requests:  69%|██████▊   | 57/83 [09:00<04:47, 11.07s/it]Running generate_until requests:  70%|██████▉   | 58/83 [09:09<04:22, 10.49s/it]Running generate_until requests:  71%|███████   | 59/83 [09:15<03:43,  9.32s/it]Running generate_until requests:  72%|███████▏  | 60/83 [09:21<03:08,  8.18s/it]Running generate_until requests:  73%|███████▎  | 61/83 [09:31<03:11,  8.71s/it]Running generate_until requests:  75%|███████▍  | 62/83 [09:38<02:52,  8.21s/it]Running generate_until requests:  76%|███████▌  | 63/83 [09:44<02:32,  7.60s/it]Running generate_until requests:  77%|███████▋  | 64/83 [09:53<02:31,  7.96s/it]Running generate_until requests:  78%|███████▊  | 65/83 [10:00<02:18,  7.67s/it]Running generate_until requests:  80%|███████▉  | 66/83 [10:06<02:03,  7.28s/it]Running generate_until requests:  81%|████████  | 67/83 [10:12<01:47,  6.73s/it]Running generate_until requests:  82%|████████▏ | 68/83 [10:18<01:39,  6.67s/it]Running generate_until requests:  83%|████████▎ | 69/83 [10:29<01:49,  7.84s/it]Running generate_until requests:  84%|████████▍ | 70/83 [10:39<01:51,  8.55s/it]Running generate_until requests:  86%|████████▌ | 71/83 [10:45<01:33,  7.80s/it]Running generate_until requests:  87%|████████▋ | 72/83 [10:53<01:26,  7.84s/it]Running generate_until requests:  88%|████████▊ | 73/83 [11:00<01:16,  7.64s/it]Running generate_until requests:  89%|████████▉ | 74/83 [11:08<01:07,  7.54s/it]Running generate_until requests:  90%|█████████ | 75/83 [11:19<01:10,  8.82s/it]Running generate_until requests:  92%|█████████▏| 76/83 [11:29<01:02,  8.98s/it]Running generate_until requests:  93%|█████████▎| 77/83 [11:35<00:48,  8.03s/it]Running generate_until requests:  94%|█████████▍| 78/83 [11:41<00:38,  7.71s/it]Running generate_until requests:  95%|█████████▌| 79/83 [11:53<00:34,  8.73s/it]Running generate_until requests:  96%|█████████▋| 80/83 [12:00<00:25,  8.45s/it]Running generate_until requests:  98%|█████████▊| 81/83 [12:10<00:17,  8.77s/it]Running generate_until requests:  99%|█████████▉| 82/83 [12:16<00:07,  7.88s/it]Running generate_until requests: 100%|██████████| 83/83 [12:22<00:00,  7.45s/it]Running generate_until requests: 100%|██████████| 83/83 [12:22<00:00,  8.95s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-06:13:01:58,450 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:01:58,455 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:01:58,501 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:01:58,541 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:01:59,082 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:01:59,200 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:01:59,639 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:02:00,185 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:02:04,766 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:02:04,767 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:02:04,772 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:02:04,772 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-06:13:02:05,375 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:02:05,376 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:02:05,381 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:02:05,381 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-06:13:02:05,583 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:02:05,584 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:02:05,588 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:02:05,588 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-06:13:02:05,709 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:02:05,710 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:02:05,715 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:02:05,715 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.2}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:13:02:06,129 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:02:06,130 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:02:06,134 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:02:06,134 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.2}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.29s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.63s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.08s/it]2024-06-06:13:02:10,335 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:02:10,336 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:02:10,347 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:02:10,347 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.2}
Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.18s/it]2024-06-06:13:02:10,816 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:02:10,817 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:02:10,822 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:02:10,822 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-06:13:02:11,075 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:02:11,077 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:02:11,085 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:02:11,086 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.2}
Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.20s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.82s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.85s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.70s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.89s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.61s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.17s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.18s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:01,  1.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.40s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.38s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:03:05,296 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:03:05,298 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:13:03:05,527 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:03:05,625 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:03:05,627 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 18%|█▊        | 15/83 [00:00<00:00, 143.85it/s] 36%|███▌      | 30/83 [00:00<00:00, 146.37it/s]2024-06-06:13:03:05,843 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
 54%|█████▍    | 45/83 [00:00<00:00, 146.88it/s]  0%|          | 0/82 [00:00<?, ?it/s] 72%|███████▏  | 60/83 [00:00<00:00, 147.64it/s] 18%|█▊        | 15/82 [00:00<00:00, 146.89it/s] 92%|█████████▏| 76/83 [00:00<00:00, 148.51it/s] 37%|███▋      | 30/82 [00:00<00:00, 147.78it/s]100%|██████████| 83/83 [00:00<00:00, 147.81it/s]
 56%|█████▌    | 46/82 [00:00<00:00, 148.85it/s] 74%|███████▍  | 61/82 [00:00<00:00, 149.25it/s] 94%|█████████▍| 77/82 [00:00<00:00, 149.54it/s]100%|██████████| 82/82 [00:00<00:00, 149.08it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:03:26,189 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:03:26,191 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:03:26,220 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:03:26,222 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:03:26,404 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-06:13:03:26,444 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 20%|█▉        | 16/82 [00:00<00:00, 151.62it/s] 13%|█▎        | 11/82 [00:00<00:00, 105.06it/s] 39%|███▉      | 32/82 [00:00<00:00, 152.06it/s] 29%|██▉       | 24/82 [00:00<00:00, 118.28it/s] 59%|█████▊    | 48/82 [00:00<00:00, 151.35it/s] 49%|████▉     | 40/82 [00:00<00:00, 133.65it/s] 78%|███████▊  | 64/82 [00:00<00:00, 150.77it/s] 68%|██████▊   | 56/82 [00:00<00:00, 140.91it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:13:03:26,907 INFO     [xhuggingface.py:315] Using 8 devices with data parallelism
 98%|█████████▊| 80/82 [00:00<00:00, 151.24it/s]100%|██████████| 82/82 [00:00<00:00, 151.32it/s]
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:03:26,981 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:03:26,984 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 88%|████████▊ | 72/82 [00:00<00:00, 145.93it/s]100%|██████████| 82/82 [00:00<00:00, 140.09it/s]
2024-06-06:13:03:27,193 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 19%|█▉        | 16/83 [00:00<00:00, 156.04it/s] 39%|███▊      | 32/83 [00:00<00:00, 156.18it/s] 58%|█████▊    | 48/83 [00:00<00:00, 156.55it/s] 77%|███████▋  | 64/83 [00:00<00:00, 156.51it/s] 96%|█████████▋| 80/83 [00:00<00:00, 156.57it/s]100%|██████████| 83/83 [00:00<00:00, 156.52it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:03:29,255 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:03:29,258 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:03:29,714 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 11%|█         | 9/83 [00:00<00:00, 82.47it/s] 22%|██▏       | 18/83 [00:00<00:00, 83.08it/s] 33%|███▎      | 27/83 [00:00<00:00, 83.34it/s] 43%|████▎     | 36/83 [00:00<00:00, 83.35it/s] 54%|█████▍    | 45/83 [00:00<00:00, 83.38it/s] 65%|██████▌   | 54/83 [00:00<00:00, 83.50it/s] 76%|███████▌  | 63/83 [00:00<00:00, 83.37it/s] 87%|████████▋ | 72/83 [00:00<00:00, 83.47it/s] 98%|█████████▊| 81/83 [00:00<00:00, 83.53it/s]100%|██████████| 83/83 [00:00<00:00, 83.38it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:03:46,255 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:03:46,258 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:03:46,726 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 10%|▉         | 8/83 [00:00<00:00, 76.87it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 19%|█▉        | 16/83 [00:00<00:00, 74.67it/s] 34%|███▎      | 28/83 [00:00<00:00, 91.96it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:03:47,076 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:03:47,078 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 46%|████▌     | 38/83 [00:00<00:00, 76.07it/s] 59%|█████▉    | 49/83 [00:00<00:00, 85.53it/s] 70%|██████▉   | 58/83 [00:00<00:00, 82.49it/s]2024-06-06:13:03:47,583 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
 81%|████████  | 67/83 [00:00<00:00, 73.51it/s]  0%|          | 0/82 [00:00<?, ?it/s] 94%|█████████▍| 78/83 [00:00<00:00, 82.67it/s]  9%|▊         | 7/82 [00:00<00:01, 66.64it/s]100%|██████████| 83/83 [00:01<00:00, 78.46it/s]
 22%|██▏       | 18/82 [00:00<00:00, 90.46it/s] 35%|███▌      | 29/82 [00:00<00:00, 95.27it/s] 48%|████▊     | 39/82 [00:00<00:00, 96.22it/s] 60%|█████▉    | 49/82 [00:00<00:00, 96.96it/s] 74%|███████▍  | 61/82 [00:00<00:00, 104.55it/s] 89%|████████▉ | 73/82 [00:00<00:00, 109.04it/s]100%|██████████| 82/82 [00:00<00:00, 102.01it/s]
2024-06-06:13:03:57,888 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:03:57,888 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:03:57,888 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:03:57,888 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:03:57,888 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:03:57,888 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:03:57,888 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:03:57,889 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:13<18:38, 13.64s/it]Running generate_until requests:   2%|▏         | 2/83 [00:30<20:35, 15.25s/it]Running generate_until requests:   4%|▎         | 3/83 [00:39<17:06, 12.83s/it]Running generate_until requests:   5%|▍         | 4/83 [00:45<13:15, 10.07s/it]Running generate_until requests:   6%|▌         | 5/83 [00:56<13:15, 10.20s/it]Running generate_until requests:   7%|▋         | 6/83 [01:03<11:53,  9.27s/it]Running generate_until requests:   8%|▊         | 7/83 [01:14<12:28,  9.85s/it]Running generate_until requests:  10%|▉         | 8/83 [01:23<11:49,  9.47s/it]Running generate_until requests:  11%|█         | 9/83 [01:32<11:41,  9.48s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:38<10:08,  8.34s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:45<09:36,  8.00s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:56<10:18,  8.71s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:00<08:39,  7.42s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:09<08:50,  7.69s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:22<10:40,  9.42s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:34<11:31, 10.33s/it]Running generate_until requests:  20%|██        | 17/83 [02:45<11:20, 10.32s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:54<10:49,  9.99s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:04<10:44, 10.07s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:18<11:44, 11.19s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:26<10:32, 10.20s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:34<09:45,  9.60s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:43<09:27,  9.46s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:51<08:47,  8.95s/it]Running generate_until requests:  30%|███       | 25/83 [03:58<08:05,  8.37s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:06<07:55,  8.35s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:18<08:43,  9.35s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:26<08:11,  8.94s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:34<07:41,  8.55s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:45<08:18,  9.41s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:57<08:55, 10.29s/it]Running generate_until requests:  39%|███▊      | 32/83 [05:07<08:27,  9.96s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:11<07:00,  8.41s/it]Running generate_until requests:  41%|████      | 34/83 [05:23<07:42,  9.44s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:29<06:45,  8.45s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:36<06:07,  7.82s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:45<06:16,  8.19s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:57<07:04,  9.43s/it]Running generate_until requests:  47%|████▋     | 39/83 [06:02<05:59,  8.18s/it]Running generate_until requests:  48%|████▊     | 40/83 [06:08<05:24,  7.54s/it]Running generate_until requests:  49%|████▉     | 41/83 [06:14<04:59,  7.13s/it]Running generate_until requests:  51%|█████     | 42/83 [06:22<05:01,  7.35s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:33<05:29,  8.24s/it]Running generate_until requests:  53%|█████▎    | 44/83 [06:39<05:00,  7.70s/it]Running generate_until requests:  54%|█████▍    | 45/83 [06:47<04:53,  7.72s/it]Running generate_until requests:  55%|█████▌    | 46/83 [06:55<04:47,  7.78s/it]Running generate_until requests:  57%|█████▋    | 47/83 [07:00<04:15,  7.09s/it]Running generate_until requests:  58%|█████▊    | 48/83 [07:09<04:22,  7.50s/it]Running generate_until requests:  59%|█████▉    | 49/83 [07:13<03:41,  6.52s/it]Running generate_until requests:  60%|██████    | 50/83 [07:23<04:10,  7.58s/it]Running generate_until requests:  61%|██████▏   | 51/83 [07:30<03:56,  7.40s/it]Running generate_until requests:  63%|██████▎   | 52/83 [07:36<03:36,  6.98s/it]Running generate_until requests:  64%|██████▍   | 53/83 [07:44<03:39,  7.32s/it]Running generate_until requests:  65%|██████▌   | 54/83 [07:49<03:14,  6.72s/it]Running generate_until requests:  66%|██████▋   | 55/83 [07:55<02:54,  6.25s/it]Running generate_until requests:  67%|██████▋   | 56/83 [08:10<04:06,  9.14s/it]Running generate_until requests:  69%|██████▊   | 57/83 [08:24<04:29, 10.37s/it]Running generate_until requests:  70%|██████▉   | 58/83 [08:31<03:55,  9.44s/it]Running generate_until requests:  71%|███████   | 59/83 [08:36<03:17,  8.22s/it]Running generate_until requests:  72%|███████▏  | 60/83 [08:41<02:47,  7.29s/it]Running generate_until requests:  73%|███████▎  | 61/83 [08:50<02:45,  7.53s/it]Running generate_until requests:  75%|███████▍  | 62/83 [08:56<02:29,  7.11s/it]Running generate_until requests:  76%|███████▌  | 63/83 [09:01<02:14,  6.71s/it]Running generate_until requests:  77%|███████▋  | 64/83 [09:08<02:09,  6.80s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:14<01:56,  6.50s/it]Running generate_until requests:  80%|███████▉  | 66/83 [09:19<01:44,  6.12s/it]Running generate_until requests:  81%|████████  | 67/83 [09:24<01:31,  5.75s/it]Running generate_until requests:  82%|████████▏ | 68/83 [09:30<01:24,  5.62s/it]Running generate_until requests:  83%|████████▎ | 69/83 [09:38<01:30,  6.44s/it]Running generate_until requests:  84%|████████▍ | 70/83 [09:48<01:37,  7.48s/it]Running generate_until requests:  86%|████████▌ | 71/83 [09:53<01:20,  6.72s/it]Running generate_until requests:  87%|████████▋ | 72/83 [09:59<01:12,  6.63s/it]Running generate_until requests:  88%|████████▊ | 73/83 [10:05<01:04,  6.42s/it]Running generate_until requests:  89%|████████▉ | 74/83 [10:11<00:56,  6.28s/it]Running generate_until requests:  90%|█████████ | 75/83 [10:21<00:58,  7.27s/it]Running generate_until requests:  92%|█████████▏| 76/83 [10:28<00:51,  7.31s/it]Running generate_until requests:  93%|█████████▎| 77/83 [10:33<00:39,  6.55s/it]Running generate_until requests:  94%|█████████▍| 78/83 [10:39<00:31,  6.38s/it]Running generate_until requests:  95%|█████████▌| 79/83 [10:48<00:28,  7.13s/it]Running generate_until requests:  96%|█████████▋| 80/83 [10:54<00:20,  6.82s/it]Running generate_until requests:  98%|█████████▊| 81/83 [11:02<00:14,  7.05s/it]Running generate_until requests:  99%|█████████▉| 82/83 [11:06<00:06,  6.38s/it]Running generate_until requests: 100%|██████████| 83/83 [11:11<00:00,  5.99s/it]Running generate_until requests: 100%|██████████| 83/83 [11:11<00:00,  8.10s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-06:13:30:52,759 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:30:53,135 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:30:53,405 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:30:53,482 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:30:53,594 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:30:53,701 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:30:53,728 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:30:53,824 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:30:58,134 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:30:58,135 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:30:58,139 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:30:58,139 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.3}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-06:13:31:00,310 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:31:00,311 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:31:00,315 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:31:00,315 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-06:13:31:01,088 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:31:01,090 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:31:01,095 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:31:01,095 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-06:13:31:01,444 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:31:01,446 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:31:01,450 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:31:01,450 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-06:13:31:01,655 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:31:01,656 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:31:01,661 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:31:01,661 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.3}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:13:31:01,691 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:31:01,692 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:31:01,696 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:31:01,696 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-06:13:31:01,768 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-06:13:31:01,769 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:31:01,776 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:31:01,776 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.3}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.42s/it]2024-06-06:13:31:01,953 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:31:01,955 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:31:01,960 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:31:01,960 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.3}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.39s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.43s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.59s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.83s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.23s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.83s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.29s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.83s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.84s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.91s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.08s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.56s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.07s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.11s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.27s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.26s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:13:31:52,748 INFO     [xhuggingface.py:315] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:31:52,853 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:31:52,855 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:31:53,074 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 19%|█▉        | 16/83 [00:00<00:00, 151.36it/s] 39%|███▊      | 32/83 [00:00<00:00, 153.35it/s] 58%|█████▊    | 48/83 [00:00<00:00, 153.98it/s] 77%|███████▋  | 64/83 [00:00<00:00, 154.38it/s] 96%|█████████▋| 80/83 [00:00<00:00, 154.57it/s]100%|██████████| 83/83 [00:00<00:00, 154.18it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:32:05,003 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:32:05,005 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:32:05,243 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 18%|█▊        | 15/82 [00:00<00:00, 146.42it/s] 37%|███▋      | 30/82 [00:00<00:00, 147.44it/s] 55%|█████▍    | 45/82 [00:00<00:00, 147.74it/s] 73%|███████▎  | 60/82 [00:00<00:00, 145.85it/s] 91%|█████████▏| 75/82 [00:00<00:00, 145.85it/s]100%|██████████| 82/82 [00:00<00:00, 146.24it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:32:33,872 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:32:33,874 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:32:34,199 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 106.56it/s] 27%|██▋       | 22/82 [00:00<00:00, 106.99it/s] 40%|████      | 33/82 [00:00<00:00, 107.37it/s] 54%|█████▎    | 44/82 [00:00<00:00, 107.57it/s] 67%|██████▋   | 55/82 [00:00<00:00, 107.49it/s] 80%|████████  | 66/82 [00:00<00:00, 107.56it/s] 94%|█████████▍| 77/82 [00:00<00:00, 107.67it/s]100%|██████████| 82/82 [00:00<00:00, 107.47it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:32:36,642 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:32:36,645 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:32:36,930 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 15%|█▍        | 12/82 [00:00<00:00, 111.80it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:32:37,129 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:32:37,132 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 29%|██▉       | 24/82 [00:00<00:00, 112.61it/s] 44%|████▍     | 36/82 [00:00<00:00, 112.42it/s] 59%|█████▊    | 48/82 [00:00<00:00, 112.49it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:13:32:37,458 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
 74%|███████▍  | 61/82 [00:00<00:00, 116.73it/s]  0%|          | 0/83 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:32:37,494 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:32:37,496 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 91%|█████████▏| 75/82 [00:00<00:00, 122.19it/s] 13%|█▎        | 11/83 [00:00<00:00, 104.66it/s]100%|██████████| 82/82 [00:00<00:00, 118.99it/s]
 27%|██▋       | 22/83 [00:00<00:00, 104.91it/s] 40%|███▉      | 33/83 [00:00<00:00, 104.80it/s]2024-06-06:13:32:37,884 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
 53%|█████▎    | 44/83 [00:00<00:00, 104.57it/s]  0%|          | 0/83 [00:00<?, ?it/s] 66%|██████▋   | 55/83 [00:00<00:00, 103.31it/s] 13%|█▎        | 11/83 [00:00<00:00, 104.88it/s] 27%|██▋       | 22/83 [00:00<00:00, 102.36it/s] 80%|███████▉  | 66/83 [00:00<00:00, 100.91it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 40%|███▉      | 33/83 [00:00<00:00, 103.90it/s] 93%|█████████▎| 77/83 [00:00<00:00, 102.53it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:32:38,250 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:32:38,253 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
100%|██████████| 83/83 [00:00<00:00, 103.37it/s]
 53%|█████▎    | 44/83 [00:00<00:00, 105.68it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 66%|██████▋   | 55/83 [00:00<00:00, 103.86it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:32:38,511 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:32:38,513 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 80%|███████▉  | 66/83 [00:00<00:00, 104.54it/s]2024-06-06:13:32:38,608 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 93%|█████████▎| 77/83 [00:00<00:00, 104.22it/s]100%|██████████| 83/83 [00:00<00:00, 104.06it/s]
 11%|█         | 9/83 [00:00<00:00, 87.06it/s] 24%|██▍       | 20/83 [00:00<00:00, 97.06it/s]2024-06-06:13:32:38,945 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
 37%|███▋      | 31/83 [00:00<00:00, 100.10it/s]  0%|          | 0/82 [00:00<?, ?it/s] 51%|█████     | 42/83 [00:00<00:00, 102.03it/s] 13%|█▎        | 11/82 [00:00<00:00, 102.14it/s] 64%|██████▍   | 53/83 [00:00<00:00, 103.05it/s] 27%|██▋       | 22/82 [00:00<00:00, 103.06it/s] 77%|███████▋  | 64/83 [00:00<00:00, 101.65it/s] 40%|████      | 33/82 [00:00<00:00, 83.90it/s]  90%|█████████ | 75/83 [00:00<00:00, 102.88it/s]100%|██████████| 83/83 [00:00<00:00, 101.70it/s]
 51%|█████     | 42/82 [00:00<00:00, 78.68it/s] 65%|██████▍   | 53/82 [00:00<00:00, 86.79it/s] 78%|███████▊  | 64/82 [00:00<00:00, 92.28it/s] 91%|█████████▏| 75/82 [00:00<00:00, 95.85it/s]100%|██████████| 82/82 [00:00<00:00, 92.64it/s]
2024-06-06:13:32:46,567 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:32:46,567 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:32:46,567 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:32:46,568 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:32:46,568 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:32:46,570 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-06:13:32:46,583 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:32:46,583 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:14<19:23, 14.19s/it]Running generate_until requests:   2%|▏         | 2/83 [00:29<20:24, 15.12s/it]Running generate_until requests:   4%|▎         | 3/83 [00:37<15:45, 11.82s/it]Running generate_until requests:   5%|▍         | 4/83 [00:42<12:02,  9.15s/it]Running generate_until requests:   6%|▌         | 5/83 [00:52<12:08,  9.34s/it]Running generate_until requests:   7%|▋         | 6/83 [00:58<10:32,  8.21s/it]Running generate_until requests:   8%|▊         | 7/83 [01:10<12:04,  9.53s/it]Running generate_until requests:  10%|▉         | 8/83 [01:18<11:00,  8.80s/it]Running generate_until requests:  11%|█         | 9/83 [01:27<11:08,  9.03s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:33<09:44,  8.01s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:40<09:17,  7.74s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:51<10:16,  8.69s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:56<08:51,  7.60s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:03<08:34,  7.45s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:18<11:08,  9.83s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:30<11:43, 10.50s/it]Running generate_until requests:  20%|██        | 17/83 [02:39<10:50,  9.86s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:46<09:52,  9.11s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:55<09:32,  8.95s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:07<10:20,  9.85s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:15<09:36,  9.30s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:21<08:38,  8.49s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:29<08:10,  8.17s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:36<07:44,  7.88s/it]Running generate_until requests:  30%|███       | 25/83 [03:42<07:04,  7.32s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:49<06:49,  7.18s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:59<07:30,  8.04s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:05<06:57,  7.59s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:15<07:25,  8.25s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:26<07:50,  8.88s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:36<08:10,  9.44s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:46<08:03,  9.49s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:51<06:42,  8.06s/it]Running generate_until requests:  41%|████      | 34/83 [05:02<07:20,  8.99s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:08<06:30,  8.13s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:16<06:16,  8.02s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:23<06:02,  7.88s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:34<06:27,  8.61s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:39<05:38,  7.69s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:53<06:46,  9.45s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:59<05:59,  8.55s/it]Running generate_until requests:  51%|█████     | 42/83 [06:07<05:43,  8.38s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:18<06:04,  9.11s/it]Running generate_until requests:  53%|█████▎    | 44/83 [06:24<05:24,  8.31s/it]Running generate_until requests:  54%|█████▍    | 45/83 [06:32<05:10,  8.18s/it]Running generate_until requests:  55%|█████▌    | 46/83 [06:40<04:59,  8.10s/it]Running generate_until requests:  57%|█████▋    | 47/83 [06:48<04:46,  7.97s/it]Running generate_until requests:  58%|█████▊    | 48/83 [06:58<05:06,  8.75s/it]Running generate_until requests:  59%|█████▉    | 49/83 [07:03<04:19,  7.63s/it]Running generate_until requests:  60%|██████    | 50/83 [07:16<05:02,  9.17s/it]Running generate_until requests:  61%|██████▏   | 51/83 [07:25<04:53,  9.16s/it]Running generate_until requests:  63%|██████▎   | 52/83 [07:33<04:29,  8.68s/it]Running generate_until requests:  64%|██████▍   | 53/83 [07:43<04:35,  9.17s/it]Running generate_until requests:  65%|██████▌   | 54/83 [07:49<04:00,  8.30s/it]Running generate_until requests:  66%|██████▋   | 55/83 [07:59<04:05,  8.77s/it]Running generate_until requests:  67%|██████▋   | 56/83 [08:20<05:35, 12.42s/it]Running generate_until requests:  69%|██████▊   | 57/83 [08:34<05:34, 12.87s/it]Running generate_until requests:  70%|██████▉   | 58/83 [08:43<04:52, 11.70s/it]Running generate_until requests:  71%|███████   | 59/83 [08:50<04:03, 10.14s/it]Running generate_until requests:  72%|███████▏  | 60/83 [08:56<03:26,  8.97s/it]Running generate_until requests:  73%|███████▎  | 61/83 [09:06<03:24,  9.29s/it]Running generate_until requests:  75%|███████▍  | 62/83 [09:13<03:03,  8.76s/it]Running generate_until requests:  76%|███████▌  | 63/83 [09:21<02:45,  8.29s/it]Running generate_until requests:  77%|███████▋  | 64/83 [09:29<02:39,  8.42s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:36<02:24,  8.05s/it]Running generate_until requests:  80%|███████▉  | 66/83 [09:43<02:09,  7.61s/it]Running generate_until requests:  81%|████████  | 67/83 [09:48<01:51,  6.94s/it]Running generate_until requests:  82%|████████▏ | 68/83 [09:55<01:42,  6.81s/it]Running generate_until requests:  83%|████████▎ | 69/83 [10:09<02:04,  8.86s/it]Running generate_until requests:  84%|████████▍ | 70/83 [10:21<02:07,  9.80s/it]Running generate_until requests:  86%|████████▌ | 71/83 [10:27<01:44,  8.67s/it]Running generate_until requests:  87%|████████▋ | 72/83 [10:35<01:34,  8.55s/it]Running generate_until requests:  88%|████████▊ | 73/83 [10:42<01:21,  8.13s/it]Running generate_until requests:  89%|████████▉ | 74/83 [10:49<01:10,  7.88s/it]Running generate_until requests:  90%|█████████ | 75/83 [11:01<01:12,  9.10s/it]Running generate_until requests:  92%|█████████▏| 76/83 [11:10<01:03,  9.13s/it]Running generate_until requests:  93%|█████████▎| 77/83 [11:18<00:51,  8.60s/it]Running generate_until requests:  94%|█████████▍| 78/83 [11:25<00:41,  8.25s/it]Running generate_until requests:  95%|█████████▌| 79/83 [11:37<00:36,  9.23s/it]Running generate_until requests:  96%|█████████▋| 80/83 [11:44<00:26,  8.74s/it]Running generate_until requests:  98%|█████████▊| 81/83 [11:55<00:18,  9.24s/it]Running generate_until requests:  99%|█████████▉| 82/83 [12:01<00:08,  8.25s/it]Running generate_until requests: 100%|██████████| 83/83 [12:07<00:00,  7.70s/it]Running generate_until requests: 100%|██████████| 83/83 [12:07<00:00,  8.77s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-06:14:01:07,885 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:01:07,885 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:01:07,885 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:01:08,346 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:01:08,590 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:01:08,826 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:01:09,176 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:01:09,177 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:01:14,007 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:01:14,008 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:01:14,013 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:01:14,013 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-06:14:01:14,517 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:01:14,518 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:01:14,521 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:01:14,521 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-06:14:01:14,564 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:01:14,566 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:01:14,570 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:01:14,570 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-06:14:01:14,985 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:01:14,985 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:01:14,989 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:01:14,989 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.4}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.98s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.94s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.76s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.56s/it]
2024-06-06:14:01:22,162 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:01:22,164 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:01:22,175 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:01:22,175 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.4}
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.54s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.32s/it]2024-06-06:14:01:23,118 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:01:23,119 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:01:23,129 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:01:23,129 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.4}
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.95s/it]
2024-06-06:14:01:23,369 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:01:23,371 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:01:23,376 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:01:23,376 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-06:14:01:23,593 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:01:23,594 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:01:23,597 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:01:23,597 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.4}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.95s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.46s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.09s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.67s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.54s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.97s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.04s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.24s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:02:13,819 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:13,821 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:14,159 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 106.64it/s] 27%|██▋       | 22/82 [00:00<00:00, 107.10it/s] 40%|████      | 33/82 [00:00<00:00, 107.50it/s] 54%|█████▎    | 44/82 [00:00<00:00, 107.69it/s] 67%|██████▋   | 55/82 [00:00<00:00, 107.74it/s] 80%|████████  | 66/82 [00:00<00:00, 107.76it/s] 94%|█████████▍| 77/82 [00:00<00:00, 107.69it/s]100%|██████████| 82/82 [00:00<00:00, 107.57it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:02:16,617 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:16,619 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:16,837 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 149.42it/s] 37%|███▋      | 31/83 [00:00<00:00, 150.23it/s] 57%|█████▋    | 47/83 [00:00<00:00, 150.55it/s] 76%|███████▌  | 63/83 [00:00<00:00, 150.48it/s] 95%|█████████▌| 79/83 [00:00<00:00, 149.98it/s]100%|██████████| 83/83 [00:00<00:00, 150.11it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:02:34,973 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:34,975 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:35,180 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 18%|█▊        | 15/82 [00:00<00:00, 147.06it/s] 38%|███▊      | 31/82 [00:00<00:00, 149.42it/s] 57%|█████▋    | 47/82 [00:00<00:00, 149.77it/s] 76%|███████▌  | 62/82 [00:00<00:00, 149.15it/s] 95%|█████████▌| 78/82 [00:00<00:00, 149.86it/s]100%|██████████| 82/82 [00:00<00:00, 149.53it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:02:35,917 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:35,919 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:36,128 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 20%|█▉        | 16/82 [00:00<00:00, 154.98it/s] 39%|███▉      | 32/82 [00:00<00:00, 154.66it/s] 59%|█████▊    | 48/82 [00:00<00:00, 154.51it/s] 78%|███████▊  | 64/82 [00:00<00:00, 153.84it/s] 98%|█████████▊| 80/82 [00:00<00:00, 154.02it/s]100%|██████████| 82/82 [00:00<00:00, 154.16it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:14:02:38,915 INFO     [xhuggingface.py:315] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:02:38,989 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:38,991 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:39,225 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 19%|█▉        | 16/83 [00:00<00:00, 153.58it/s] 39%|███▊      | 32/83 [00:00<00:00, 154.03it/s] 58%|█████▊    | 48/83 [00:00<00:00, 150.72it/s] 77%|███████▋  | 64/83 [00:00<00:00, 148.46it/s] 96%|█████████▋| 80/83 [00:00<00:00, 150.27it/s]100%|██████████| 83/83 [00:00<00:00, 150.87it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:02:40,605 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:40,608 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:40,954 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 104.51it/s] 27%|██▋       | 22/83 [00:00<00:00, 104.03it/s] 40%|███▉      | 33/83 [00:00<00:00, 103.82it/s] 53%|█████▎    | 44/83 [00:00<00:00, 104.33it/s] 66%|██████▋   | 55/83 [00:00<00:00, 104.52it/s] 80%|███████▉  | 66/83 [00:00<00:00, 103.97it/s] 93%|█████████▎| 77/83 [00:00<00:00, 104.30it/s]100%|██████████| 83/83 [00:00<00:00, 104.23it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:02:56,487 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:56,490 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:56,946 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 11%|█         | 9/83 [00:00<00:00, 75.09it/s] 20%|██        | 17/83 [00:00<00:01, 60.36it/s] 29%|██▉       | 24/83 [00:00<00:01, 58.92it/s] 36%|███▌      | 30/83 [00:00<00:00, 55.22it/s] 53%|█████▎    | 44/83 [00:00<00:00, 79.40it/s] 66%|██████▋   | 55/83 [00:00<00:00, 87.36it/s] 80%|███████▉  | 66/83 [00:00<00:00, 92.30it/s] 94%|█████████▍| 78/83 [00:00<00:00, 98.12it/s]100%|██████████| 83/83 [00:00<00:00, 83.88it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:02:58,292 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:58,294 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:02:59,009 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]  7%|▋         | 6/82 [00:00<00:01, 55.89it/s] 15%|█▍        | 12/82 [00:00<00:01, 51.66it/s] 24%|██▍       | 20/82 [00:00<00:01, 61.55it/s] 34%|███▍      | 28/82 [00:00<00:00, 65.24it/s] 44%|████▍     | 36/82 [00:00<00:00, 70.12it/s] 56%|█████▌    | 46/82 [00:00<00:00, 77.73it/s] 66%|██████▌   | 54/82 [00:00<00:00, 72.77it/s] 76%|███████▌  | 62/82 [00:00<00:00, 66.43it/s] 88%|████████▊ | 72/82 [00:01<00:00, 72.59it/s]100%|██████████| 82/82 [00:01<00:00, 72.20it/s]
2024-06-06:14:03:09,883 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:14:03:09,883 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:14:03:09,883 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:14:03:09,883 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:14:03:09,884 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:14:03:09,884 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-06:14:03:09,884 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:14:03:09,903 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:15<21:30, 15.73s/it]Running generate_until requests:   2%|▏         | 2/83 [00:34<23:29, 17.40s/it]Running generate_until requests:   4%|▎         | 3/83 [00:44<18:38, 13.98s/it]Running generate_until requests:   5%|▍         | 4/83 [00:50<14:11, 10.78s/it]Running generate_until requests:   6%|▌         | 5/83 [00:59<13:26, 10.34s/it]Running generate_until requests:   7%|▋         | 6/83 [01:07<12:24,  9.66s/it]Running generate_until requests:   8%|▊         | 7/83 [01:25<15:34, 12.30s/it]Running generate_until requests:  10%|▉         | 8/83 [01:35<14:16, 11.42s/it]Running generate_until requests:  11%|█         | 9/83 [01:47<14:33, 11.81s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:57<13:22, 10.99s/it]Running generate_until requests:  13%|█▎        | 11/83 [02:05<12:19, 10.27s/it]Running generate_until requests:  14%|█▍        | 12/83 [02:19<13:34, 11.47s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:29<12:35, 10.80s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:38<11:51, 10.31s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:57<14:42, 12.98s/it]Running generate_until requests:  19%|█▉        | 16/83 [03:13<15:19, 13.73s/it]Running generate_until requests:  20%|██        | 17/83 [03:22<13:51, 12.59s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:32<12:32, 11.58s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:42<11:58, 11.23s/it]Running generate_until requests:  24%|██▍       | 20/83 [04:01<14:16, 13.59s/it]Running generate_until requests:  25%|██▌       | 21/83 [04:11<13:00, 12.59s/it]Running generate_until requests:  27%|██▋       | 22/83 [04:19<11:21, 11.18s/it]Running generate_until requests:  28%|██▊       | 23/83 [04:28<10:31, 10.52s/it]Running generate_until requests:  29%|██▉       | 24/83 [04:37<09:49,  9.99s/it]Running generate_until requests:  30%|███       | 25/83 [04:51<10:40, 11.04s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:59<09:42, 10.21s/it]Running generate_until requests:  33%|███▎      | 27/83 [05:13<10:46, 11.54s/it]Running generate_until requests:  34%|███▎      | 28/83 [05:22<09:45, 10.64s/it]Running generate_until requests:  35%|███▍      | 29/83 [05:34<09:58, 11.08s/it]Running generate_until requests:  36%|███▌      | 30/83 [05:45<09:44, 11.03s/it]Running generate_until requests:  37%|███▋      | 31/83 [06:02<11:12, 12.94s/it]Running generate_until requests:  39%|███▊      | 32/83 [06:16<11:06, 13.07s/it]Running generate_until requests:  40%|███▉      | 33/83 [06:21<09:01, 10.83s/it]Running generate_until requests:  41%|████      | 34/83 [06:40<10:39, 13.05s/it]Running generate_until requests:  42%|████▏     | 35/83 [06:47<09:03, 11.32s/it]Running generate_until requests:  43%|████▎     | 36/83 [06:57<08:37, 11.01s/it]Running generate_until requests:  45%|████▍     | 37/83 [07:07<08:02, 10.50s/it]Running generate_until requests:  46%|████▌     | 38/83 [07:19<08:16, 11.03s/it]Running generate_until requests:  47%|████▋     | 39/83 [07:28<07:37, 10.41s/it]Running generate_until requests:  48%|████▊     | 40/83 [07:43<08:27, 11.80s/it]Running generate_until requests:  49%|████▉     | 41/83 [07:49<07:07, 10.18s/it]Running generate_until requests:  51%|█████     | 42/83 [07:58<06:44,  9.86s/it]Running generate_until requests:  52%|█████▏    | 43/83 [08:09<06:44, 10.12s/it]Running generate_until requests:  53%|█████▎    | 44/83 [08:15<05:49,  8.95s/it]Running generate_until requests:  54%|█████▍    | 45/83 [08:23<05:30,  8.69s/it]Running generate_until requests:  55%|█████▌    | 46/83 [08:34<05:42,  9.25s/it]Running generate_until requests:  57%|█████▋    | 47/83 [08:41<05:14,  8.73s/it]Running generate_until requests:  58%|█████▊    | 48/83 [08:52<05:23,  9.24s/it]Running generate_until requests:  59%|█████▉    | 49/83 [08:58<04:41,  8.27s/it]Running generate_until requests:  60%|██████    | 50/83 [09:09<05:05,  9.26s/it]Running generate_until requests:  61%|██████▏   | 51/83 [09:19<04:59,  9.36s/it]Running generate_until requests:  63%|██████▎   | 52/83 [09:28<04:42,  9.11s/it]Running generate_until requests:  64%|██████▍   | 53/83 [09:40<05:01, 10.04s/it]Running generate_until requests:  65%|██████▌   | 54/83 [09:47<04:25,  9.14s/it]Running generate_until requests:  66%|██████▋   | 55/83 [09:57<04:22,  9.38s/it]Running generate_until requests:  67%|██████▋   | 56/83 [10:17<05:43, 12.72s/it]Running generate_until requests:  69%|██████▊   | 57/83 [10:31<05:38, 13.03s/it]Running generate_until requests:  70%|██████▉   | 58/83 [10:41<05:01, 12.08s/it]Running generate_until requests:  71%|███████   | 59/83 [10:47<04:09, 10.41s/it]Running generate_until requests:  72%|███████▏  | 60/83 [10:54<03:30,  9.15s/it]Running generate_until requests:  73%|███████▎  | 61/83 [11:06<03:40, 10.02s/it]Running generate_until requests:  75%|███████▍  | 62/83 [11:17<03:38, 10.42s/it]Running generate_until requests:  76%|███████▌  | 63/83 [11:24<03:06,  9.30s/it]Running generate_until requests:  77%|███████▋  | 64/83 [11:31<02:45,  8.71s/it]Running generate_until requests:  78%|███████▊  | 65/83 [11:37<02:24,  8.01s/it]Running generate_until requests:  80%|███████▉  | 66/83 [11:42<02:01,  7.13s/it]Running generate_until requests:  81%|████████  | 67/83 [11:47<01:42,  6.40s/it]Running generate_until requests:  82%|████████▏ | 68/83 [11:54<01:37,  6.52s/it]Running generate_until requests:  83%|████████▎ | 69/83 [12:09<02:08,  9.20s/it]Running generate_until requests:  84%|████████▍ | 70/83 [12:19<02:00,  9.26s/it]Running generate_until requests:  86%|████████▌ | 71/83 [12:24<01:34,  7.92s/it]Running generate_until requests:  87%|████████▋ | 72/83 [12:33<01:32,  8.39s/it]Running generate_until requests:  88%|████████▊ | 73/83 [12:40<01:19,  7.98s/it]Running generate_until requests:  89%|████████▉ | 74/83 [12:46<01:05,  7.31s/it]Running generate_until requests:  90%|█████████ | 75/83 [12:57<01:06,  8.31s/it]Running generate_until requests:  92%|█████████▏| 76/83 [13:09<01:06,  9.47s/it]Running generate_until requests:  93%|█████████▎| 77/83 [13:17<00:55,  9.23s/it]Running generate_until requests:  94%|█████████▍| 78/83 [13:25<00:43,  8.79s/it]Running generate_until requests:  95%|█████████▌| 79/83 [13:39<00:41, 10.31s/it]Running generate_until requests:  96%|█████████▋| 80/83 [13:46<00:28,  9.34s/it]Running generate_until requests:  98%|█████████▊| 81/83 [13:54<00:17,  8.90s/it]Running generate_until requests:  99%|█████████▉| 82/83 [14:00<00:07,  8.00s/it]Running generate_until requests: 100%|██████████| 83/83 [14:05<00:00,  7.14s/it]Running generate_until requests: 100%|██████████| 83/83 [14:05<00:00, 10.19s/it]
