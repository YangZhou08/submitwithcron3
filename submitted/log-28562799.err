Already on 'yangexp2'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/huggingface-cli", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py", line 51, in main
    service.run()
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/user.py", line 98, in run
    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 111, in login
    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 307, in _login
    raise ValueError("Invalid token passed!")
ValueError: Invalid token passed!
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:01:31:08,420 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:08,421 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:08,421 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:08,430 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:09,275 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:09,857 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:10,655 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:11,587 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:14,808 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:14,808 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:14,809 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:14,809 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:14,820 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:14,820 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:14,820 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
2024-06-04:01:31:14,820 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
2024-06-04:01:31:14,834 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:14,835 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:14,839 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:14,839 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
2024-06-04:01:31:15,175 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:15,176 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:15,181 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:15,181 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:01:31:19,047 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:19,049 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:19,057 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:19,057 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:01:31:20,146 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:20,147 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:20,153 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:20,153 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:01:31:21,710 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:21,712 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:21,718 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:21,718 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:01:31:25,407 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:25,410 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:25,421 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:25,421 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:02, 20.79s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:27<01:23, 27.98s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:24, 28.25s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:52, 17.53s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:24, 28.09s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:28<01:24, 28.09s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:10, 23.37s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:14, 24.88s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:54<00:54, 27.30s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:55<00:55, 27.88s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:45<00:46, 23.48s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:55<00:55, 27.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:52<00:52, 26.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:55<00:55, 27.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:49<00:50, 25.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:51<00:52, 26.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:25<00:28, 28.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:22<00:28, 28.29s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:26<00:29, 29.16s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:19<00:27, 27.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:26<00:29, 29.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:26<00:29, 29.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:15<00:26, 26.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:21<00:28, 28.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 19.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 22.99s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:27<00:00, 19.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:27<00:00, 21.80s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 19.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 23.01s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 19.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 23.02s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 19.99s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 23.02s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 19.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.19s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:21<00:00, 18.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:21<00:00, 20.40s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 19.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.51s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:01:33:32,170 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:32,250 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:32,253 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:32,549 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/83 [00:00<00:00, 194.84it/s] 48%|████▊     | 40/83 [00:00<00:00, 195.96it/s] 72%|███████▏  | 60/83 [00:00<00:00, 196.30it/s] 96%|█████████▋| 80/83 [00:00<00:00, 196.38it/s]100%|██████████| 83/83 [00:00<00:00, 196.22it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:34:05,977 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:05,980 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:06,382 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 14%|█▍        | 12/83 [00:00<00:00, 119.61it/s] 30%|███       | 25/83 [00:00<00:00, 120.31it/s] 46%|████▌     | 38/83 [00:00<00:00, 120.46it/s] 61%|██████▏   | 51/83 [00:00<00:00, 120.54it/s] 77%|███████▋  | 64/83 [00:00<00:00, 120.51it/s] 93%|█████████▎| 77/83 [00:00<00:00, 120.55it/s]100%|██████████| 83/83 [00:00<00:00, 120.48it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:34:07,999 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:08,001 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:34:08,397 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:08,399 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:34:08,462 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:08,465 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:08,501 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:34:08,596 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:08,599 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 12%|█▏        | 10/82 [00:00<00:00, 99.69it/s]2024-06-04:01:34:08,711 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 100.54it/s]2024-06-04:01:34:08,789 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 131.98it/s] 39%|███▉      | 32/82 [00:00<00:00, 100.71it/s]2024-06-04:01:34:08,911 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
 16%|█▌        | 13/82 [00:00<00:00, 126.83it/s]  0%|          | 0/82 [00:00<?, ?it/s] 34%|███▍      | 28/82 [00:00<00:00, 133.60it/s] 52%|█████▏    | 43/82 [00:00<00:00, 100.74it/s] 33%|███▎      | 27/82 [00:00<00:00, 129.52it/s] 17%|█▋        | 14/82 [00:00<00:00, 130.98it/s] 51%|█████     | 42/82 [00:00<00:00, 134.03it/s] 66%|██████▌   | 54/82 [00:00<00:00, 100.61it/s] 50%|█████     | 41/82 [00:00<00:00, 131.17it/s] 34%|███▍      | 28/82 [00:00<00:00, 131.98it/s] 68%|██████▊   | 56/82 [00:00<00:00, 134.35it/s] 79%|███████▉  | 65/82 [00:00<00:00, 100.71it/s] 67%|██████▋   | 55/82 [00:00<00:00, 131.66it/s] 51%|█████     | 42/82 [00:00<00:00, 132.40it/s] 85%|████████▌ | 70/82 [00:00<00:00, 134.56it/s] 93%|█████████▎| 76/82 [00:00<00:00, 100.88it/s] 84%|████████▍ | 69/82 [00:00<00:00, 131.97it/s]100%|██████████| 82/82 [00:00<00:00, 134.33it/s]
100%|██████████| 82/82 [00:00<00:00, 100.73it/s]
 68%|██████▊   | 56/82 [00:00<00:00, 132.24it/s]100%|██████████| 82/82 [00:00<00:00, 131.37it/s]
 85%|████████▌ | 70/82 [00:00<00:00, 131.38it/s]100%|██████████| 82/82 [00:00<00:00, 130.84it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:34:48,727 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:48,731 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:49,507 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s]  8%|▊         | 7/83 [00:00<00:01, 65.30it/s] 17%|█▋        | 14/83 [00:00<00:01, 60.15it/s] 25%|██▌       | 21/83 [00:00<00:01, 58.81it/s] 34%|███▎      | 28/83 [00:00<00:00, 61.49it/s] 42%|████▏     | 35/83 [00:00<00:00, 63.04it/s] 51%|█████     | 42/83 [00:00<00:00, 60.38it/s] 59%|█████▉    | 49/83 [00:00<00:00, 59.31it/s] 70%|██████▉   | 58/83 [00:00<00:00, 66.54it/s] 82%|████████▏ | 68/83 [00:01<00:00, 75.61it/s] 92%|█████████▏| 76/83 [00:01<00:00, 75.58it/s]100%|██████████| 83/83 [00:01<00:00, 69.42it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:35:38,310 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:35:38,313 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:35:39,062 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 109.67it/s] 28%|██▊       | 23/83 [00:00<00:00, 114.44it/s] 42%|████▏     | 35/83 [00:00<00:00, 116.74it/s] 57%|█████▋    | 47/83 [00:00<00:00, 115.40it/s] 71%|███████   | 59/83 [00:00<00:00, 116.31it/s] 86%|████████▌ | 71/83 [00:00<00:00, 116.44it/s]100%|██████████| 83/83 [00:00<00:00, 116.67it/s]100%|██████████| 83/83 [00:00<00:00, 115.96it/s]
2024-06-04:01:35:50,870 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:35:50,870 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:35:50,870 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:35:50,870 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:35:50,870 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:35:50,870 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:35:50,871 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:01:35:50,870 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:20<27:53, 20.41s/it]Running generate_until requests:   2%|▏         | 2/83 [00:38<25:59, 19.26s/it]Running generate_until requests:   4%|▎         | 3/83 [00:57<25:11, 18.90s/it]Running generate_until requests:   5%|▍         | 4/83 [01:15<24:35, 18.68s/it]Running generate_until requests:   6%|▌         | 5/83 [01:33<24:05, 18.53s/it]Running generate_until requests:   7%|▋         | 6/83 [01:52<23:37, 18.41s/it]Running generate_until requests:   8%|▊         | 7/83 [02:10<23:12, 18.32s/it]Running generate_until requests:  10%|▉         | 8/83 [02:28<22:49, 18.26s/it]Running generate_until requests:  11%|█         | 9/83 [02:46<22:29, 18.24s/it]Running generate_until requests:  12%|█▏        | 10/83 [03:04<22:10, 18.22s/it]Running generate_until requests:  13%|█▎        | 11/83 [03:22<21:50, 18.20s/it]Running generate_until requests:  14%|█▍        | 12/83 [03:41<21:32, 18.20s/it]Running generate_until requests:  16%|█▌        | 13/83 [03:59<21:14, 18.20s/it]Running generate_until requests:  17%|█▋        | 14/83 [04:17<20:55, 18.20s/it]Running generate_until requests:  18%|█▊        | 15/83 [04:35<20:36, 18.19s/it]Running generate_until requests:  19%|█▉        | 16/83 [04:53<20:17, 18.17s/it]Running generate_until requests:  20%|██        | 17/83 [05:11<19:59, 18.17s/it]Running generate_until requests:  22%|██▏       | 18/83 [05:30<19:41, 18.18s/it]Running generate_until requests:  23%|██▎       | 19/83 [05:48<19:22, 18.17s/it]Running generate_until requests:  24%|██▍       | 20/83 [06:06<19:04, 18.16s/it]Running generate_until requests:  25%|██▌       | 21/83 [06:24<18:42, 18.10s/it]Running generate_until requests:  27%|██▋       | 22/83 [06:42<18:22, 18.07s/it]Running generate_until requests:  28%|██▊       | 23/83 [07:00<18:02, 18.04s/it]Running generate_until requests:  29%|██▉       | 24/83 [07:18<17:43, 18.02s/it]Running generate_until requests:  30%|███       | 25/83 [07:36<17:24, 18.01s/it]Running generate_until requests:  31%|███▏      | 26/83 [07:54<17:05, 18.00s/it]Running generate_until requests:  33%|███▎      | 27/83 [08:12<16:47, 17.99s/it]Running generate_until requests:  34%|███▎      | 28/83 [08:30<16:29, 18.00s/it]Running generate_until requests:  35%|███▍      | 29/83 [08:48<16:11, 17.99s/it]Running generate_until requests:  36%|███▌      | 30/83 [09:06<15:52, 17.98s/it]Running generate_until requests:  37%|███▋      | 31/83 [09:24<15:34, 17.97s/it]Running generate_until requests:  39%|███▊      | 32/83 [09:42<15:16, 17.96s/it]Running generate_until requests:  40%|███▉      | 33/83 [10:00<14:57, 17.96s/it]Running generate_until requests:  41%|████      | 34/83 [10:18<14:39, 17.95s/it]Running generate_until requests:  42%|████▏     | 35/83 [10:35<14:21, 17.95s/it]Running generate_until requests:  43%|████▎     | 36/83 [10:53<14:03, 17.95s/it]Running generate_until requests:  45%|████▍     | 37/83 [11:05<12:18, 16.05s/it]Running generate_until requests:  46%|████▌     | 38/83 [11:23<12:27, 16.61s/it]Running generate_until requests:  47%|████▋     | 39/83 [11:41<12:26, 16.98s/it]Running generate_until requests:  48%|████▊     | 40/83 [11:59<12:21, 17.25s/it]Running generate_until requests:  49%|████▉     | 41/83 [12:17<12:11, 17.43s/it]Running generate_until requests:  51%|█████     | 42/83 [12:34<12:00, 17.57s/it]Running generate_until requests:  52%|█████▏    | 43/83 [12:52<11:46, 17.67s/it]Running generate_until requests:  53%|█████▎    | 44/83 [13:10<11:31, 17.73s/it]Running generate_until requests:  54%|█████▍    | 45/83 [13:28<11:15, 17.78s/it]Running generate_until requests:  55%|█████▌    | 46/83 [13:46<10:58, 17.80s/it]Running generate_until requests:  57%|█████▋    | 47/83 [14:04<10:41, 17.82s/it]Running generate_until requests:  58%|█████▊    | 48/83 [14:22<10:24, 17.85s/it]Running generate_until requests:  59%|█████▉    | 49/83 [14:40<10:07, 17.87s/it]Running generate_until requests:  60%|██████    | 50/83 [14:57<09:49, 17.85s/it]Running generate_until requests:  61%|██████▏   | 51/83 [15:15<09:30, 17.83s/it]Running generate_until requests:  63%|██████▎   | 52/83 [15:33<09:12, 17.83s/it]Running generate_until requests:  64%|██████▍   | 53/83 [15:51<08:54, 17.83s/it]Running generate_until requests:  65%|██████▌   | 54/83 [16:09<08:36, 17.81s/it]Running generate_until requests:  66%|██████▋   | 55/83 [16:26<08:19, 17.82s/it]Running generate_until requests:  67%|██████▋   | 56/83 [16:44<08:00, 17.81s/it]Running generate_until requests:  69%|██████▊   | 57/83 [17:02<07:43, 17.82s/it]Running generate_until requests:  70%|██████▉   | 58/83 [17:20<07:25, 17.81s/it]Running generate_until requests:  71%|███████   | 59/83 [17:38<07:07, 17.80s/it]Running generate_until requests:  72%|███████▏  | 60/83 [17:55<06:49, 17.81s/it]Running generate_until requests:  73%|███████▎  | 61/83 [18:13<06:31, 17.80s/it]Running generate_until requests:  75%|███████▍  | 62/83 [18:31<06:13, 17.81s/it]Running generate_until requests:  76%|███████▌  | 63/83 [18:49<05:56, 17.80s/it]Running generate_until requests:  77%|███████▋  | 64/83 [19:07<05:38, 17.81s/it]Running generate_until requests:  78%|███████▊  | 65/83 [19:25<05:20, 17.82s/it]Running generate_until requests:  80%|███████▉  | 66/83 [19:42<05:02, 17.82s/it]Running generate_until requests:  81%|████████  | 67/83 [20:00<04:45, 17.82s/it]Running generate_until requests:  82%|████████▏ | 68/83 [20:18<04:27, 17.80s/it]Running generate_until requests:  83%|████████▎ | 69/83 [20:36<04:09, 17.79s/it]Running generate_until requests:  84%|████████▍ | 70/83 [20:53<03:51, 17.78s/it]Running generate_until requests:  86%|████████▌ | 71/83 [21:11<03:33, 17.77s/it]Running generate_until requests:  87%|████████▋ | 72/83 [21:29<03:15, 17.76s/it]Running generate_until requests:  88%|████████▊ | 73/83 [21:47<02:57, 17.76s/it]Running generate_until requests:  89%|████████▉ | 74/83 [22:04<02:39, 17.75s/it]Running generate_until requests:  90%|█████████ | 75/83 [22:22<02:21, 17.74s/it]Running generate_until requests:  92%|█████████▏| 76/83 [22:40<02:03, 17.71s/it]Running generate_until requests:  93%|█████████▎| 77/83 [22:57<01:46, 17.68s/it]Running generate_until requests:  94%|█████████▍| 78/83 [23:15<01:28, 17.66s/it]Running generate_until requests:  95%|█████████▌| 79/83 [23:33<01:10, 17.64s/it]Running generate_until requests:  96%|█████████▋| 80/83 [23:50<00:52, 17.64s/it]Running generate_until requests:  98%|█████████▊| 81/83 [24:08<00:35, 17.64s/it]Running generate_until requests:  99%|█████████▉| 82/83 [24:25<00:17, 17.59s/it]Running generate_until requests: 100%|██████████| 83/83 [24:43<00:00, 17.62s/it]Running generate_until requests: 100%|██████████| 83/83 [24:43<00:00, 17.87s/it]
[2024-06-04 02:15:56,142] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -7) local_rank: 0 (pid: 472693) of binary: /private/home/beidic/.conda/envs/griffin/bin/python
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
main.py FAILED
------------------------------------------------------
Failures:
[1]:
  time      : 2024-06-04_02:15:56
  host      : learnfair5243.h2.fair
  rank      : 1 (local_rank: 1)
  exitcode  : -7 (pid: 472694)
  error_file: <N/A>
  traceback : Signal 7 (SIGBUS) received by PID 472694
[2]:
  time      : 2024-06-04_02:15:56
  host      : learnfair5243.h2.fair
  rank      : 2 (local_rank: 2)
  exitcode  : -7 (pid: 472695)
  error_file: <N/A>
  traceback : Signal 7 (SIGBUS) received by PID 472695
[3]:
  time      : 2024-06-04_02:15:56
  host      : learnfair5243.h2.fair
  rank      : 3 (local_rank: 3)
  exitcode  : -7 (pid: 472696)
  error_file: <N/A>
  traceback : Signal 7 (SIGBUS) received by PID 472696
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_02:15:56
  host      : learnfair5243.h2.fair
  rank      : 0 (local_rank: 0)
  exitcode  : -7 (pid: 472693)
  error_file: <N/A>
  traceback : Signal 7 (SIGBUS) received by PID 472693
======================================================
/var/spool/slurm//job28562799/slurm_script: line 65: 472654 Bus error               (core dumped) accelerate launch --main_process_port 29510 --num_processes 8 --num_machines 1 main.py --model xhf --model_args pretrained=meta-llama/Meta-Llama-3-8B-Instruct,cats=True,spr=$sparsitylevel,check=False --tasks gsm8k --batch_size 1 --limit 0.5
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:02:16:10,155 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:16:10,156 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:16:10,157 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:16:10,166 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:16:10,242 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:16:10,304 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:16:10,573 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:16:12,456 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:16:16,820 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:16:16,821 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:16:16,826 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:16:16,826 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
2024-06-04:02:16:17,171 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:16:17,173 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:16:17,177 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:16:17,177 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:16:17,178 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
2024-06-04:02:16:17,178 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:16:17,182 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:16:17,182 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
2024-06-04:02:16:17,245 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:16:17,246 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:16:17,251 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:16:17,251 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
2024-06-04:02:16:17,258 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:16:17,260 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:16:17,264 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:16:17,264 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
2024-06-04:02:16:17,731 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:16:17,732 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:16:17,738 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:16:17,738 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:16:20,704 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:16:20,706 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:16:20,711 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:16:20,711 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.72s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.94s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.82s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.82s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.79s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.84s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:16:22,733 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:16:22,735 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:16:22,742 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:16:22,742 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.27s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.98s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.89s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.16s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.32s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.28s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.30s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.33s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.72s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:16,  5.55s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:04,  4.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.07s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:17:43,329 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:17:43,413 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:17:43,415 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:17:43,711 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 136.76it/s] 34%|███▎      | 28/83 [00:00<00:00, 137.57it/s] 51%|█████     | 42/83 [00:00<00:00, 137.83it/s] 67%|██████▋   | 56/83 [00:00<00:00, 137.94it/s] 84%|████████▍ | 70/83 [00:00<00:00, 138.00it/s]100%|██████████| 83/83 [00:00<00:00, 137.88it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:17:47,185 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:17:47,187 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:17:47,396 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:17:47,399 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:17:47,400 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:17:47,429 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:17:47,431 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 16%|█▌        | 13/82 [00:00<00:00, 129.06it/s] 32%|███▏      | 26/82 [00:00<00:00, 128.83it/s] 48%|████▊     | 39/82 [00:00<00:00, 128.62it/s]2024-06-04:02:17:47,714 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:02:17:47,784 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 65%|██████▍   | 53/82 [00:00<00:00, 130.79it/s] 17%|█▋        | 14/83 [00:00<00:00, 133.74it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 17%|█▋        | 14/82 [00:00<00:00, 133.28it/s] 82%|████████▏ | 67/82 [00:00<00:00, 132.46it/s] 34%|███▎      | 28/83 [00:00<00:00, 134.64it/s] 34%|███▍      | 28/82 [00:00<00:00, 135.31it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:17:48,011 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:17:48,013 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 99%|█████████▉| 81/82 [00:00<00:00, 133.31it/s]100%|██████████| 82/82 [00:00<00:00, 131.80it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 51%|█████     | 42/83 [00:00<00:00, 134.23it/s] 51%|█████     | 42/82 [00:00<00:00, 134.23it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:17:48,121 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:17:48,124 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 67%|██████▋   | 56/83 [00:00<00:00, 132.06it/s] 68%|██████▊   | 56/82 [00:00<00:00, 131.31it/s] 84%|████████▍ | 70/83 [00:00<00:00, 130.45it/s]2024-06-04:02:17:48,320 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
 85%|████████▌ | 70/82 [00:00<00:00, 129.04it/s]  0%|          | 0/83 [00:00<?, ?it/s]100%|██████████| 83/83 [00:00<00:00, 131.18it/s]
100%|██████████| 82/82 [00:00<00:00, 129.73it/s]
2024-06-04:02:17:48,434 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
 16%|█▌        | 13/83 [00:00<00:00, 127.07it/s]  0%|          | 0/82 [00:00<?, ?it/s] 31%|███▏      | 26/83 [00:00<00:00, 120.57it/s] 16%|█▌        | 13/82 [00:00<00:00, 127.96it/s] 32%|███▏      | 26/82 [00:00<00:00, 128.89it/s] 47%|████▋     | 39/83 [00:00<00:00, 116.63it/s] 48%|████▊     | 39/82 [00:00<00:00, 124.14it/s] 61%|██████▏   | 51/83 [00:00<00:00, 113.24it/s] 63%|██████▎   | 52/82 [00:00<00:00, 115.18it/s] 76%|███████▌  | 63/83 [00:00<00:00, 109.62it/s] 89%|████████▉ | 74/83 [00:00<00:00, 107.51it/s] 78%|███████▊  | 64/82 [00:00<00:00, 111.10it/s]100%|██████████| 83/83 [00:00<00:00, 110.67it/s]
 93%|█████████▎| 76/82 [00:00<00:00, 108.62it/s]100%|██████████| 82/82 [00:00<00:00, 113.13it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:18:38,470 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:18:38,474 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:18:38,958 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 100.33it/s] 27%|██▋       | 22/83 [00:00<00:00, 102.64it/s] 40%|███▉      | 33/83 [00:00<00:00, 89.51it/s]  52%|█████▏    | 43/83 [00:00<00:00, 72.82it/s] 61%|██████▏   | 51/83 [00:00<00:00, 73.95it/s] 72%|███████▏  | 60/83 [00:00<00:00, 77.83it/s] 83%|████████▎ | 69/83 [00:00<00:00, 72.07it/s] 93%|█████████▎| 77/83 [00:01<00:00, 71.54it/s]100%|██████████| 83/83 [00:01<00:00, 78.29it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:18:47,767 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:18:47,770 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:18:48,237 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 105.93it/s] 27%|██▋       | 22/82 [00:00<00:00, 106.39it/s] 40%|████      | 33/82 [00:00<00:00, 106.66it/s] 54%|█████▎    | 44/82 [00:00<00:00, 106.82it/s] 67%|██████▋   | 55/82 [00:00<00:00, 106.91it/s] 80%|████████  | 66/82 [00:00<00:00, 106.48it/s] 94%|█████████▍| 77/82 [00:00<00:00, 106.77it/s]100%|██████████| 82/82 [00:00<00:00, 106.66it/s]
2024-06-04:02:19:00,003 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:19:00,003 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:19:00,003 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:19:00,003 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:19:00,003 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:19:00,003 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:19:00,003 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:19:00,004 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:23<32:05, 23.48s/it]Running generate_until requests:   2%|▏         | 2/83 [00:45<30:11, 22.37s/it]Running generate_until requests:   4%|▎         | 3/83 [01:06<29:21, 22.01s/it]Running generate_until requests:   5%|▍         | 4/83 [01:28<28:42, 21.81s/it]Running generate_until requests:   6%|▌         | 5/83 [01:49<28:12, 21.70s/it]Running generate_until requests:   7%|▋         | 6/83 [02:11<27:43, 21.61s/it]Running generate_until requests:   8%|▊         | 7/83 [02:32<27:17, 21.55s/it]Running generate_until requests:  10%|▉         | 8/83 [02:53<26:54, 21.53s/it]Running generate_until requests:  11%|█         | 9/83 [03:15<26:30, 21.49s/it]Running generate_until requests:  12%|█▏        | 10/83 [03:26<22:06, 18.17s/it]Running generate_until requests:  13%|█▎        | 11/83 [03:47<22:59, 19.15s/it]Running generate_until requests:  14%|█▍        | 12/83 [04:08<23:27, 19.82s/it]Running generate_until requests:  16%|█▌        | 13/83 [04:30<23:40, 20.29s/it]Running generate_until requests:  17%|█▋        | 14/83 [04:51<23:40, 20.59s/it]Running generate_until requests:  18%|█▊        | 15/83 [05:04<20:42, 18.27s/it]Running generate_until requests:  19%|█▉        | 16/83 [05:25<21:24, 19.18s/it]Running generate_until requests:  20%|██        | 17/83 [05:46<21:47, 19.80s/it]Running generate_until requests:  22%|██▏       | 18/83 [06:08<21:53, 20.21s/it]Running generate_until requests:  23%|██▎       | 19/83 [06:29<21:52, 20.51s/it]Running generate_until requests:  24%|██▍       | 20/83 [06:50<21:45, 20.72s/it]Running generate_until requests:  25%|██▌       | 21/83 [07:11<21:31, 20.83s/it]Running generate_until requests:  27%|██▋       | 22/83 [07:32<21:15, 20.90s/it]Running generate_until requests:  28%|██▊       | 23/83 [07:53<20:57, 20.96s/it]Running generate_until requests:  29%|██▉       | 24/83 [08:14<20:39, 21.00s/it]Running generate_until requests:  30%|███       | 25/83 [08:35<20:19, 21.03s/it]Running generate_until requests:  31%|███▏      | 26/83 [08:46<17:07, 18.02s/it]Running generate_until requests:  33%|███▎      | 27/83 [09:08<17:40, 18.95s/it]Running generate_until requests:  34%|███▎      | 28/83 [09:29<17:57, 19.59s/it]Running generate_until requests:  35%|███▍      | 29/83 [09:50<18:01, 20.03s/it]Running generate_until requests:  36%|███▌      | 30/83 [10:11<17:59, 20.36s/it]Running generate_until requests:  37%|███▋      | 31/83 [10:32<17:50, 20.58s/it]Running generate_until requests:  39%|███▊      | 32/83 [10:53<17:35, 20.70s/it]Running generate_until requests:  40%|███▉      | 33/83 [11:18<18:18, 21.96s/it]Running generate_until requests:  41%|████      | 34/83 [11:39<17:44, 21.73s/it]Running generate_until requests:  42%|████▏     | 35/83 [12:00<17:12, 21.52s/it]Running generate_until requests:  43%|████▎     | 36/83 [12:21<16:45, 21.39s/it]Running generate_until requests:  45%|████▍     | 37/83 [12:35<14:39, 19.11s/it]Running generate_until requests:  46%|████▌     | 38/83 [12:56<14:44, 19.66s/it]Running generate_until requests:  47%|████▋     | 39/83 [13:03<11:35, 15.82s/it]Running generate_until requests:  48%|████▊     | 40/83 [13:24<12:26, 17.35s/it]Running generate_until requests:  49%|████▉     | 41/83 [13:45<12:54, 18.45s/it]Running generate_until requests:  51%|█████     | 42/83 [14:06<13:06, 19.19s/it]Running generate_until requests:  52%|█████▏    | 43/83 [14:27<13:09, 19.73s/it]Running generate_until requests:  53%|█████▎    | 44/83 [14:35<10:37, 16.35s/it]Running generate_until requests:  54%|█████▍    | 45/83 [14:56<11:09, 17.62s/it]Running generate_until requests:  55%|█████▌    | 46/83 [15:17<11:29, 18.64s/it]Running generate_until requests:  57%|█████▋    | 47/83 [15:38<11:35, 19.31s/it]Running generate_until requests:  58%|█████▊    | 48/83 [15:52<10:20, 17.72s/it]Running generate_until requests:  59%|█████▉    | 49/83 [16:13<10:35, 18.70s/it]Running generate_until requests:  60%|██████    | 50/83 [16:33<10:38, 19.34s/it]Running generate_until requests:  61%|██████▏   | 51/83 [16:54<10:34, 19.81s/it]Running generate_until requests:  63%|██████▎   | 52/83 [17:15<10:22, 20.07s/it]Running generate_until requests:  64%|██████▍   | 53/83 [17:36<10:09, 20.32s/it]Running generate_until requests:  65%|██████▌   | 54/83 [17:57<09:53, 20.47s/it]Running generate_until requests:  66%|██████▋   | 55/83 [18:18<09:36, 20.60s/it]Running generate_until requests:  67%|██████▋   | 56/83 [18:38<09:17, 20.66s/it]Running generate_until requests:  69%|██████▊   | 57/83 [18:59<08:59, 20.74s/it]Running generate_until requests:  70%|██████▉   | 58/83 [19:13<07:46, 18.64s/it]Running generate_until requests:  71%|███████   | 59/83 [19:34<07:42, 19.26s/it]Running generate_until requests:  72%|███████▏  | 60/83 [19:55<07:34, 19.76s/it]Running generate_until requests:  73%|███████▎  | 61/83 [20:15<07:21, 20.07s/it]Running generate_until requests:  75%|███████▍  | 62/83 [20:36<07:06, 20.32s/it]Running generate_until requests:  76%|███████▌  | 63/83 [20:57<06:49, 20.47s/it]Running generate_until requests:  77%|███████▋  | 64/83 [21:18<06:31, 20.62s/it]Running generate_until requests:  78%|███████▊  | 65/83 [21:39<06:12, 20.68s/it]Running generate_until requests:  80%|███████▉  | 66/83 [21:58<05:43, 20.23s/it]Running generate_until requests:  81%|████████  | 67/83 [22:19<05:26, 20.41s/it]Running generate_until requests:  82%|████████▏ | 68/83 [22:40<05:07, 20.53s/it]Running generate_until requests:  83%|████████▎ | 69/83 [23:01<04:48, 20.59s/it]Running generate_until requests:  84%|████████▍ | 70/83 [23:21<04:28, 20.67s/it]Running generate_until requests:  86%|████████▌ | 71/83 [23:42<04:08, 20.69s/it]Running generate_until requests:  87%|████████▋ | 72/83 [24:03<03:48, 20.73s/it]Running generate_until requests:  88%|████████▊ | 73/83 [24:24<03:27, 20.75s/it]Running generate_until requests:  89%|████████▉ | 74/83 [24:44<03:06, 20.74s/it]Running generate_until requests:  90%|█████████ | 75/83 [25:05<02:46, 20.76s/it]Running generate_until requests:  92%|█████████▏| 76/83 [25:26<02:25, 20.75s/it]Running generate_until requests:  93%|█████████▎| 77/83 [25:46<02:03, 20.62s/it]Running generate_until requests:  94%|█████████▍| 78/83 [26:07<01:43, 20.64s/it]Running generate_until requests:  95%|█████████▌| 79/83 [26:28<01:22, 20.70s/it]Running generate_until requests:  96%|█████████▋| 80/83 [26:49<01:02, 20.73s/it]Running generate_until requests:  98%|█████████▊| 81/83 [27:10<00:41, 20.78s/it]Running generate_until requests:  99%|█████████▉| 82/83 [27:30<00:20, 20.79s/it]Running generate_until requests: 100%|██████████| 83/83 [27:51<00:00, 20.79s/it]Running generate_until requests: 100%|██████████| 83/83 [27:51<00:00, 20.14s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:03:04:59,646 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:04:59,647 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:04:59,664 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:04:59,729 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:04:59,800 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:00,187 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:00,342 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:01,886 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:06,512 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:06,513 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:06,514 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:06,514 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:06,518 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:06,518 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
2024-06-04:03:05:06,518 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:06,518 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
2024-06-04:03:05:06,533 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:06,534 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:06,538 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:06,538 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
2024-06-04:03:05:06,542 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:06,544 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:06,548 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:06,548 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
2024-06-04:03:05:06,637 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:06,639 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:06,643 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:06,643 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
2024-06-04:03:05:07,195 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:07,196 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:07,200 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:07,200 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:05:10,534 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:10,536 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:10,545 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:10,545 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.94s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.97s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.93s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.98s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.98s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.17s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:05:12,654 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:12,659 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:12,676 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:12,676 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.93s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.04s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.10s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.17s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.82s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.83s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.83s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.86s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.33s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.33s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.32s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.34s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.33s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.38s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.72s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:36,985 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:36,987 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:37,027 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:37,029 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:37,127 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:37,129 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:06:37,160 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:37,201 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:37,201 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:37,203 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:37,204 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:37,217 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:37,233 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:37,235 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:03:06:37,320 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
2024-06-04:03:06:37,330 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/82 [00:00<?, ?it/s] 16%|█▌        | 13/83 [00:00<00:00, 127.50it/s]  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 193.61it/s] 31%|███▏      | 26/83 [00:00<00:00, 127.89it/s] 16%|█▌        | 13/83 [00:00<00:00, 128.83it/s]2024-06-04:03:06:37,517 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
 49%|████▉     | 40/82 [00:00<00:00, 195.31it/s]  0%|          | 0/82 [00:00<?, ?it/s] 47%|████▋     | 39/83 [00:00<00:00, 128.04it/s] 31%|███▏      | 26/83 [00:00<00:00, 129.40it/s]2024-06-04:03:06:37,617 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
 73%|███████▎  | 60/82 [00:00<00:00, 196.05it/s]  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 131.72it/s] 64%|██████▍   | 53/83 [00:00<00:00, 129.92it/s] 47%|████▋     | 39/83 [00:00<00:00, 129.27it/s]2024-06-04:03:06:37,665 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 98%|█████████▊| 80/82 [00:00<00:00, 196.43it/s] 16%|█▌        | 13/83 [00:00<00:00, 128.57it/s]100%|██████████| 82/82 [00:00<00:00, 196.07it/s]
 34%|███▍      | 28/82 [00:00<00:00, 132.10it/s] 81%|████████  | 67/83 [00:00<00:00, 130.80it/s] 63%|██████▎   | 52/83 [00:00<00:00, 129.37it/s] 14%|█▍        | 12/83 [00:00<00:00, 119.00it/s] 33%|███▎      | 27/83 [00:00<00:00, 130.70it/s] 51%|█████     | 42/82 [00:00<00:00, 134.18it/s] 98%|█████████▊| 81/83 [00:00<00:00, 132.87it/s] 80%|███████▉  | 66/83 [00:00<00:00, 130.63it/s]100%|██████████| 83/83 [00:00<00:00, 131.62it/s]
 29%|██▉       | 24/83 [00:00<00:00, 119.57it/s] 77%|███████▋  | 63/82 [00:00<00:00, 161.12it/s] 49%|████▉     | 41/83 [00:00<00:00, 129.16it/s] 96%|█████████▋| 80/83 [00:00<00:00, 131.38it/s]100%|██████████| 83/83 [00:00<00:00, 130.56it/s]
 45%|████▍     | 37/83 [00:00<00:00, 120.23it/s]100%|██████████| 82/82 [00:00<00:00, 161.08it/s]
 71%|███████   | 59/83 [00:00<00:00, 147.33it/s] 60%|██████    | 50/83 [00:00<00:00, 119.16it/s] 96%|█████████▋| 80/83 [00:00<00:00, 168.38it/s]100%|██████████| 83/83 [00:00<00:00, 155.16it/s]
 75%|███████▍  | 62/83 [00:00<00:00, 119.36it/s] 89%|████████▉ | 74/83 [00:00<00:00, 116.59it/s]100%|██████████| 83/83 [00:00<00:00, 118.25it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:07:16,750 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:07:16,754 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:07:17,383 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s]  7%|▋         | 6/82 [00:00<00:01, 56.38it/s] 20%|█▉        | 16/82 [00:00<00:00, 72.07it/s] 32%|███▏      | 26/82 [00:00<00:00, 81.89it/s] 43%|████▎     | 35/82 [00:00<00:00, 74.83it/s] 56%|█████▌    | 46/82 [00:00<00:00, 79.99it/s] 67%|██████▋   | 55/82 [00:00<00:00, 77.48it/s] 77%|███████▋  | 63/82 [00:00<00:00, 77.68it/s] 90%|█████████ | 74/82 [00:00<00:00, 85.57it/s]100%|██████████| 82/82 [00:01<00:00, 79.92it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:07:33,522 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:07:33,526 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:07:34,607 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s]  7%|▋         | 6/82 [00:00<00:01, 55.58it/s] 15%|█▍        | 12/82 [00:00<00:01, 53.17it/s] 22%|██▏       | 18/82 [00:00<00:01, 54.48it/s] 29%|██▉       | 24/82 [00:00<00:01, 54.00it/s] 37%|███▋      | 30/82 [00:00<00:00, 55.78it/s] 44%|████▍     | 36/82 [00:00<00:00, 55.83it/s] 51%|█████     | 42/82 [00:00<00:00, 54.38it/s] 60%|█████▉    | 49/82 [00:00<00:00, 54.91it/s] 67%|██████▋   | 55/82 [00:01<00:00, 52.99it/s] 76%|███████▌  | 62/82 [00:01<00:00, 54.04it/s] 83%|████████▎ | 68/82 [00:01<00:00, 54.17it/s] 90%|█████████ | 74/82 [00:01<00:00, 51.61it/s] 98%|█████████▊| 80/82 [00:01<00:00, 53.52it/s]100%|██████████| 82/82 [00:01<00:00, 53.57it/s]
2024-06-04:03:07:45,891 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:45,891 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:45,891 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:45,891 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:45,891 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:45,891 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:45,891 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:45,892 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:19<27:14, 19.93s/it]Running generate_until requests:   2%|▏         | 2/83 [00:38<25:44, 19.07s/it]Running generate_until requests:   4%|▎         | 3/83 [00:56<25:03, 18.79s/it]Running generate_until requests:   5%|▍         | 4/83 [01:00<16:58, 12.89s/it]Running generate_until requests:   6%|▌         | 5/83 [01:18<19:15, 14.81s/it]Running generate_until requests:   7%|▋         | 6/83 [01:37<20:27, 15.94s/it]Running generate_until requests:   8%|▊         | 7/83 [01:55<21:06, 16.66s/it]Running generate_until requests:  10%|▉         | 8/83 [02:13<21:24, 17.13s/it]Running generate_until requests:  11%|█         | 9/83 [02:28<20:21, 16.51s/it]Running generate_until requests:  12%|█▏        | 10/83 [02:36<16:50, 13.85s/it]Running generate_until requests:  13%|█▎        | 11/83 [02:54<18:12, 15.17s/it]Running generate_until requests:  14%|█▍        | 12/83 [03:12<19:01, 16.07s/it]Running generate_until requests:  16%|█▌        | 13/83 [03:18<15:09, 12.99s/it]Running generate_until requests:  17%|█▋        | 14/83 [03:36<16:40, 14.49s/it]Running generate_until requests:  18%|█▊        | 15/83 [03:54<17:39, 15.58s/it]Running generate_until requests:  19%|█▉        | 16/83 [04:12<18:14, 16.33s/it]Running generate_until requests:  20%|██        | 17/83 [04:30<18:32, 16.85s/it]Running generate_until requests:  22%|██▏       | 18/83 [04:48<18:38, 17.21s/it]Running generate_until requests:  23%|██▎       | 19/83 [05:06<18:38, 17.48s/it]Running generate_until requests:  24%|██▍       | 20/83 [05:25<18:32, 17.66s/it]Running generate_until requests:  25%|██▌       | 21/83 [05:42<18:19, 17.74s/it]Running generate_until requests:  27%|██▋       | 22/83 [05:52<15:26, 15.20s/it]Running generate_until requests:  28%|██▊       | 23/83 [06:10<16:00, 16.01s/it]Running generate_until requests:  29%|██▉       | 24/83 [06:28<16:18, 16.58s/it]Running generate_until requests:  30%|███       | 25/83 [06:45<16:25, 17.00s/it]Running generate_until requests:  31%|███▏      | 26/83 [07:03<16:24, 17.27s/it]Running generate_until requests:  33%|███▎      | 27/83 [07:14<14:21, 15.38s/it]Running generate_until requests:  34%|███▎      | 28/83 [07:22<12:04, 13.17s/it]Running generate_until requests:  35%|███▍      | 29/83 [07:30<10:17, 11.44s/it]Running generate_until requests:  36%|███▌      | 30/83 [07:48<11:49, 13.38s/it]Running generate_until requests:  37%|███▋      | 31/83 [08:06<12:46, 14.73s/it]Running generate_until requests:  39%|███▊      | 32/83 [08:13<10:44, 12.65s/it]Running generate_until requests:  40%|███▉      | 33/83 [08:31<11:50, 14.21s/it]Running generate_until requests:  41%|████      | 34/83 [08:49<12:30, 15.32s/it]Running generate_until requests:  42%|████▏     | 35/83 [09:07<12:52, 16.10s/it]Running generate_until requests:  43%|████▎     | 36/83 [09:25<13:02, 16.65s/it]Running generate_until requests:  45%|████▍     | 37/83 [09:43<13:02, 17.02s/it]Running generate_until requests:  46%|████▌     | 38/83 [10:01<12:57, 17.27s/it]Running generate_until requests:  47%|████▋     | 39/83 [10:08<10:24, 14.20s/it]Running generate_until requests:  48%|████▊     | 40/83 [10:26<10:58, 15.31s/it]Running generate_until requests:  49%|████▉     | 41/83 [10:30<08:23, 11.98s/it]Running generate_until requests:  51%|█████     | 42/83 [10:48<09:25, 13.78s/it]Running generate_until requests:  52%|█████▏    | 43/83 [11:00<08:52, 13.31s/it]Running generate_until requests:  53%|█████▎    | 44/83 [11:18<09:32, 14.67s/it]Running generate_until requests:  54%|█████▍    | 45/83 [11:27<08:13, 12.98s/it]Running generate_until requests:  55%|█████▌    | 46/83 [11:45<08:54, 14.44s/it]Running generate_until requests:  57%|█████▋    | 47/83 [11:53<07:30, 12.52s/it]Running generate_until requests:  58%|█████▊    | 48/83 [12:00<06:17, 10.79s/it]Running generate_until requests:  59%|█████▉    | 49/83 [12:10<06:05, 10.75s/it]Running generate_until requests:  60%|██████    | 50/83 [12:18<05:20,  9.71s/it]Running generate_until requests:  61%|██████▏   | 51/83 [12:32<05:53, 11.06s/it]Running generate_until requests:  63%|██████▎   | 52/83 [12:42<05:31, 10.69s/it]Running generate_until requests:  64%|██████▍   | 53/83 [12:59<06:24, 12.82s/it]Running generate_until requests:  65%|██████▌   | 54/83 [13:17<06:55, 14.32s/it]Running generate_until requests:  66%|██████▋   | 55/83 [13:24<05:39, 12.13s/it]Running generate_until requests:  67%|██████▋   | 56/83 [13:42<06:14, 13.86s/it]Running generate_until requests:  69%|██████▊   | 57/83 [14:00<06:31, 15.06s/it]Running generate_until requests:  70%|██████▉   | 58/83 [14:18<06:37, 15.90s/it]Running generate_until requests:  71%|███████   | 59/83 [14:24<05:12, 13.03s/it]Running generate_until requests:  72%|███████▏  | 60/83 [14:42<05:32, 14.46s/it]Running generate_until requests:  73%|███████▎  | 61/83 [15:00<05:39, 15.43s/it]Running generate_until requests:  75%|███████▍  | 62/83 [15:06<04:24, 12.58s/it]Running generate_until requests:  76%|███████▌  | 63/83 [15:13<03:41, 11.07s/it]Running generate_until requests:  77%|███████▋  | 64/83 [15:31<04:08, 13.10s/it]Running generate_until requests:  78%|███████▊  | 65/83 [15:37<03:17, 11.00s/it]Running generate_until requests:  80%|███████▉  | 66/83 [15:55<03:41, 13.04s/it]Running generate_until requests:  81%|████████  | 67/83 [16:13<03:51, 14.47s/it]Running generate_until requests:  82%|████████▏ | 68/83 [16:30<03:51, 15.45s/it]Running generate_until requests:  83%|████████▎ | 69/83 [16:48<03:46, 16.15s/it]Running generate_until requests:  84%|████████▍ | 70/83 [17:06<03:35, 16.61s/it]Running generate_until requests:  86%|████████▌ | 71/83 [17:11<02:38, 13.20s/it]Running generate_until requests:  87%|████████▋ | 72/83 [17:29<02:39, 14.50s/it]Running generate_until requests:  88%|████████▊ | 73/83 [17:46<02:34, 15.46s/it]Running generate_until requests:  89%|████████▉ | 74/83 [17:53<01:55, 12.88s/it]Running generate_until requests:  90%|█████████ | 75/83 [18:11<01:54, 14.27s/it]Running generate_until requests:  92%|█████████▏| 76/83 [18:28<01:46, 15.27s/it]Running generate_until requests:  93%|█████████▎| 77/83 [18:46<01:35, 15.98s/it]Running generate_until requests:  94%|█████████▍| 78/83 [19:04<01:22, 16.47s/it]Running generate_until requests:  95%|█████████▌| 79/83 [19:21<01:07, 16.81s/it]Running generate_until requests:  96%|█████████▋| 80/83 [19:30<00:43, 14.51s/it]Running generate_until requests:  98%|█████████▊| 81/83 [19:35<00:23, 11.62s/it]Running generate_until requests:  99%|█████████▉| 82/83 [19:42<00:10, 10.15s/it]Running generate_until requests: 100%|██████████| 83/83 [19:59<00:00, 12.38s/it]Running generate_until requests: 100%|██████████| 83/83 [19:59<00:00, 14.46s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:03:53:27,301 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:53:27,442 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:53:27,477 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:53:27,674 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:53:28,054 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:53:28,829 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:53:29,459 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:53:29,596 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:53:32,896 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:53:32,897 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:53:32,900 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:53:32,900 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
2024-06-04:03:53:33,113 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:53:33,114 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:53:33,119 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:53:33,119 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
2024-06-04:03:53:33,753 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:53:33,753 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:53:33,757 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:53:33,757 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
2024-06-04:03:53:34,039 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:53:34,040 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:53:34,044 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:53:34,044 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
2024-06-04:03:53:34,188 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:53:34,189 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:53:34,193 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:53:34,193 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.03s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.75s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.75s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.03s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.76s/it]2024-06-04:03:53:39,008 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:53:39,010 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:53:39,015 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:53:39,016 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.78s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.03s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.78s/it]2024-06-04:03:53:40,612 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:53:40,614 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:53:40,620 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:53:40,620 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.79s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]2024-06-04:03:53:43,165 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:53:43,167 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:53:43,173 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:53:43,173 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.25s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.22s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.21s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.19s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.16s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.07s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.68s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.96s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:05,  5.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.69s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:54:24,831 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:54:24,833 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:54:25,130 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 137.07it/s] 34%|███▎      | 28/83 [00:00<00:00, 133.13it/s] 51%|█████     | 42/83 [00:00<00:00, 130.62it/s] 67%|██████▋   | 56/83 [00:00<00:00, 132.99it/s] 84%|████████▍ | 70/83 [00:00<00:00, 134.38it/s]100%|██████████| 83/83 [00:00<00:00, 134.37it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:54:45,958 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:54:45,960 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:54:46,119 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:54:46,121 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:54:46,246 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:03:54:46,293 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 136.65it/s] 24%|██▍       | 20/83 [00:00<00:00, 197.31it/s] 34%|███▍      | 28/82 [00:00<00:00, 137.58it/s] 48%|████▊     | 40/83 [00:00<00:00, 197.88it/s] 51%|█████     | 42/82 [00:00<00:00, 137.78it/s] 72%|███████▏  | 60/83 [00:00<00:00, 198.00it/s] 68%|██████▊   | 56/82 [00:00<00:00, 137.98it/s] 96%|█████████▋| 80/83 [00:00<00:00, 198.11it/s]100%|██████████| 83/83 [00:00<00:00, 198.01it/s]
 87%|████████▋ | 71/82 [00:00<00:00, 139.12it/s]100%|██████████| 82/82 [00:00<00:00, 139.13it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:54:55,297 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:54:55,344 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:54:55,346 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:54:55,366 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:54:55,368 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:54:55,528 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:03:54:55,547 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 208.39it/s] 26%|██▌       | 21/82 [00:00<00:00, 200.52it/s] 51%|█████     | 42/83 [00:00<00:00, 206.72it/s] 51%|█████     | 42/82 [00:00<00:00, 200.78it/s] 76%|███████▌  | 63/83 [00:00<00:00, 207.84it/s] 77%|███████▋  | 63/82 [00:00<00:00, 202.08it/s]100%|██████████| 83/83 [00:00<00:00, 208.51it/s]
100%|██████████| 82/82 [00:00<00:00, 193.02it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:55:12,528 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:55:12,531 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:55:12,920 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 15%|█▍        | 12/82 [00:00<00:00, 119.51it/s] 30%|███       | 25/82 [00:00<00:00, 120.09it/s] 46%|████▋     | 38/82 [00:00<00:00, 120.15it/s] 62%|██████▏   | 51/82 [00:00<00:00, 120.39it/s] 78%|███████▊  | 64/82 [00:00<00:00, 120.32it/s] 94%|█████████▍| 77/82 [00:00<00:00, 120.69it/s]100%|██████████| 82/82 [00:00<00:00, 120.39it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:55:46,385 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:55:46,387 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:55:46,782 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 16%|█▌        | 13/82 [00:00<00:00, 120.97it/s] 32%|███▏      | 26/82 [00:00<00:00, 121.60it/s] 48%|████▊     | 39/82 [00:00<00:00, 121.64it/s] 63%|██████▎   | 52/82 [00:00<00:00, 121.75it/s] 79%|███████▉  | 65/82 [00:00<00:00, 121.27it/s] 95%|█████████▌| 78/82 [00:00<00:00, 121.38it/s]100%|██████████| 82/82 [00:00<00:00, 121.41it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:57:37,597 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:57:37,603 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:57:38,723 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 101.72it/s] 27%|██▋       | 22/83 [00:00<00:00, 104.04it/s] 40%|███▉      | 33/83 [00:00<00:00, 104.84it/s] 53%|█████▎    | 44/83 [00:00<00:00, 105.07it/s] 66%|██████▋   | 55/83 [00:00<00:00, 104.97it/s] 80%|███████▉  | 66/83 [00:00<00:00, 105.18it/s] 93%|█████████▎| 77/83 [00:00<00:00, 105.42it/s]100%|██████████| 83/83 [00:00<00:00, 104.98it/s]
2024-06-04:03:57:49,937 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:57:49,937 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:57:49,937 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:57:49,937 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:57:49,937 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:57:49,938 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:57:49,938 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:03:57:49,950 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:19<27:05, 19.83s/it]Running generate_until requests:   2%|▏         | 2/83 [00:31<20:26, 15.14s/it]Running generate_until requests:   4%|▎         | 3/83 [00:39<15:40, 11.76s/it]Running generate_until requests:   5%|▍         | 4/83 [00:44<11:55,  9.06s/it]Running generate_until requests:   6%|▌         | 5/83 [00:52<11:17,  8.68s/it]Running generate_until requests:   7%|▋         | 6/83 [00:59<10:38,  8.29s/it]Running generate_until requests:   8%|▊         | 7/83 [01:11<12:03,  9.52s/it]Running generate_until requests:  10%|▉         | 8/83 [01:19<10:58,  8.79s/it]Running generate_until requests:  11%|█         | 9/83 [01:27<10:37,  8.62s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:35<10:13,  8.41s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:44<10:13,  8.51s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:59<12:40, 10.71s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:05<10:38,  9.12s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:11<09:36,  8.35s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:18<08:59,  7.94s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:36<12:13, 10.94s/it]Running generate_until requests:  20%|██        | 17/83 [02:45<11:10, 10.16s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:52<10:11,  9.40s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:02<10:00,  9.38s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:12<10:14,  9.76s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:18<08:45,  8.48s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:23<07:43,  7.60s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:32<08:01,  8.03s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:41<08:10,  8.31s/it]Running generate_until requests:  30%|███       | 25/83 [03:48<07:43,  7.99s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:55<07:08,  7.52s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:03<07:16,  7.80s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:09<06:28,  7.07s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:16<06:29,  7.21s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:25<06:51,  7.76s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:31<06:04,  7.01s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:38<06:00,  7.07s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:55<08:32, 10.26s/it]Running generate_until requests:  41%|████      | 34/83 [05:06<08:20, 10.22s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:13<07:23,  9.25s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:19<06:35,  8.41s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:28<06:35,  8.60s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:46<08:28, 11.31s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:51<06:57,  9.50s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:56<05:56,  8.28s/it]Running generate_until requests:  49%|████▉     | 41/83 [06:02<05:11,  7.42s/it]Running generate_until requests:  51%|█████     | 42/83 [06:14<06:00,  8.79s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:22<05:45,  8.63s/it]Running generate_until requests:  53%|█████▎    | 44/83 [06:29<05:17,  8.15s/it]Running generate_until requests:  54%|█████▍    | 45/83 [06:34<04:38,  7.32s/it]Running generate_until requests:  55%|█████▌    | 46/83 [06:40<04:12,  6.82s/it]Running generate_until requests:  57%|█████▋    | 47/83 [06:46<03:52,  6.46s/it]Running generate_until requests:  58%|█████▊    | 48/83 [06:54<04:00,  6.88s/it]Running generate_until requests:  59%|█████▉    | 49/83 [06:58<03:24,  6.01s/it]Running generate_until requests:  60%|██████    | 50/83 [07:05<03:27,  6.28s/it]Running generate_until requests:  61%|██████▏   | 51/83 [07:16<04:13,  7.93s/it]Running generate_until requests:  63%|██████▎   | 52/83 [07:20<03:30,  6.78s/it]Running generate_until requests:  64%|██████▍   | 53/83 [07:27<03:21,  6.70s/it]Running generate_until requests:  65%|██████▌   | 54/83 [07:32<03:04,  6.36s/it]Running generate_until requests:  66%|██████▋   | 55/83 [07:37<02:44,  5.88s/it]Running generate_until requests:  67%|██████▋   | 56/83 [07:44<02:45,  6.11s/it]Running generate_until requests:  69%|██████▊   | 57/83 [08:01<04:07,  9.53s/it]Running generate_until requests:  70%|██████▉   | 58/83 [08:09<03:40,  8.82s/it]Running generate_until requests:  71%|███████   | 59/83 [08:14<03:05,  7.73s/it]Running generate_until requests:  72%|███████▏  | 60/83 [08:19<02:42,  7.08s/it]Running generate_until requests:  73%|███████▎  | 61/83 [08:26<02:32,  6.93s/it]Running generate_until requests:  75%|███████▍  | 62/83 [08:31<02:11,  6.25s/it]Running generate_until requests:  76%|███████▌  | 63/83 [08:48<03:11,  9.59s/it]Running generate_until requests:  77%|███████▋  | 64/83 [09:05<03:47, 11.97s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:11<03:01, 10.07s/it]Running generate_until requests:  80%|███████▉  | 66/83 [09:17<02:28,  8.73s/it]Running generate_until requests:  81%|████████  | 67/83 [09:20<01:54,  7.15s/it]Running generate_until requests:  82%|████████▏ | 68/83 [09:26<01:41,  6.77s/it]Running generate_until requests:  83%|████████▎ | 69/83 [09:38<01:54,  8.20s/it]Running generate_until requests:  84%|████████▍ | 70/83 [09:48<01:55,  8.89s/it]Running generate_until requests:  86%|████████▌ | 71/83 [09:53<01:31,  7.63s/it]Running generate_until requests:  87%|████████▋ | 72/83 [09:57<01:13,  6.65s/it]Running generate_until requests:  88%|████████▊ | 73/83 [10:03<01:03,  6.33s/it]Running generate_until requests:  89%|████████▉ | 74/83 [10:08<00:54,  6.06s/it]Running generate_until requests:  90%|█████████ | 75/83 [10:18<00:56,  7.10s/it]Running generate_until requests:  92%|█████████▏| 76/83 [10:24<00:47,  6.78s/it]Running generate_until requests:  93%|█████████▎| 77/83 [10:29<00:38,  6.34s/it]Running generate_until requests:  94%|█████████▍| 78/83 [10:36<00:32,  6.54s/it]Running generate_until requests:  95%|█████████▌| 79/83 [10:40<00:23,  5.91s/it]Running generate_until requests:  96%|█████████▋| 80/83 [10:45<00:16,  5.48s/it]Running generate_until requests:  98%|█████████▊| 81/83 [10:51<00:11,  5.56s/it]Running generate_until requests:  99%|█████████▉| 82/83 [10:56<00:05,  5.36s/it]Running generate_until requests: 100%|██████████| 83/83 [11:00<00:00,  5.04s/it]Running generate_until requests: 100%|██████████| 83/83 [11:00<00:00,  7.96s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:04:20:13,404 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:20:13,415 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:20:13,469 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:20:13,587 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:20:13,656 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:20:13,800 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:20:14,028 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:20:14,132 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:20:20,326 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:20:20,327 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:20:20,332 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:20:20,332 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
2024-06-04:04:20:20,342 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:20:20,343 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:20:20,348 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:20:20,348 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
2024-06-04:04:20:20,457 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:20:20,458 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:20:20,462 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:20:20,462 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
2024-06-04:04:20:20,623 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:20:20,624 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:20:20,628 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:20:20,628 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
2024-06-04:04:20:20,674 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:20:20,676 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:20:20,681 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:20:20,681 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
2024-06-04:04:20:20,745 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:20:20,746 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:20:20,750 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:20:20,750 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:04:20:23,641 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:20:23,643 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:20:23,649 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:20:23,649 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.04s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.74s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.77s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.70s/it]2024-06-04:04:20:25,321 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.80s/it]2024-06-04:04:20:25,322 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:20:25,327 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:20:25,327 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.84s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.07s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.27s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.99s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.33s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.26s/it]

Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.24s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.26s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.97s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:21:49,794 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:21:49,795 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:21:49,801 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:21:49,803 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:21:50,106 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:04:21:50,172 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 18%|█▊        | 15/82 [00:00<00:00, 144.60it/s] 20%|█▉        | 16/82 [00:00<00:00, 157.39it/s] 37%|███▋      | 30/82 [00:00<00:00, 143.50it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:04:21:50,425 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
 39%|███▉      | 32/82 [00:00<00:00, 137.86it/s] 55%|█████▍    | 45/82 [00:00<00:00, 141.55it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:21:50,500 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:21:50,502 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 56%|█████▌    | 46/82 [00:00<00:00, 133.15it/s] 76%|███████▌  | 62/82 [00:00<00:00, 151.64it/s] 73%|███████▎  | 60/82 [00:00<00:00, 134.98it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 95%|█████████▌| 78/82 [00:00<00:00, 145.39it/s]100%|██████████| 82/82 [00:00<00:00, 144.54it/s]
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:21:50,725 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:21:50,727 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 90%|█████████ | 74/82 [00:00<00:00, 135.51it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:04:21:50,768 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]100%|██████████| 82/82 [00:00<00:00, 136.56it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:21:50,847 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:21:50,849 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:21:50,884 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:21:50,886 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 17%|█▋        | 14/83 [00:00<00:00, 130.12it/s] 34%|███▎      | 28/83 [00:00<00:00, 129.34it/s]2024-06-04:04:21:51,011 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:04:21:51,030 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:04:21:51,096 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 51%|█████     | 42/83 [00:00<00:00, 130.65it/s] 24%|██▍       | 20/83 [00:00<00:00, 198.31it/s] 17%|█▋        | 14/83 [00:00<00:00, 134.17it/s] 24%|██▍       | 20/82 [00:00<00:00, 194.82it/s] 67%|██████▋   | 56/83 [00:00<00:00, 132.18it/s] 48%|████▊     | 40/83 [00:00<00:00, 199.06it/s] 34%|███▎      | 28/83 [00:00<00:00, 134.54it/s] 49%|████▉     | 40/82 [00:00<00:00, 195.76it/s] 84%|████████▍ | 70/83 [00:00<00:00, 133.02it/s] 72%|███████▏  | 60/83 [00:00<00:00, 199.33it/s] 51%|█████     | 42/83 [00:00<00:00, 134.22it/s] 73%|███████▎  | 60/82 [00:00<00:00, 196.12it/s]100%|██████████| 83/83 [00:00<00:00, 132.04it/s]
 98%|█████████▊| 81/83 [00:00<00:00, 199.01it/s]100%|██████████| 83/83 [00:00<00:00, 195.96it/s]
 67%|██████▋   | 56/83 [00:00<00:00, 133.05it/s] 84%|████████▍ | 70/83 [00:00<00:00, 133.50it/s] 98%|█████████▊| 80/82 [00:00<00:00, 159.11it/s]100%|██████████| 82/82 [00:00<00:00, 168.10it/s]
100%|██████████| 83/83 [00:00<00:00, 131.96it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:22:21,501 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:22:21,505 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:22:21,981 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 11%|█         | 9/83 [00:00<00:00, 87.58it/s] 22%|██▏       | 18/83 [00:00<00:00, 71.40it/s] 31%|███▏      | 26/83 [00:00<00:00, 72.87it/s] 41%|████      | 34/83 [00:00<00:00, 69.52it/s] 51%|█████     | 42/83 [00:00<00:00, 72.25it/s] 60%|██████    | 50/83 [00:00<00:00, 64.79it/s] 69%|██████▊   | 57/83 [00:00<00:00, 62.68it/s] 78%|███████▊  | 65/83 [00:00<00:00, 66.38it/s] 92%|█████████▏| 76/83 [00:01<00:00, 74.41it/s]100%|██████████| 83/83 [00:01<00:00, 70.65it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:22:37,053 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:22:37,056 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:22:37,517 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s]  6%|▌         | 5/82 [00:00<00:01, 46.16it/s] 15%|█▍        | 12/82 [00:00<00:01, 56.64it/s] 22%|██▏       | 18/82 [00:00<00:01, 54.02it/s] 30%|███       | 25/82 [00:00<00:00, 58.44it/s] 38%|███▊      | 31/82 [00:00<00:00, 54.12it/s] 45%|████▌     | 37/82 [00:00<00:00, 55.45it/s] 52%|█████▏    | 43/82 [00:00<00:00, 55.32it/s] 60%|█████▉    | 49/82 [00:00<00:00, 56.33it/s] 67%|██████▋   | 55/82 [00:00<00:00, 55.64it/s] 74%|███████▍  | 61/82 [00:01<00:00, 53.78it/s] 82%|████████▏ | 67/82 [00:01<00:00, 54.92it/s] 90%|█████████ | 74/82 [00:01<00:00, 55.76it/s] 98%|█████████▊| 80/82 [00:01<00:00, 54.44it/s]100%|██████████| 82/82 [00:01<00:00, 55.10it/s]
2024-06-04:04:22:49,665 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:22:49,665 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:22:49,665 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:22:49,665 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:22:49,665 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:04:22:49,666 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:22:49,666 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:22:49,681 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:11<15:24, 11.28s/it]Running generate_until requests:   2%|▏         | 2/83 [00:32<23:14, 17.22s/it]Running generate_until requests:   4%|▎         | 3/83 [00:40<17:14, 12.94s/it]Running generate_until requests:   5%|▍         | 4/83 [00:47<13:43, 10.42s/it]Running generate_until requests:   6%|▌         | 5/83 [00:55<12:43,  9.79s/it]Running generate_until requests:   7%|▋         | 6/83 [01:02<11:18,  8.81s/it]Running generate_until requests:   8%|▊         | 7/83 [01:11<11:22,  8.98s/it]Running generate_until requests:  10%|▉         | 8/83 [01:19<10:47,  8.63s/it]Running generate_until requests:  11%|█         | 9/83 [01:30<11:20,  9.19s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:37<10:16,  8.44s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:44<09:42,  8.09s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:55<10:49,  9.14s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:02<09:39,  8.29s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:10<09:35,  8.33s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:25<11:34, 10.21s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:41<13:28, 12.06s/it]Running generate_until requests:  20%|██        | 17/83 [02:51<12:31, 11.39s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:59<11:23, 10.51s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:09<10:49, 10.15s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:22<11:31, 10.97s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:29<10:17,  9.96s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:37<09:20,  9.19s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:45<08:56,  8.94s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:52<08:22,  8.52s/it]Running generate_until requests:  30%|███       | 25/83 [04:00<07:51,  8.13s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:07<07:28,  7.88s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:15<07:22,  7.90s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:22<07:07,  7.77s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:31<07:11,  7.99s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:42<07:58,  9.03s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:50<07:35,  8.75s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:59<07:24,  8.72s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:04<06:22,  7.64s/it]Running generate_until requests:  41%|████      | 34/83 [05:16<07:17,  8.94s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:24<06:50,  8.55s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:31<06:20,  8.10s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:39<06:14,  8.14s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:50<06:50,  9.11s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:56<05:59,  8.16s/it]Running generate_until requests:  48%|████▊     | 40/83 [06:03<05:30,  7.69s/it]Running generate_until requests:  49%|████▉     | 41/83 [06:10<05:16,  7.53s/it]Running generate_until requests:  51%|█████     | 42/83 [06:19<05:22,  7.86s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:29<05:43,  8.60s/it]Running generate_until requests:  53%|█████▎    | 44/83 [06:36<05:18,  8.17s/it]Running generate_until requests:  54%|█████▍    | 45/83 [06:44<05:00,  7.89s/it]Running generate_until requests:  55%|█████▌    | 46/83 [06:50<04:41,  7.61s/it]Running generate_until requests:  57%|█████▋    | 47/83 [06:57<04:20,  7.24s/it]Running generate_until requests:  58%|█████▊    | 48/83 [07:07<04:38,  7.96s/it]Running generate_until requests:  59%|█████▉    | 49/83 [07:12<04:03,  7.17s/it]Running generate_until requests:  60%|██████    | 50/83 [07:20<04:04,  7.41s/it]Running generate_until requests:  61%|██████▏   | 51/83 [07:28<04:06,  7.71s/it]Running generate_until requests:  63%|██████▎   | 52/83 [07:35<03:49,  7.40s/it]Running generate_until requests:  64%|██████▍   | 53/83 [07:42<03:41,  7.39s/it]Running generate_until requests:  65%|██████▌   | 54/83 [07:49<03:28,  7.19s/it]Running generate_until requests:  66%|██████▋   | 55/83 [07:55<03:12,  6.87s/it]Running generate_until requests:  67%|██████▋   | 56/83 [08:09<04:01,  8.94s/it]Running generate_until requests:  69%|██████▊   | 57/83 [08:21<04:17,  9.91s/it]Running generate_until requests:  70%|██████▉   | 58/83 [08:29<03:53,  9.35s/it]Running generate_until requests:  71%|███████   | 59/83 [08:36<03:26,  8.59s/it]Running generate_until requests:  72%|███████▏  | 60/83 [08:42<02:58,  7.77s/it]Running generate_until requests:  73%|███████▎  | 61/83 [08:51<02:58,  8.12s/it]Running generate_until requests:  75%|███████▍  | 62/83 [08:56<02:30,  7.16s/it]Running generate_until requests:  76%|███████▌  | 63/83 [09:02<02:16,  6.84s/it]Running generate_until requests:  77%|███████▋  | 64/83 [09:09<02:12,  6.99s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:15<02:02,  6.82s/it]Running generate_until requests:  80%|███████▉  | 66/83 [09:23<01:57,  6.89s/it]Running generate_until requests:  81%|████████  | 67/83 [09:26<01:35,  5.97s/it]Running generate_until requests:  82%|████████▏ | 68/83 [09:30<01:20,  5.40s/it]Running generate_until requests:  83%|████████▎ | 69/83 [09:39<01:30,  6.49s/it]Running generate_until requests:  84%|████████▍ | 70/83 [09:53<01:50,  8.53s/it]Running generate_until requests:  86%|████████▌ | 71/83 [09:58<01:31,  7.60s/it]Running generate_until requests:  87%|████████▋ | 72/83 [10:05<01:19,  7.23s/it]Running generate_until requests:  88%|████████▊ | 73/83 [10:11<01:11,  7.11s/it]Running generate_until requests:  89%|████████▉ | 74/83 [10:18<01:01,  6.84s/it]Running generate_until requests:  90%|█████████ | 75/83 [10:28<01:04,  8.04s/it]Running generate_until requests:  92%|█████████▏| 76/83 [10:36<00:54,  7.83s/it]Running generate_until requests:  93%|█████████▎| 77/83 [10:42<00:44,  7.49s/it]Running generate_until requests:  94%|█████████▍| 78/83 [10:50<00:37,  7.55s/it]Running generate_until requests:  95%|█████████▌| 79/83 [10:58<00:30,  7.69s/it]Running generate_until requests:  96%|█████████▋| 80/83 [11:03<00:20,  6.96s/it]Running generate_until requests:  98%|█████████▊| 81/83 [11:10<00:13,  6.88s/it]Running generate_until requests:  99%|█████████▉| 82/83 [11:16<00:06,  6.56s/it]Running generate_until requests: 100%|██████████| 83/83 [11:21<00:00,  5.99s/it]Running generate_until requests: 100%|██████████| 83/83 [11:21<00:00,  8.21s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:04:37:22,899 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:37:22,899 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:37:22,899 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:37:23,087 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:37:23,116 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:37:23,199 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:37:23,671 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:37:24,735 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:37:29,709 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:37:29,710 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:37:29,715 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:37:29,715 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
2024-06-04:04:37:29,726 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:37:29,727 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:37:29,732 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:37:29,732 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
2024-06-04:04:37:29,824 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:37:29,825 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:37:29,830 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:37:29,830 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
2024-06-04:04:37:30,013 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:37:30,014 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:37:30,019 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:37:30,019 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
2024-06-04:04:37:30,034 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:37:30,035 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:37:30,040 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:37:30,040 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
2024-06-04:04:37:30,081 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:37:30,083 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:37:30,088 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:37:30,088 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.98s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.70s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.75s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.78s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.77s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.81s/it]2024-06-04:04:37:34,844 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:37:34,845 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:37:34,849 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:37:34,849 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
2024-06-04:04:37:34,864 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:37:34,867 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:37:34,877 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:37:34,877 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.90s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.06s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.50s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.78s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.78s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.19s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.31s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.88s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.24s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.22s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.30s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:01,  1.99s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.65s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:08,  4.24s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.81s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:38:24,110 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:38:24,113 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:38:24,511 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 130.49it/s] 34%|███▍      | 28/82 [00:00<00:00, 131.68it/s] 51%|█████     | 42/82 [00:00<00:00, 132.10it/s] 68%|██████▊   | 56/82 [00:00<00:00, 132.26it/s] 85%|████████▌ | 70/82 [00:00<00:00, 132.14it/s]100%|██████████| 82/82 [00:00<00:00, 132.14it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:38:43,813 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:38:43,815 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:38:43,934 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:38:43,936 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:38:43,982 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/83 [00:00<00:00, 194.53it/s] 48%|████▊     | 40/83 [00:00<00:00, 194.73it/s]2024-06-04:04:38:44,298 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
 72%|███████▏  | 60/83 [00:00<00:00, 194.80it/s]  0%|          | 0/83 [00:00<?, ?it/s] 96%|█████████▋| 80/83 [00:00<00:00, 195.37it/s]100%|██████████| 83/83 [00:00<00:00, 195.08it/s]
 17%|█▋        | 14/83 [00:00<00:00, 132.14it/s] 34%|███▎      | 28/83 [00:00<00:00, 135.43it/s] 51%|█████     | 42/83 [00:00<00:00, 129.44it/s] 69%|██████▊   | 57/83 [00:00<00:00, 135.02it/s] 93%|█████████▎| 77/83 [00:00<00:00, 156.63it/s]100%|██████████| 83/83 [00:00<00:00, 148.84it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:04:38:52,230 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:38:52,302 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:38:52,304 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:38:52,464 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/83 [00:00<00:00, 198.37it/s] 49%|████▉     | 41/83 [00:00<00:00, 202.98it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 75%|███████▍  | 62/83 [00:00<00:00, 204.32it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:38:52,831 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:38:52,833 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
100%|██████████| 83/83 [00:00<00:00, 204.99it/s]100%|██████████| 83/83 [00:00<00:00, 204.06it/s]
2024-06-04:04:38:53,113 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 16%|█▌        | 13/82 [00:00<00:00, 121.03it/s] 32%|███▏      | 26/82 [00:00<00:00, 109.51it/s] 46%|████▋     | 38/82 [00:00<00:00, 106.34it/s] 60%|█████▉    | 49/82 [00:00<00:00, 104.97it/s] 73%|███████▎  | 60/82 [00:00<00:00, 104.12it/s] 87%|████████▋ | 71/82 [00:00<00:00, 103.51it/s]100%|██████████| 82/82 [00:00<00:00, 103.50it/s]100%|██████████| 82/82 [00:00<00:00, 105.18it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:39:01,726 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:39:01,731 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:39:02,227 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 16%|█▌        | 13/83 [00:00<00:00, 125.45it/s] 31%|███▏      | 26/83 [00:00<00:00, 121.67it/s] 47%|████▋     | 39/83 [00:00<00:00, 122.00it/s] 63%|██████▎   | 52/83 [00:00<00:00, 119.65it/s] 78%|███████▊  | 65/83 [00:00<00:00, 119.84it/s] 94%|█████████▍| 78/83 [00:00<00:00, 120.46it/s]100%|██████████| 83/83 [00:00<00:00, 120.54it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:39:25,621 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:39:25,623 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:39:26,298 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 100.82it/s] 27%|██▋       | 22/82 [00:00<00:00, 93.03it/s]  39%|███▉      | 32/82 [00:00<00:00, 72.94it/s] 49%|████▉     | 40/82 [00:00<00:00, 68.41it/s] 59%|█████▊    | 48/82 [00:00<00:00, 67.12it/s] 70%|██████▉   | 57/82 [00:00<00:00, 71.52it/s] 79%|███████▉  | 65/82 [00:00<00:00, 72.60it/s] 89%|████████▉ | 73/82 [00:00<00:00, 71.57it/s]100%|██████████| 82/82 [00:01<00:00, 75.04it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:40:37,967 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:40:37,969 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:40:38,478 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 105.76it/s] 27%|██▋       | 22/82 [00:00<00:00, 106.60it/s] 40%|████      | 33/82 [00:00<00:00, 107.03it/s] 54%|█████▎    | 44/82 [00:00<00:00, 107.24it/s] 67%|██████▋   | 55/82 [00:00<00:00, 107.08it/s] 80%|████████  | 66/82 [00:00<00:00, 107.09it/s] 94%|█████████▍| 77/82 [00:00<00:00, 107.10it/s]100%|██████████| 82/82 [00:00<00:00, 107.01it/s]
2024-06-04:04:40:50,254 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:40:50,254 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:40:50,254 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:40:50,254 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:40:50,254 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:40:50,254 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:40:50,254 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:04:40:50,256 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:14<19:14, 14.08s/it]Running generate_until requests:   2%|▏         | 2/83 [00:28<19:27, 14.41s/it]Running generate_until requests:   4%|▎         | 3/83 [00:36<15:04, 11.31s/it]Running generate_until requests:   5%|▍         | 4/83 [00:42<12:00,  9.12s/it]Running generate_until requests:   6%|▌         | 5/83 [00:50<11:19,  8.72s/it]Running generate_until requests:   7%|▋         | 6/83 [00:55<09:39,  7.53s/it]Running generate_until requests:   8%|▊         | 7/83 [01:03<09:44,  7.69s/it]Running generate_until requests:  10%|▉         | 8/83 [01:10<09:22,  7.49s/it]Running generate_until requests:  11%|█         | 9/83 [01:19<09:51,  8.00s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:25<08:47,  7.23s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:33<09:05,  7.58s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:43<09:44,  8.24s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:46<07:56,  6.81s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:54<08:00,  6.96s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:03<08:38,  7.63s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:14<09:55,  8.88s/it]Running generate_until requests:  20%|██        | 17/83 [02:21<09:07,  8.29s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:28<08:35,  7.93s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:36<08:20,  7.82s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:47<09:08,  8.71s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:52<08:02,  7.79s/it]Running generate_until requests:  27%|██▋       | 22/83 [02:59<07:30,  7.39s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:06<07:14,  7.24s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:12<06:52,  6.99s/it]Running generate_until requests:  30%|███       | 25/83 [03:19<06:47,  7.03s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:26<06:30,  6.86s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:36<07:24,  7.94s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:43<06:54,  7.54s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:49<06:31,  7.26s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:00<07:10,  8.12s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:04<06:06,  7.05s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:11<05:52,  6.92s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:16<05:14,  6.29s/it]Running generate_until requests:  41%|████      | 34/83 [04:26<06:08,  7.52s/it]Running generate_until requests:  42%|████▏     | 35/83 [04:32<05:40,  7.09s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:38<05:22,  6.86s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:46<05:22,  7.01s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:58<06:23,  8.53s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:03<05:32,  7.56s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:09<05:04,  7.07s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:14<04:33,  6.52s/it]Running generate_until requests:  51%|█████     | 42/83 [05:20<04:20,  6.34s/it]Running generate_until requests:  52%|█████▏    | 43/83 [05:29<04:38,  6.97s/it]Running generate_until requests:  53%|█████▎    | 44/83 [05:35<04:23,  6.76s/it]Running generate_until requests:  54%|█████▍    | 45/83 [05:41<04:12,  6.64s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:48<04:03,  6.59s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:53<03:40,  6.14s/it]Running generate_until requests:  58%|█████▊    | 48/83 [06:00<03:46,  6.47s/it]Running generate_until requests:  59%|█████▉    | 49/83 [06:05<03:20,  5.91s/it]Running generate_until requests:  60%|██████    | 50/83 [06:13<03:33,  6.48s/it]Running generate_until requests:  61%|██████▏   | 51/83 [06:19<03:29,  6.54s/it]Running generate_until requests:  63%|██████▎   | 52/83 [06:25<03:19,  6.42s/it]Running generate_until requests:  64%|██████▍   | 53/83 [06:32<03:12,  6.43s/it]Running generate_until requests:  65%|██████▌   | 54/83 [06:37<02:53,  6.00s/it]Running generate_until requests:  66%|██████▋   | 55/83 [06:43<02:47,  5.98s/it]Running generate_until requests:  67%|██████▋   | 56/83 [06:56<03:43,  8.28s/it]Running generate_until requests:  69%|██████▊   | 57/83 [07:10<04:18,  9.93s/it]Running generate_until requests:  70%|██████▉   | 58/83 [07:17<03:46,  9.04s/it]Running generate_until requests:  71%|███████   | 59/83 [07:23<03:14,  8.09s/it]Running generate_until requests:  72%|███████▏  | 60/83 [07:28<02:44,  7.14s/it]Running generate_until requests:  73%|███████▎  | 61/83 [07:36<02:40,  7.30s/it]Running generate_until requests:  75%|███████▍  | 62/83 [07:41<02:22,  6.77s/it]Running generate_until requests:  76%|███████▌  | 63/83 [07:46<02:06,  6.31s/it]Running generate_until requests:  77%|███████▋  | 64/83 [07:54<02:07,  6.73s/it]Running generate_until requests:  78%|███████▊  | 65/83 [08:00<01:55,  6.39s/it]Running generate_until requests:  80%|███████▉  | 66/83 [08:08<01:59,  7.03s/it]Running generate_until requests:  81%|████████  | 67/83 [08:12<01:37,  6.10s/it]Running generate_until requests:  82%|████████▏ | 68/83 [08:18<01:29,  5.94s/it]Running generate_until requests:  83%|████████▎ | 69/83 [08:27<01:38,  7.04s/it]Running generate_until requests:  84%|████████▍ | 70/83 [08:35<01:36,  7.39s/it]Running generate_until requests:  86%|████████▌ | 71/83 [08:40<01:19,  6.62s/it]Running generate_until requests:  87%|████████▋ | 72/83 [08:46<01:09,  6.30s/it]Running generate_until requests:  88%|████████▊ | 73/83 [08:53<01:04,  6.50s/it]Running generate_until requests:  89%|████████▉ | 74/83 [08:59<00:56,  6.30s/it]Running generate_until requests:  90%|█████████ | 75/83 [09:07<00:55,  6.88s/it]Running generate_until requests:  92%|█████████▏| 76/83 [09:14<00:48,  6.94s/it]Running generate_until requests:  93%|█████████▎| 77/83 [09:19<00:37,  6.32s/it]Running generate_until requests:  94%|█████████▍| 78/83 [09:25<00:32,  6.40s/it]Running generate_until requests:  95%|█████████▌| 79/83 [09:34<00:28,  7.07s/it]Running generate_until requests:  96%|█████████▋| 80/83 [09:41<00:21,  7.10s/it]Running generate_until requests:  98%|█████████▊| 81/83 [09:47<00:13,  6.79s/it]Running generate_until requests:  99%|█████████▉| 82/83 [09:52<00:06,  6.14s/it]Running generate_until requests: 100%|██████████| 83/83 [09:56<00:00,  5.57s/it]Running generate_until requests: 100%|██████████| 83/83 [09:56<00:00,  7.19s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:05:03:28,417 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:03:28,418 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:03:28,419 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:03:28,476 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:03:28,666 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:03:28,888 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:03:30,384 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:03:31,801 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:03:33,170 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:03:33,171 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:03:33,176 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:03:33,176 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
2024-06-04:05:03:33,238 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:03:33,239 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:03:33,243 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:03:33,243 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
2024-06-04:05:03:34,112 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:03:34,112 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:03:34,117 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:03:34,117 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:05:03:34,925 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:03:34,927 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:03:34,933 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:03:34,933 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.03s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.37s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.95s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.62s/it]2024-06-04:05:03:40,222 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:03:40,224 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:03:40,229 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:03:40,229 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
2024-06-04:05:03:40,381 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:03:40,382 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:03:40,390 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:03:40,390 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:01,  1.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.67s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.63s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.56s/it]2024-06-04:05:03:42,359 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:03:42,362 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:03:42,371 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:03:42,371 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:05:03:42,784 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:03:42,786 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:03:42,792 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:03:42,792 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.05s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.38s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.99s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.18s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.27s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.16s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.84s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.37s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.32s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:10,  5.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.11s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:03,  3.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.25s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:04:30,665 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:04:30,667 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:04:31,062 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 131.93it/s] 34%|███▍      | 28/82 [00:00<00:00, 132.26it/s] 51%|█████     | 42/82 [00:00<00:00, 132.38it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 68%|██████▊   | 56/82 [00:00<00:00, 133.23it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:04:31,518 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:04:31,520 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 85%|████████▌ | 70/82 [00:00<00:00, 131.85it/s]100%|██████████| 82/82 [00:00<00:00, 131.54it/s]
2024-06-04:05:04:31,771 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 196.53it/s] 49%|████▉     | 40/82 [00:00<00:00, 194.99it/s] 73%|███████▎  | 60/82 [00:00<00:00, 194.95it/s] 98%|█████████▊| 80/82 [00:00<00:00, 195.80it/s]100%|██████████| 82/82 [00:00<00:00, 195.64it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:04:47,615 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:04:47,616 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:04:47,732 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:04:47,734 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:04:47,784 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 195.73it/s]2024-06-04:05:04:47,938 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 49%|████▉     | 40/82 [00:00<00:00, 196.50it/s] 24%|██▍       | 20/83 [00:00<00:00, 198.94it/s] 73%|███████▎  | 60/82 [00:00<00:00, 196.67it/s] 49%|████▉     | 41/83 [00:00<00:00, 199.71it/s] 98%|█████████▊| 80/82 [00:00<00:00, 196.95it/s]100%|██████████| 82/82 [00:00<00:00, 196.74it/s]
 75%|███████▍  | 62/83 [00:00<00:00, 201.97it/s]100%|██████████| 83/83 [00:00<00:00, 204.96it/s]100%|██████████| 83/83 [00:00<00:00, 203.27it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:05:00,429 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:05:00,431 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:05:00,602 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 207.29it/s] 51%|█████     | 42/83 [00:00<00:00, 208.70it/s] 76%|███████▌  | 63/83 [00:00<00:00, 209.24it/s]100%|██████████| 83/83 [00:00<00:00, 209.35it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:05:22,146 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:05:22,148 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:05:22,540 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 14%|█▍        | 12/83 [00:00<00:00, 119.66it/s] 30%|███       | 25/83 [00:00<00:00, 120.43it/s] 46%|████▌     | 38/83 [00:00<00:00, 118.78it/s] 60%|██████    | 50/83 [00:00<00:00, 116.18it/s] 76%|███████▌  | 63/83 [00:00<00:00, 117.84it/s] 92%|█████████▏| 76/83 [00:00<00:00, 118.96it/s]100%|██████████| 83/83 [00:00<00:00, 118.82it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:05:05:53,183 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:05:53,287 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:05:53,290 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:05:53,666 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 16%|█▌        | 13/83 [00:00<00:00, 127.82it/s] 31%|███▏      | 26/83 [00:00<00:00, 128.27it/s] 47%|████▋     | 39/83 [00:00<00:00, 128.26it/s] 63%|██████▎   | 52/83 [00:00<00:00, 128.35it/s] 78%|███████▊  | 65/83 [00:00<00:00, 128.19it/s] 94%|█████████▍| 78/83 [00:00<00:00, 128.35it/s]100%|██████████| 83/83 [00:00<00:00, 128.27it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:06:12,945 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:06:12,949 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:06:14,000 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s]  6%|▌         | 5/82 [00:00<00:01, 40.87it/s] 12%|█▏        | 10/82 [00:00<00:01, 41.55it/s] 18%|█▊        | 15/82 [00:00<00:01, 41.25it/s] 24%|██▍       | 20/82 [00:00<00:01, 41.92it/s] 30%|███       | 25/82 [00:00<00:01, 41.78it/s] 37%|███▋      | 30/82 [00:00<00:01, 41.97it/s] 43%|████▎     | 35/82 [00:00<00:01, 41.39it/s] 49%|████▉     | 40/82 [00:00<00:01, 41.70it/s] 55%|█████▍    | 45/82 [00:01<00:00, 41.49it/s] 61%|██████    | 50/82 [00:01<00:00, 41.33it/s] 67%|██████▋   | 55/82 [00:01<00:00, 41.31it/s] 73%|███████▎  | 60/82 [00:01<00:00, 41.23it/s] 79%|███████▉  | 65/82 [00:01<00:00, 41.41it/s] 85%|████████▌ | 70/82 [00:01<00:00, 41.50it/s] 91%|█████████▏| 75/82 [00:01<00:00, 41.42it/s] 98%|█████████▊| 80/82 [00:01<00:00, 41.34it/s]100%|██████████| 82/82 [00:01<00:00, 41.46it/s]
2024-06-04:05:06:26,560 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:05:06:26,560 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:05:06:26,561 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:05:06:26,561 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:05:06:26,561 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:05:06:26,561 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:05:06:26,561 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:05:06:26,564 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:10<14:57, 10.95s/it]Running generate_until requests:   2%|▏         | 2/83 [00:23<16:18, 12.08s/it]Running generate_until requests:   4%|▎         | 3/83 [00:31<13:25, 10.07s/it]Running generate_until requests:   5%|▍         | 4/83 [00:36<10:36,  8.06s/it]Running generate_until requests:   6%|▌         | 5/83 [00:44<10:29,  8.07s/it]Running generate_until requests:   7%|▋         | 6/83 [00:50<09:25,  7.35s/it]Running generate_until requests:   8%|▊         | 7/83 [00:59<09:49,  7.75s/it]Running generate_until requests:  10%|▉         | 8/83 [01:06<09:23,  7.51s/it]Running generate_until requests:  11%|█         | 9/83 [01:15<09:53,  8.03s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:20<08:48,  7.24s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:27<08:31,  7.10s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:37<09:28,  8.01s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:41<08:01,  6.88s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:48<07:51,  6.83s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:04<10:42,  9.46s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:15<11:20, 10.15s/it]Running generate_until requests:  20%|██        | 17/83 [02:24<10:33,  9.59s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:31<09:39,  8.91s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:39<09:14,  8.66s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:50<09:53,  9.42s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:56<08:34,  8.30s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:03<07:56,  7.81s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:10<07:41,  7.70s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:16<07:10,  7.29s/it]Running generate_until requests:  30%|███       | 25/83 [03:22<06:38,  6.87s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:29<06:29,  6.83s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:36<06:29,  6.96s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:43<06:17,  6.87s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:49<05:57,  6.63s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:59<06:42,  7.60s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:06<06:31,  7.52s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:13<06:07,  7.20s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:17<05:22,  6.44s/it]Running generate_until requests:  41%|████      | 34/83 [04:28<06:19,  7.74s/it]Running generate_until requests:  42%|████▏     | 35/83 [04:34<05:47,  7.25s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:41<05:35,  7.13s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:48<05:27,  7.11s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:00<06:23,  8.52s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:05<05:32,  7.56s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:11<05:02,  7.04s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:16<04:28,  6.39s/it]Running generate_until requests:  51%|█████     | 42/83 [05:21<04:09,  6.09s/it]Running generate_until requests:  52%|█████▏    | 43/83 [05:29<04:24,  6.62s/it]Running generate_until requests:  53%|█████▎    | 44/83 [05:34<04:01,  6.19s/it]Running generate_until requests:  54%|█████▍    | 45/83 [05:41<03:57,  6.26s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:48<03:57,  6.43s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:54<03:47,  6.31s/it]Running generate_until requests:  58%|█████▊    | 48/83 [06:01<03:53,  6.68s/it]Running generate_until requests:  59%|█████▉    | 49/83 [06:05<03:22,  5.94s/it]Running generate_until requests:  60%|██████    | 50/83 [06:14<03:43,  6.77s/it]Running generate_until requests:  61%|██████▏   | 51/83 [06:21<03:38,  6.82s/it]Running generate_until requests:  63%|██████▎   | 52/83 [06:27<03:22,  6.53s/it]Running generate_until requests:  64%|██████▍   | 53/83 [06:35<03:30,  7.01s/it]Running generate_until requests:  65%|██████▌   | 54/83 [06:40<03:04,  6.35s/it]Running generate_until requests:  66%|██████▋   | 55/83 [06:45<02:47,  5.98s/it]Running generate_until requests:  67%|██████▋   | 56/83 [07:00<03:55,  8.71s/it]Running generate_until requests:  69%|██████▊   | 57/83 [07:09<03:50,  8.86s/it]Running generate_until requests:  70%|██████▉   | 58/83 [07:16<03:25,  8.21s/it]Running generate_until requests:  71%|███████   | 59/83 [07:21<02:56,  7.37s/it]Running generate_until requests:  72%|███████▏  | 60/83 [07:26<02:33,  6.68s/it]Running generate_until requests:  73%|███████▎  | 61/83 [07:34<02:34,  7.02s/it]Running generate_until requests:  75%|███████▍  | 62/83 [07:40<02:19,  6.64s/it]Running generate_until requests:  76%|███████▌  | 63/83 [07:46<02:06,  6.33s/it]Running generate_until requests:  77%|███████▋  | 64/83 [07:51<01:54,  6.04s/it]Running generate_until requests:  78%|███████▊  | 65/83 [07:55<01:39,  5.53s/it]Running generate_until requests:  80%|███████▉  | 66/83 [08:00<01:31,  5.38s/it]Running generate_until requests:  81%|████████  | 67/83 [08:04<01:17,  4.82s/it]Running generate_until requests:  82%|████████▏ | 68/83 [08:08<01:09,  4.62s/it]Running generate_until requests:  83%|████████▎ | 69/83 [08:14<01:11,  5.13s/it]Running generate_until requests:  84%|████████▍ | 70/83 [08:20<01:10,  5.41s/it]Running generate_until requests:  86%|████████▌ | 71/83 [08:24<00:57,  4.77s/it]Running generate_until requests:  87%|████████▋ | 72/83 [08:29<00:52,  4.80s/it]Running generate_until requests:  88%|████████▊ | 73/83 [08:33<00:46,  4.67s/it]Running generate_until requests:  89%|████████▉ | 74/83 [08:37<00:41,  4.63s/it]Running generate_until requests:  90%|█████████ | 75/83 [08:44<00:42,  5.31s/it]Running generate_until requests:  92%|█████████▏| 76/83 [08:50<00:38,  5.55s/it]Running generate_until requests:  93%|█████████▎| 77/83 [08:54<00:30,  5.04s/it]Running generate_until requests:  94%|█████████▍| 78/83 [08:59<00:23,  4.78s/it]Running generate_until requests:  95%|█████████▌| 79/83 [09:05<00:21,  5.31s/it]Running generate_until requests:  96%|█████████▋| 80/83 [09:11<00:16,  5.36s/it]Running generate_until requests:  98%|█████████▊| 81/83 [09:16<00:10,  5.41s/it]Running generate_until requests:  99%|█████████▉| 82/83 [09:20<00:04,  4.89s/it]Running generate_until requests: 100%|██████████| 83/83 [09:23<00:00,  4.42s/it]Running generate_until requests: 100%|██████████| 83/83 [09:23<00:00,  6.79s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:05:28:39,213 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:28:39,213 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:28:39,213 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:28:39,230 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:28:39,248 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:28:39,282 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:28:39,472 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:28:43,080 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:28:46,120 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:28:46,122 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:28:46,126 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:28:46,126 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
2024-06-04:05:28:46,126 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:28:46,128 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:28:46,132 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:28:46,132 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
2024-06-04:05:28:46,133 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:28:46,134 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:28:46,137 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:28:46,137 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
2024-06-04:05:28:46,141 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:28:46,142 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:28:46,146 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:28:46,146 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
2024-06-04:05:28:46,164 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:28:46,164 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:28:46,165 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:28:46,166 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:28:46,170 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:28:46,171 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
2024-06-04:05:28:46,172 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:28:46,172 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:05:28:49,508 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:28:49,510 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:28:49,518 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:28:49,518 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.79s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.09s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.70s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.78s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.77s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.89s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.96s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.84s/it]2024-06-04:05:28:55,023 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:28:55,025 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:28:55,029 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:28:55,030 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.83s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.25s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.24s/it]

Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.34s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.24s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.12s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.05s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:01,  1.99s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.84s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.65s/it]
