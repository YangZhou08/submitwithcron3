Already on 'yangexp2'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/huggingface-cli", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py", line 51, in main
    service.run()
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/user.py", line 98, in run
    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 111, in login
    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 307, in _login
    raise ValueError("Invalid token passed!")
ValueError: Invalid token passed!
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-06:11:35:14,205 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:14,205 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:14,206 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:14,206 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:14,207 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:14,267 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:14,981 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:15,018 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:11:35:22,668 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:22,668 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:22,669 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:22,669 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:22,670 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:22,670 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:22,671 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:22,671 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:22,682 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:22,682 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:22,682 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:22,682 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:22,682 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-06:11:35:22,682 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-06:11:35:22,682 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-06:11:35:22,682 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-06:11:35:22,699 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:22,700 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:22,704 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:22,704 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.05}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-06:11:35:24,735 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:24,737 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:24,753 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:24,753 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.05}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-06:11:35:26,294 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:26,296 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:26,309 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:26,309 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-06:11:35:26,598 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:11:35:26,600 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:11:35:26,604 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:11:35:26,605 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.05}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:32<01:37, 32.55s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:32<01:38, 32.95s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:29<01:28, 29.46s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:32<01:38, 32.74s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:32<01:38, 32.95s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:29<01:27, 29.31s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:32<01:38, 32.99s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:31<01:33, 31.11s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:02<01:02, 31.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:03<01:03, 31.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:04<01:03, 31.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:03<01:03, 31.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:04<01:03, 31.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:00<01:00, 30.40s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:00<01:01, 30.51s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:02<01:02, 31.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:32<00:30, 30.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:34<00:31, 31.16s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:34<00:31, 31.17s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:34<00:31, 31.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:34<00:31, 31.18s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:31<00:30, 30.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:30<00:30, 30.39s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:32<00:30, 30.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:39<00:00, 21.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:39<00:00, 24.96s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:40<00:00, 21.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:40<00:00, 25.04s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:39<00:00, 21.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:39<00:00, 24.99s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:40<00:00, 21.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:40<00:00, 25.05s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:40<00:00, 21.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:40<00:00, 25.05s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 20.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 24.15s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:38<00:00, 20.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:38<00:00, 24.59s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 20.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 24.24s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:38:00,163 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:00,167 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:00,570 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 18%|█▊        | 15/82 [00:00<00:00, 144.23it/s] 37%|███▋      | 30/82 [00:00<00:00, 146.57it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:38:00,836 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:00,838 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 55%|█████▍    | 45/82 [00:00<00:00, 145.54it/s] 73%|███████▎  | 60/82 [00:00<00:00, 145.43it/s] 91%|█████████▏| 75/82 [00:00<00:00, 145.47it/s]100%|██████████| 82/82 [00:00<00:00, 145.56it/s]
2024-06-06:11:38:01,155 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 14%|█▍        | 12/83 [00:00<00:00, 110.46it/s] 31%|███▏      | 26/83 [00:00<00:00, 125.77it/s] 49%|████▉     | 41/83 [00:00<00:00, 135.66it/s] 67%|██████▋   | 56/83 [00:00<00:00, 140.47it/s] 86%|████████▌ | 71/83 [00:00<00:00, 142.91it/s]100%|██████████| 83/83 [00:00<00:00, 139.00it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:38:13,306 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:13,308 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:13,529 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:38:13,638 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:13,640 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 20%|█▉        | 16/82 [00:00<00:00, 152.59it/s] 39%|███▉      | 32/82 [00:00<00:00, 151.14it/s]2024-06-06:11:38:13,850 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
 59%|█████▊    | 48/82 [00:00<00:00, 151.06it/s]  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 147.48it/s] 78%|███████▊  | 64/82 [00:00<00:00, 150.86it/s] 36%|███▌      | 30/83 [00:00<00:00, 148.03it/s] 98%|█████████▊| 80/82 [00:00<00:00, 150.81it/s]100%|██████████| 82/82 [00:00<00:00, 150.96it/s]
 55%|█████▌    | 46/83 [00:00<00:00, 149.23it/s] 75%|███████▍  | 62/83 [00:00<00:00, 150.30it/s] 94%|█████████▍| 78/83 [00:00<00:00, 151.02it/s]100%|██████████| 83/83 [00:00<00:00, 150.26it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:11:38:15,371 INFO     [xhuggingface.py:315] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:38:15,445 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:15,447 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:15,648 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 19%|█▉        | 16/83 [00:00<00:00, 156.16it/s] 39%|███▊      | 32/83 [00:00<00:00, 157.06it/s] 58%|█████▊    | 48/83 [00:00<00:00, 157.51it/s] 77%|███████▋  | 64/83 [00:00<00:00, 157.09it/s] 96%|█████████▋| 80/83 [00:00<00:00, 157.30it/s]100%|██████████| 83/83 [00:00<00:00, 157.18it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:38:17,084 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:17,087 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:17,486 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 101.53it/s] 27%|██▋       | 22/83 [00:00<00:00, 103.00it/s] 40%|███▉      | 33/83 [00:00<00:00, 103.70it/s] 53%|█████▎    | 44/83 [00:00<00:00, 104.08it/s] 66%|██████▋   | 55/83 [00:00<00:00, 104.05it/s] 80%|███████▉  | 66/83 [00:00<00:00, 104.31it/s] 93%|█████████▎| 77/83 [00:00<00:00, 104.37it/s]100%|██████████| 83/83 [00:00<00:00, 103.98it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:38:31,357 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:31,360 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:11:38:31,713 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:31,715 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:11:38:31,943 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 11%|█         | 9/82 [00:00<00:00, 85.75it/s] 22%|██▏       | 18/82 [00:00<00:01, 57.26it/s]2024-06-06:11:38:32,328 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 30%|███       | 25/82 [00:00<00:01, 51.41it/s] 12%|█▏        | 10/82 [00:00<00:00, 90.32it/s] 43%|████▎     | 35/82 [00:00<00:00, 65.15it/s] 56%|█████▌    | 46/82 [00:00<00:00, 73.44it/s] 24%|██▍       | 20/82 [00:00<00:01, 59.48it/s] 66%|██████▌   | 54/82 [00:00<00:00, 67.45it/s] 33%|███▎      | 27/82 [00:00<00:00, 56.80it/s] 79%|███████▉  | 65/82 [00:00<00:00, 77.40it/s] 40%|████      | 33/82 [00:00<00:00, 51.12it/s] 90%|█████████ | 74/82 [00:01<00:00, 79.99it/s] 50%|█████     | 41/82 [00:00<00:00, 55.94it/s]100%|██████████| 82/82 [00:01<00:00, 68.53it/s]
 57%|█████▋    | 47/82 [00:00<00:00, 56.58it/s] 68%|██████▊   | 56/82 [00:00<00:00, 63.54it/s] 77%|███████▋  | 63/82 [00:01<00:00, 57.87it/s] 84%|████████▍ | 69/82 [00:01<00:00, 58.00it/s] 93%|█████████▎| 76/82 [00:01<00:00, 58.09it/s]100%|██████████| 82/82 [00:01<00:00, 57.89it/s]100%|██████████| 82/82 [00:01<00:00, 58.33it/s]
2024-06-06:11:38:42,628 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:11:38:42,628 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:11:38:42,628 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:11:38:42,628 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:11:38:42,628 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:11:38:42,628 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:11:38:42,628 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-06:11:38:42,634 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:27<37:14, 27.25s/it]Running generate_until requests:   2%|▏         | 2/83 [00:56<38:25, 28.46s/it]Running generate_until requests:   4%|▎         | 3/83 [01:08<28:07, 21.09s/it]Running generate_until requests:   5%|▍         | 4/83 [01:20<22:41, 17.23s/it]Running generate_until requests:   6%|▌         | 5/83 [01:33<20:28, 15.76s/it]Running generate_until requests:   7%|▋         | 6/83 [01:49<20:30, 15.98s/it]Running generate_until requests:   8%|▊         | 7/83 [02:10<22:04, 17.43s/it]Running generate_until requests:  10%|▉         | 8/83 [02:21<19:19, 15.45s/it]Running generate_until requests:  11%|█         | 9/83 [02:37<19:13, 15.59s/it]Running generate_until requests:  12%|█▏        | 10/83 [02:45<16:12, 13.32s/it]Running generate_until requests:  13%|█▎        | 11/83 [02:57<15:36, 13.01s/it]Running generate_until requests:  14%|█▍        | 12/83 [03:18<18:10, 15.35s/it]Running generate_until requests:  16%|█▌        | 13/83 [03:27<15:44, 13.50s/it]Running generate_until requests:  17%|█▋        | 14/83 [03:43<16:21, 14.23s/it]Running generate_until requests:  18%|█▊        | 15/83 [03:59<16:48, 14.83s/it]Running generate_until requests:  19%|█▉        | 16/83 [04:37<24:07, 21.60s/it]Running generate_until requests:  20%|██        | 17/83 [04:53<22:09, 20.15s/it]Running generate_until requests:  22%|██▏       | 18/83 [05:06<19:17, 17.81s/it]Running generate_until requests:  23%|██▎       | 19/83 [05:22<18:30, 17.35s/it]Running generate_until requests:  24%|██▍       | 20/83 [05:44<19:36, 18.68s/it]Running generate_until requests:  25%|██▌       | 21/83 [06:05<20:00, 19.36s/it]Running generate_until requests:  27%|██▋       | 22/83 [06:15<16:59, 16.71s/it]Running generate_until requests:  28%|██▊       | 23/83 [06:43<20:06, 20.10s/it]Running generate_until requests:  29%|██▉       | 24/83 [06:54<16:54, 17.20s/it]Running generate_until requests:  30%|███       | 25/83 [07:17<18:20, 18.97s/it]Running generate_until requests:  31%|███▏      | 26/83 [07:32<16:45, 17.65s/it]Running generate_until requests:  33%|███▎      | 27/83 [08:00<19:36, 21.00s/it]Running generate_until requests:  34%|███▎      | 28/83 [08:12<16:45, 18.28s/it]Running generate_until requests:  35%|███▍      | 29/83 [08:28<15:49, 17.58s/it]Running generate_until requests:  36%|███▌      | 30/83 [08:45<15:12, 17.22s/it]Running generate_until requests:  37%|███▋      | 31/83 [09:00<14:25, 16.64s/it]Running generate_until requests:  39%|███▊      | 32/83 [09:17<14:14, 16.76s/it]Running generate_until requests:  40%|███▉      | 33/83 [09:24<11:32, 13.85s/it]Running generate_until requests:  41%|████      | 34/83 [09:47<13:27, 16.48s/it]Running generate_until requests:  42%|████▏     | 35/83 [10:04<13:18, 16.63s/it]Running generate_until requests:  43%|████▎     | 36/83 [10:18<12:28, 15.93s/it]Running generate_until requests:  45%|████▍     | 37/83 [10:33<12:04, 15.75s/it]Running generate_until requests:  46%|████▌     | 38/83 [10:56<13:26, 17.91s/it]Running generate_until requests:  47%|████▋     | 39/83 [11:04<10:53, 14.84s/it]Running generate_until requests:  48%|████▊     | 40/83 [11:18<10:29, 14.65s/it]Running generate_until requests:  49%|████▉     | 41/83 [11:31<09:53, 14.13s/it]Running generate_until requests:  51%|█████     | 42/83 [11:48<10:20, 15.15s/it]Running generate_until requests:  52%|█████▏    | 43/83 [12:01<09:32, 14.32s/it]Running generate_until requests:  53%|█████▎    | 44/83 [12:14<09:08, 14.07s/it]Running generate_until requests:  54%|█████▍    | 45/83 [12:25<08:12, 12.97s/it]Running generate_until requests:  55%|█████▌    | 46/83 [12:37<07:46, 12.61s/it]Running generate_until requests:  57%|█████▋    | 47/83 [12:55<08:41, 14.50s/it]Running generate_until requests:  58%|█████▊    | 48/83 [13:09<08:13, 14.11s/it]Running generate_until requests:  59%|█████▉    | 49/83 [13:18<07:12, 12.71s/it]Running generate_until requests:  60%|██████    | 50/83 [13:38<08:12, 14.92s/it]Running generate_until requests:  61%|██████▏   | 51/83 [13:45<06:36, 12.39s/it]Running generate_until requests:  63%|██████▎   | 52/83 [13:56<06:16, 12.15s/it]Running generate_until requests:  64%|██████▍   | 53/83 [14:08<06:01, 12.05s/it]Running generate_until requests:  65%|██████▌   | 54/83 [14:18<05:27, 11.28s/it]Running generate_until requests:  66%|██████▋   | 55/83 [14:28<05:07, 10.99s/it]Running generate_until requests:  67%|██████▋   | 56/83 [15:02<08:06, 18.01s/it]Running generate_until requests:  69%|██████▊   | 57/83 [15:25<08:24, 19.40s/it]Running generate_until requests:  70%|██████▉   | 58/83 [15:36<06:59, 16.77s/it]Running generate_until requests:  71%|███████   | 59/83 [15:44<05:43, 14.30s/it]Running generate_until requests:  72%|███████▏  | 60/83 [16:00<05:40, 14.81s/it]Running generate_until requests:  73%|███████▎  | 61/83 [16:19<05:55, 16.15s/it]Running generate_until requests:  75%|███████▍  | 62/83 [16:30<05:06, 14.57s/it]Running generate_until requests:  76%|███████▌  | 63/83 [16:43<04:38, 13.93s/it]Running generate_until requests:  77%|███████▋  | 64/83 [16:57<04:28, 14.15s/it]Running generate_until requests:  78%|███████▊  | 65/83 [17:07<03:49, 12.75s/it]Running generate_until requests:  80%|███████▉  | 66/83 [17:24<03:59, 14.08s/it]Running generate_until requests:  81%|████████  | 67/83 [17:33<03:20, 12.50s/it]Running generate_until requests:  82%|████████▏ | 68/83 [17:43<02:56, 11.80s/it]Running generate_until requests:  83%|████████▎ | 69/83 [17:59<03:03, 13.08s/it]Running generate_until requests:  84%|████████▍ | 70/83 [18:15<03:01, 13.93s/it]Running generate_until requests:  86%|████████▌ | 71/83 [18:22<02:22, 11.88s/it]Running generate_until requests:  87%|████████▋ | 72/83 [18:33<02:08, 11.69s/it]Running generate_until requests:  88%|████████▊ | 73/83 [18:45<01:58, 11.83s/it]Running generate_until requests:  89%|████████▉ | 74/83 [19:00<01:53, 12.67s/it]Running generate_until requests:  90%|█████████ | 75/83 [19:20<01:59, 14.91s/it]Running generate_until requests:  92%|█████████▏| 76/83 [19:25<01:23, 11.91s/it]Running generate_until requests:  93%|█████████▎| 77/83 [19:44<01:24, 14.06s/it]Running generate_until requests:  94%|█████████▍| 78/83 [19:55<01:05, 13.03s/it]Running generate_until requests:  95%|█████████▌| 79/83 [20:11<00:55, 13.94s/it]Running generate_until requests:  96%|█████████▋| 80/83 [20:24<00:40, 13.65s/it]Running generate_until requests:  98%|█████████▊| 81/83 [20:36<00:26, 13.06s/it]Running generate_until requests:  99%|█████████▉| 82/83 [20:48<00:12, 12.80s/it]Running generate_until requests: 100%|██████████| 83/83 [20:55<00:00, 11.27s/it]Running generate_until requests: 100%|██████████| 83/83 [20:55<00:00, 15.13s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-06:12:04:28,684 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:04:28,802 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:04:28,844 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:04:29,000 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:04:29,010 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:04:29,109 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:04:29,507 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:04:29,522 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:04:35,605 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:04:35,607 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:04:35,611 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:04:35,611 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-06:12:04:35,840 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:04:35,841 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:04:35,844 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:04:35,844 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-06:12:04:35,937 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:04:35,938 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:04:35,943 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:04:35,943 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-06:12:04:36,153 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:04:36,154 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:04:36,159 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:04:36,159 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-06:12:04:36,392 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:04:36,394 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:04:36,399 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:04:36,399 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-06:12:04:36,585 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:04:36,586 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:04:36,590 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:04:36,590 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-06:12:04:37,841 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:04:37,843 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:04:37,847 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:04:37,847 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-06:12:04:37,868 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:04:37,870 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:04:37,874 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:04:37,874 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.11s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.50s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.73s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.70s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.48s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.24s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.76s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.18s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.75s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.78s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.97s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.19s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.95s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.22s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.04s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.35s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.39s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:05:36,998 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:05:37,000 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:05:37,242 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 18%|█▊        | 15/82 [00:00<00:00, 145.68it/s] 37%|███▋      | 30/82 [00:00<00:00, 146.80it/s] 55%|█████▍    | 45/82 [00:00<00:00, 147.21it/s] 73%|███████▎  | 60/82 [00:00<00:00, 146.80it/s] 91%|█████████▏| 75/82 [00:00<00:00, 146.56it/s]100%|██████████| 82/82 [00:00<00:00, 146.61it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:05:37,991 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:05:37,993 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:05:38,200 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 149.80it/s] 37%|███▋      | 31/83 [00:00<00:00, 150.02it/s] 57%|█████▋    | 47/83 [00:00<00:00, 150.38it/s] 76%|███████▌  | 63/83 [00:00<00:00, 150.55it/s] 95%|█████████▌| 79/83 [00:00<00:00, 149.91it/s]100%|██████████| 83/83 [00:00<00:00, 149.90it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:05:54,133 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:05:54,136 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:05:54,361 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 18%|█▊        | 15/82 [00:00<00:00, 147.67it/s] 37%|███▋      | 30/82 [00:00<00:00, 148.47it/s] 55%|█████▍    | 45/82 [00:00<00:00, 148.49it/s] 73%|███████▎  | 60/82 [00:00<00:00, 148.53it/s] 91%|█████████▏| 75/82 [00:00<00:00, 148.54it/s]100%|██████████| 82/82 [00:00<00:00, 148.61it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:05:56,053 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:05:56,055 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:05:56,271 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 20%|█▉        | 16/82 [00:00<00:00, 152.47it/s] 39%|███▉      | 32/82 [00:00<00:00, 153.17it/s] 59%|█████▊    | 48/82 [00:00<00:00, 153.45it/s] 78%|███████▊  | 64/82 [00:00<00:00, 153.42it/s] 98%|█████████▊| 80/82 [00:00<00:00, 153.49it/s]100%|██████████| 82/82 [00:00<00:00, 153.36it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:12:05:57,398 INFO     [xhuggingface.py:315] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:05:57,470 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:05:57,472 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:05:57,680 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:05:57,778 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:05:57,781 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 19%|█▉        | 16/83 [00:00<00:00, 157.01it/s] 39%|███▊      | 32/83 [00:00<00:00, 155.93it/s] 58%|█████▊    | 48/83 [00:00<00:00, 154.97it/s]2024-06-06:12:05:58,010 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 77%|███████▋  | 64/83 [00:00<00:00, 154.36it/s] 19%|█▉        | 16/83 [00:00<00:00, 151.96it/s] 96%|█████████▋| 80/83 [00:00<00:00, 154.04it/s]100%|██████████| 83/83 [00:00<00:00, 154.51it/s]
 39%|███▊      | 32/83 [00:00<00:00, 152.42it/s] 58%|█████▊    | 48/83 [00:00<00:00, 129.66it/s] 75%|███████▍  | 62/83 [00:00<00:00, 114.77it/s] 89%|████████▉ | 74/83 [00:00<00:00, 109.34it/s]100%|██████████| 83/83 [00:00<00:00, 109.42it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:06:12,131 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:12,134 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:12,424 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 101.62it/s] 28%|██▊       | 23/82 [00:00<00:00, 107.45it/s] 41%|████▏     | 34/82 [00:00<00:00, 108.56it/s] 56%|█████▌    | 46/82 [00:00<00:00, 109.25it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 71%|███████   | 58/82 [00:00<00:00, 112.53it/s] 90%|█████████ | 74/82 [00:00<00:00, 126.65it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:06:13,103 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:06:13,105 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
100%|██████████| 82/82 [00:00<00:00, 118.61it/s]
2024-06-06:12:06:13,443 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 104.83it/s] 27%|██▋       | 22/83 [00:00<00:00, 99.88it/s]  40%|███▉      | 33/83 [00:00<00:00, 94.86it/s] 52%|█████▏    | 43/83 [00:00<00:00, 93.21it/s] 64%|██████▍   | 53/83 [00:00<00:00, 92.28it/s] 76%|███████▌  | 63/83 [00:00<00:00, 91.88it/s] 88%|████████▊ | 73/83 [00:00<00:00, 91.20it/s]100%|██████████| 83/83 [00:00<00:00, 94.68it/s]
2024-06-06:12:06:21,738 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:06:21,738 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:06:21,738 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:06:21,738 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:06:21,738 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:06:21,738 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:06:21,754 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:06:21,754 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:22<31:00, 22.69s/it]Running generate_until requests:   2%|▏         | 2/83 [00:52<35:58, 26.65s/it]Running generate_until requests:   4%|▎         | 3/83 [01:04<26:51, 20.14s/it]Running generate_until requests:   5%|▍         | 4/83 [01:17<22:32, 17.12s/it]Running generate_until requests:   6%|▌         | 5/83 [01:30<20:24, 15.69s/it]Running generate_until requests:   7%|▋         | 6/83 [01:40<17:54, 13.96s/it]Running generate_until requests:   8%|▊         | 7/83 [02:12<25:15, 19.94s/it]Running generate_until requests:  10%|▉         | 8/83 [02:28<22:58, 18.39s/it]Running generate_until requests:  11%|█         | 9/83 [02:42<21:19, 17.29s/it]Running generate_until requests:  12%|█▏        | 10/83 [02:52<18:02, 14.82s/it]Running generate_until requests:  13%|█▎        | 11/83 [03:08<18:08, 15.12s/it]Running generate_until requests:  14%|█▍        | 12/83 [03:28<19:55, 16.84s/it]Running generate_until requests:  16%|█▌        | 13/83 [03:39<17:31, 15.02s/it]Running generate_until requests:  17%|█▋        | 14/83 [03:55<17:41, 15.38s/it]Running generate_until requests:  18%|█▊        | 15/83 [04:43<28:21, 25.03s/it]Running generate_until requests:  19%|█▉        | 16/83 [05:20<32:13, 28.86s/it]Running generate_until requests:  20%|██        | 17/83 [05:38<28:03, 25.50s/it]Running generate_until requests:  22%|██▏       | 18/83 [05:49<22:55, 21.16s/it]Running generate_until requests:  23%|██▎       | 19/83 [06:04<20:36, 19.32s/it]Running generate_until requests:  24%|██▍       | 20/83 [06:30<22:24, 21.35s/it]Running generate_until requests:  25%|██▌       | 21/83 [06:45<20:03, 19.41s/it]Running generate_until requests:  27%|██▋       | 22/83 [06:58<17:45, 17.47s/it]Running generate_until requests:  28%|██▊       | 23/83 [07:31<22:05, 22.10s/it]Running generate_until requests:  29%|██▉       | 24/83 [07:48<20:11, 20.53s/it]Running generate_until requests:  30%|███       | 25/83 [08:12<20:57, 21.68s/it]Running generate_until requests:  31%|███▏      | 26/83 [08:27<18:38, 19.62s/it]Running generate_until requests:  33%|███▎      | 27/83 [08:54<20:20, 21.79s/it]Running generate_until requests:  34%|███▎      | 28/83 [09:08<17:44, 19.36s/it]Running generate_until requests:  35%|███▍      | 29/83 [09:34<19:19, 21.48s/it]Running generate_until requests:  36%|███▌      | 30/83 [09:53<18:14, 20.64s/it]Running generate_until requests:  37%|███▋      | 31/83 [10:12<17:36, 20.33s/it]Running generate_until requests:  39%|███▊      | 32/83 [10:33<17:24, 20.48s/it]Running generate_until requests:  40%|███▉      | 33/83 [10:40<13:42, 16.46s/it]Running generate_until requests:  41%|████      | 34/83 [11:04<15:18, 18.74s/it]Running generate_until requests:  42%|████▏     | 35/83 [11:21<14:32, 18.19s/it]Running generate_until requests:  43%|████▎     | 36/83 [11:40<14:26, 18.43s/it]Running generate_until requests:  45%|████▍     | 37/83 [11:56<13:25, 17.52s/it]Running generate_until requests:  46%|████▌     | 38/83 [12:33<17:33, 23.42s/it]Running generate_until requests:  47%|████▋     | 39/83 [12:41<13:45, 18.75s/it]Running generate_until requests:  48%|████▊     | 40/83 [12:55<12:29, 17.43s/it]Running generate_until requests:  49%|████▉     | 41/83 [13:08<11:17, 16.14s/it]Running generate_until requests:  51%|█████     | 42/83 [14:05<19:21, 28.33s/it]Running generate_until requests:  52%|█████▏    | 43/83 [14:17<15:40, 23.51s/it]Running generate_until requests:  53%|█████▎    | 44/83 [14:31<13:18, 20.47s/it]Running generate_until requests:  54%|█████▍    | 45/83 [14:41<11:05, 17.52s/it]Running generate_until requests:  55%|█████▌    | 46/83 [14:53<09:45, 15.83s/it]Running generate_until requests:  57%|█████▋    | 47/83 [15:10<09:36, 16.01s/it]Running generate_until requests:  58%|█████▊    | 48/83 [15:27<09:32, 16.35s/it]Running generate_until requests:  59%|█████▉    | 49/83 [15:36<08:07, 14.33s/it]Running generate_until requests:  60%|██████    | 50/83 [15:57<08:51, 16.11s/it]Running generate_until requests:  61%|██████▏   | 51/83 [16:10<08:08, 15.27s/it]Running generate_until requests:  63%|██████▎   | 52/83 [16:21<07:17, 14.10s/it]Running generate_until requests:  64%|██████▍   | 53/83 [16:33<06:44, 13.50s/it]Running generate_until requests:  65%|██████▌   | 54/83 [16:51<07:07, 14.73s/it]Running generate_until requests:  66%|██████▋   | 55/83 [17:03<06:30, 13.93s/it]Running generate_until requests:  67%|██████▋   | 56/83 [17:43<09:44, 21.66s/it]Running generate_until requests:  69%|██████▊   | 57/83 [18:20<11:21, 26.22s/it]Running generate_until requests:  70%|██████▉   | 58/83 [18:30<08:58, 21.55s/it]Running generate_until requests:  71%|███████   | 59/83 [18:40<07:14, 18.09s/it]Running generate_until requests:  72%|███████▏  | 60/83 [18:58<06:51, 17.89s/it]Running generate_until requests:  73%|███████▎  | 61/83 [19:17<06:41, 18.26s/it]Running generate_until requests:  75%|███████▍  | 62/83 [19:32<06:07, 17.50s/it]Running generate_until requests:  76%|███████▌  | 63/83 [19:46<05:26, 16.31s/it]Running generate_until requests:  77%|███████▋  | 64/83 [20:03<05:16, 16.66s/it]Running generate_until requests:  78%|███████▊  | 65/83 [20:14<04:26, 14.81s/it]Running generate_until requests:  80%|███████▉  | 66/83 [20:25<03:51, 13.61s/it]Running generate_until requests:  81%|████████  | 67/83 [20:34<03:14, 12.16s/it]Running generate_until requests:  82%|████████▏ | 68/83 [20:44<02:53, 11.55s/it]Running generate_until requests:  83%|████████▎ | 69/83 [21:11<03:46, 16.20s/it]Running generate_until requests:  84%|████████▍ | 70/83 [21:28<03:35, 16.56s/it]Running generate_until requests:  86%|████████▌ | 71/83 [21:35<02:44, 13.73s/it]Running generate_until requests:  87%|████████▋ | 72/83 [21:40<02:02, 11.11s/it]Running generate_until requests:  88%|████████▊ | 73/83 [21:54<01:57, 11.79s/it]Running generate_until requests:  89%|████████▉ | 74/83 [22:08<01:53, 12.65s/it]Running generate_until requests:  90%|█████████ | 75/83 [22:35<02:15, 16.97s/it]Running generate_until requests:  92%|█████████▏| 76/83 [22:47<01:48, 15.45s/it]Running generate_until requests:  93%|█████████▎| 77/83 [23:01<01:29, 14.88s/it]Running generate_until requests:  94%|█████████▍| 78/83 [23:11<01:07, 13.56s/it]Running generate_until requests:  95%|█████████▌| 79/83 [23:36<01:07, 16.85s/it]Running generate_until requests:  96%|█████████▋| 80/83 [23:48<00:46, 15.41s/it]Running generate_until requests:  98%|█████████▊| 81/83 [24:00<00:28, 14.34s/it]Running generate_until requests:  99%|█████████▉| 82/83 [24:12<00:13, 13.60s/it]Running generate_until requests: 100%|██████████| 83/83 [24:19<00:00, 11.87s/it]Running generate_until requests: 100%|██████████| 83/83 [24:19<00:00, 17.59s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-06:12:32:48,947 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:32:48,948 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:32:48,948 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:32:49,016 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:32:49,077 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:32:49,792 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:32:50,192 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:32:50,636 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:12:32:55,697 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:32:55,698 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:32:55,702 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:32:55,702 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-06:12:32:55,800 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:32:55,801 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:32:55,805 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:32:55,805 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-06:12:32:55,879 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:32:55,880 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:32:55,884 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:32:55,884 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-06:12:32:56,289 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:32:56,290 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:32:56,293 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:32:56,294 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-06:12:32:56,522 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:32:56,523 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:32:56,527 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:32:56,527 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.15}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.27s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.13s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.99s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.92s/it]2024-06-06:12:33:01,374 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:33:01,375 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:33:01,386 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:33:01,386 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.15}
Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.17s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.95s/it]2024-06-06:12:33:01,917 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:33:01,919 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:33:01,923 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:33:01,924 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-06:12:33:02,657 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:12:33:02,658 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:12:33:02,663 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:12:33:02,663 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.15}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.94s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.19s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.03s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.54s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.77s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.65s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.72s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.78s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.21s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.20s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.70s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.88s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.85s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.36s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.27s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:33:55,816 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:33:55,818 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:33:56,147 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 108.77it/s] 28%|██▊       | 23/83 [00:00<00:00, 109.60it/s] 41%|████      | 34/83 [00:00<00:00, 109.75it/s] 54%|█████▍    | 45/83 [00:00<00:00, 109.72it/s] 67%|██████▋   | 56/83 [00:00<00:00, 109.62it/s] 81%|████████  | 67/83 [00:00<00:00, 109.71it/s] 94%|█████████▍| 78/83 [00:00<00:00, 109.60it/s]100%|██████████| 83/83 [00:00<00:00, 109.54it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:34:02,420 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:02,422 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:02,637 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 147.98it/s] 37%|███▋      | 31/83 [00:00<00:00, 149.11it/s] 55%|█████▌    | 46/83 [00:00<00:00, 149.49it/s] 73%|███████▎  | 61/83 [00:00<00:00, 149.31it/s] 92%|█████████▏| 76/83 [00:00<00:00, 149.34it/s]100%|██████████| 83/83 [00:00<00:00, 149.26it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:34:14,567 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:14,569 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:14,838 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 149.92it/s] 37%|███▋      | 31/83 [00:00<00:00, 150.76it/s] 57%|█████▋    | 47/83 [00:00<00:00, 151.15it/s] 76%|███████▌  | 63/83 [00:00<00:00, 151.15it/s] 95%|█████████▌| 79/83 [00:00<00:00, 151.32it/s]100%|██████████| 83/83 [00:00<00:00, 150.93it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:34:16,821 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:16,823 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:17,034 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 20%|█▉        | 16/82 [00:00<00:00, 151.97it/s] 39%|███▉      | 32/82 [00:00<00:00, 152.41it/s] 59%|█████▊    | 48/82 [00:00<00:00, 152.61it/s] 78%|███████▊  | 64/82 [00:00<00:00, 152.64it/s] 98%|█████████▊| 80/82 [00:00<00:00, 153.04it/s]100%|██████████| 82/82 [00:00<00:00, 152.81it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:12:34:18,186 INFO     [xhuggingface.py:315] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:34:18,257 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:18,259 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:18,460 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 19%|█▉        | 16/83 [00:00<00:00, 152.25it/s] 39%|███▊      | 32/83 [00:00<00:00, 154.42it/s] 58%|█████▊    | 48/83 [00:00<00:00, 155.51it/s] 77%|███████▋  | 64/83 [00:00<00:00, 155.86it/s] 96%|█████████▋| 80/83 [00:00<00:00, 155.89it/s]100%|██████████| 83/83 [00:00<00:00, 155.47it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:34:19,853 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:19,856 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:20,263 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 11%|█         | 9/82 [00:00<00:00, 84.55it/s] 22%|██▏       | 18/82 [00:00<00:00, 85.17it/s] 33%|███▎      | 27/82 [00:00<00:00, 85.32it/s] 44%|████▍     | 36/82 [00:00<00:00, 85.23it/s] 55%|█████▍    | 45/82 [00:00<00:00, 85.40it/s] 66%|██████▌   | 54/82 [00:00<00:00, 85.19it/s] 77%|███████▋  | 63/82 [00:00<00:00, 85.28it/s] 88%|████████▊ | 72/82 [00:00<00:00, 85.32it/s] 99%|█████████▉| 81/82 [00:00<00:00, 85.49it/s]100%|██████████| 82/82 [00:00<00:00, 85.29it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:34:36,081 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:36,083 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:36,629 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 15%|█▍        | 12/82 [00:00<00:00, 110.85it/s] 29%|██▉       | 24/82 [00:00<00:00, 111.19it/s] 44%|████▍     | 36/82 [00:00<00:00, 111.24it/s] 59%|█████▊    | 48/82 [00:00<00:00, 111.40it/s] 73%|███████▎  | 60/82 [00:00<00:00, 111.39it/s] 88%|████████▊ | 72/82 [00:00<00:00, 111.48it/s]100%|██████████| 82/82 [00:00<00:00, 111.39it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:12:34:38,802 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:38,805 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:12:34:39,200 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 12%|█▏        | 10/82 [00:00<00:00, 92.86it/s] 24%|██▍       | 20/82 [00:00<00:00, 92.25it/s] 37%|███▋      | 30/82 [00:00<00:00, 92.06it/s] 49%|████▉     | 40/82 [00:00<00:00, 91.83it/s] 61%|██████    | 50/82 [00:00<00:00, 91.58it/s] 73%|███████▎  | 60/82 [00:00<00:00, 91.56it/s] 85%|████████▌ | 70/82 [00:00<00:00, 91.50it/s] 98%|█████████▊| 80/82 [00:00<00:00, 91.38it/s]100%|██████████| 82/82 [00:00<00:00, 91.62it/s]
2024-06-06:12:34:44,882 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:34:44,882 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:34:44,882 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:34:44,882 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:34:44,882 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:34:44,883 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-06:12:34:44,898 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:12:34:44,901 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:25<34:38, 25.35s/it]Running generate_until requests:   2%|▏         | 2/83 [00:53<36:30, 27.04s/it]Running generate_until requests:   4%|▎         | 3/83 [01:06<27:09, 20.37s/it]Running generate_until requests:   5%|▍         | 4/83 [01:18<22:46, 17.30s/it]Running generate_until requests:   6%|▌         | 5/83 [01:31<20:32, 15.80s/it]Running generate_until requests:   7%|▋         | 6/83 [01:42<18:00, 14.03s/it]Running generate_until requests:   8%|▊         | 7/83 [02:10<23:32, 18.59s/it]Running generate_until requests:  10%|▉         | 8/83 [02:25<21:45, 17.40s/it]Running generate_until requests:  11%|█         | 9/83 [02:45<22:30, 18.25s/it]Running generate_until requests:  12%|█▏        | 10/83 [02:56<19:29, 16.02s/it]Running generate_until requests:  13%|█▎        | 11/83 [03:15<20:28, 17.07s/it]Running generate_until requests:  14%|█▍        | 12/83 [03:38<22:20, 18.88s/it]Running generate_until requests:  16%|█▌        | 13/83 [03:49<19:14, 16.49s/it]Running generate_until requests:  17%|█▋        | 14/83 [04:09<19:57, 17.36s/it]Running generate_until requests:  18%|█▊        | 15/83 [04:46<26:37, 23.49s/it]Running generate_until requests:  19%|█▉        | 16/83 [05:18<29:01, 25.99s/it]Running generate_until requests:  20%|██        | 17/83 [05:36<25:46, 23.43s/it]Running generate_until requests:  22%|██▏       | 18/83 [05:47<21:25, 19.78s/it]Running generate_until requests:  23%|██▎       | 19/83 [05:59<18:31, 17.37s/it]Running generate_until requests:  24%|██▍       | 20/83 [06:24<20:42, 19.73s/it]Running generate_until requests:  25%|██▌       | 21/83 [06:41<19:26, 18.82s/it]Running generate_until requests:  27%|██▋       | 22/83 [06:54<17:22, 17.09s/it]Running generate_until requests:  28%|██▊       | 23/83 [07:27<21:50, 21.84s/it]Running generate_until requests:  29%|██▉       | 24/83 [07:38<18:31, 18.85s/it]Running generate_until requests:  30%|███       | 25/83 [08:04<20:18, 21.02s/it]Running generate_until requests:  31%|███▏      | 26/83 [08:18<17:47, 18.73s/it]Running generate_until requests:  33%|███▎      | 27/83 [08:45<19:52, 21.30s/it]Running generate_until requests:  34%|███▎      | 28/83 [08:58<17:07, 18.69s/it]Running generate_until requests:  35%|███▍      | 29/83 [09:19<17:32, 19.50s/it]Running generate_until requests:  36%|███▌      | 30/83 [09:34<15:56, 18.05s/it]Running generate_until requests:  37%|███▋      | 31/83 [09:51<15:30, 17.89s/it]Running generate_until requests:  39%|███▊      | 32/83 [10:07<14:30, 17.07s/it]Running generate_until requests:  40%|███▉      | 33/83 [10:12<11:22, 13.66s/it]Running generate_until requests:  41%|████      | 34/83 [10:35<13:27, 16.48s/it]Running generate_until requests:  42%|████▏     | 35/83 [10:48<12:22, 15.48s/it]Running generate_until requests:  43%|████▎     | 36/83 [11:03<11:59, 15.30s/it]Running generate_until requests:  45%|████▍     | 37/83 [11:23<12:38, 16.48s/it]Running generate_until requests:  46%|████▌     | 38/83 [11:53<15:24, 20.54s/it]Running generate_until requests:  47%|████▋     | 39/83 [12:03<12:45, 17.40s/it]Running generate_until requests:  48%|████▊     | 40/83 [12:17<11:47, 16.45s/it]Running generate_until requests:  49%|████▉     | 41/83 [12:31<10:58, 15.68s/it]Running generate_until requests:  51%|█████     | 42/83 [12:50<11:28, 16.80s/it]Running generate_until requests:  52%|█████▏    | 43/83 [13:01<09:57, 14.94s/it]Running generate_until requests:  53%|█████▎    | 44/83 [13:11<08:52, 13.65s/it]Running generate_until requests:  54%|█████▍    | 45/83 [13:20<07:38, 12.05s/it]Running generate_until requests:  55%|█████▌    | 46/83 [13:30<07:05, 11.50s/it]Running generate_until requests:  57%|█████▋    | 47/83 [13:43<07:09, 11.92s/it]Running generate_until requests:  58%|█████▊    | 48/83 [13:58<07:35, 13.03s/it]Running generate_until requests:  59%|█████▉    | 49/83 [14:06<06:27, 11.40s/it]Running generate_until requests:  60%|██████    | 50/83 [14:22<06:59, 12.71s/it]Running generate_until requests:  61%|██████▏   | 51/83 [14:29<05:54, 11.06s/it]Running generate_until requests:  63%|██████▎   | 52/83 [14:38<05:24, 10.47s/it]Running generate_until requests:  64%|██████▍   | 53/83 [14:47<05:03, 10.12s/it]Running generate_until requests:  65%|██████▌   | 54/83 [15:02<05:33, 11.49s/it]Running generate_until requests:  66%|██████▋   | 55/83 [15:16<05:38, 12.08s/it]Running generate_until requests:  67%|██████▋   | 56/83 [15:52<08:41, 19.31s/it]Running generate_until requests:  69%|██████▊   | 57/83 [16:26<10:15, 23.67s/it]Running generate_until requests:  70%|██████▉   | 58/83 [16:36<08:12, 19.72s/it]Running generate_until requests:  71%|███████   | 59/83 [16:46<06:44, 16.85s/it]Running generate_until requests:  72%|███████▏  | 60/83 [17:01<06:15, 16.34s/it]Running generate_until requests:  73%|███████▎  | 61/83 [17:16<05:48, 15.83s/it]Running generate_until requests:  75%|███████▍  | 62/83 [17:31<05:27, 15.59s/it]Running generate_until requests:  76%|███████▌  | 63/83 [17:44<04:58, 14.91s/it]Running generate_until requests:  77%|███████▋  | 64/83 [18:12<05:52, 18.58s/it]Running generate_until requests:  78%|███████▊  | 65/83 [18:22<04:51, 16.18s/it]Running generate_until requests:  80%|███████▉  | 66/83 [18:33<04:06, 14.50s/it]Running generate_until requests:  81%|████████  | 67/83 [18:43<03:33, 13.33s/it]Running generate_until requests:  82%|████████▏ | 68/83 [18:54<03:07, 12.52s/it]Running generate_until requests:  83%|████████▎ | 69/83 [19:21<03:57, 16.93s/it]Running generate_until requests:  84%|████████▍ | 70/83 [19:37<03:35, 16.60s/it]Running generate_until requests:  86%|████████▌ | 71/83 [19:44<02:45, 13.79s/it]Running generate_until requests:  87%|████████▋ | 72/83 [19:56<02:24, 13.16s/it]Running generate_until requests:  88%|████████▊ | 73/83 [20:08<02:08, 12.90s/it]Running generate_until requests:  89%|████████▉ | 74/83 [20:25<02:07, 14.20s/it]Running generate_until requests:  90%|█████████ | 75/83 [20:49<02:15, 16.93s/it]Running generate_until requests:  92%|█████████▏| 76/83 [20:58<01:42, 14.64s/it]Running generate_until requests:  93%|█████████▎| 77/83 [21:11<01:24, 14.06s/it]Running generate_until requests:  94%|█████████▍| 78/83 [21:21<01:04, 12.97s/it]Running generate_until requests:  95%|█████████▌| 79/83 [21:47<01:07, 16.85s/it]Running generate_until requests:  96%|█████████▋| 80/83 [21:59<00:46, 15.44s/it]Running generate_until requests:  98%|█████████▊| 81/83 [22:11<00:28, 14.31s/it]Running generate_until requests:  99%|█████████▉| 82/83 [22:23<00:13, 13.60s/it]Running generate_until requests: 100%|██████████| 83/83 [22:35<00:00, 13.15s/it]Running generate_until requests: 100%|██████████| 83/83 [22:35<00:00, 16.33s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-06:13:03:43,384 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:03:43,384 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:03:43,456 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:03:43,487 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:03:43,542 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:03:43,680 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:03:44,267 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:03:44,319 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:03:49,970 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:03:49,971 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:03:49,975 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:03:49,975 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-06:13:03:50,175 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:03:50,176 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:03:50,179 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:03:50,180 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-06:13:03:50,888 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:03:50,890 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:03:50,894 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:03:50,895 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-06:13:03:50,947 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:03:50,948 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:03:50,953 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:03:50,953 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-06:13:03:51,015 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:03:51,017 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:03:51,021 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:03:51,021 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-06:13:03:51,438 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:03:51,439 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:03:51,442 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:03:51,442 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.2}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-06:13:03:52,516 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:03:52,517 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:03:52,522 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:03:52,522 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-06:13:03:52,648 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:03:52,649 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:03:52,654 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:03:52,654 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.2}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.30s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.73s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.12s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.92s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.10s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.69s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.35s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.22s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:04,  2.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.78s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.49s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.06s/it]

Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.15s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.99s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.03s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.15s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.91s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.99s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.39s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.44s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:13:04:49,336 INFO     [xhuggingface.py:315] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:04:49,411 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:04:49,413 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:04:49,729 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 14%|█▍        | 12/83 [00:00<00:00, 110.05it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 31%|███▏      | 26/83 [00:00<00:00, 125.52it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:04:49,968 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:04:49,971 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 47%|████▋     | 39/83 [00:00<00:00, 117.74it/s] 61%|██████▏   | 51/83 [00:00<00:00, 110.70it/s] 76%|███████▌  | 63/83 [00:00<00:00, 106.43it/s]2024-06-06:13:04:50,348 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 89%|████████▉ | 74/83 [00:00<00:00, 104.36it/s] 12%|█▏        | 10/82 [00:00<00:00, 97.32it/s]100%|██████████| 83/83 [00:00<00:00, 111.40it/s]
 27%|██▋       | 22/82 [00:00<00:00, 104.95it/s] 40%|████      | 33/82 [00:00<00:00, 107.15it/s] 55%|█████▍    | 45/82 [00:00<00:00, 108.38it/s] 68%|██████▊   | 56/82 [00:00<00:00, 108.80it/s] 83%|████████▎ | 68/82 [00:00<00:00, 109.26it/s] 96%|█████████▋| 79/82 [00:00<00:00, 109.44it/s]100%|██████████| 82/82 [00:00<00:00, 108.16it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:05:12,270 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:05:12,272 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:05:12,586 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 106.93it/s] 27%|██▋       | 22/83 [00:00<00:00, 106.85it/s] 40%|███▉      | 33/83 [00:00<00:00, 107.17it/s] 53%|█████▎    | 44/83 [00:00<00:00, 107.30it/s] 66%|██████▋   | 55/83 [00:00<00:00, 107.51it/s] 80%|███████▉  | 66/83 [00:00<00:00, 107.75it/s] 93%|█████████▎| 77/83 [00:00<00:00, 107.71it/s]100%|██████████| 83/83 [00:00<00:00, 107.54it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:05:21,877 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:05:21,880 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:05:22,206 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 105.47it/s] 27%|██▋       | 22/82 [00:00<00:00, 105.48it/s] 40%|████      | 33/82 [00:00<00:00, 105.76it/s] 54%|█████▎    | 44/82 [00:00<00:00, 105.81it/s] 67%|██████▋   | 55/82 [00:00<00:00, 105.90it/s] 80%|████████  | 66/82 [00:00<00:00, 106.12it/s] 94%|█████████▍| 77/82 [00:00<00:00, 106.11it/s]100%|██████████| 82/82 [00:00<00:00, 105.90it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:05:24,476 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:05:24,482 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:05:24,798 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 105.74it/s] 27%|██▋       | 22/82 [00:00<00:00, 105.33it/s] 40%|████      | 33/82 [00:00<00:00, 105.55it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 54%|█████▎    | 44/82 [00:00<00:00, 105.60it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:05:25,267 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:05:25,269 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 67%|██████▋   | 55/82 [00:00<00:00, 105.64it/s] 80%|████████  | 66/82 [00:00<00:00, 105.31it/s] 94%|█████████▍| 77/82 [00:00<00:00, 105.17it/s]100%|██████████| 82/82 [00:00<00:00, 105.35it/s]
2024-06-06:13:05:25,624 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 105.21it/s] 27%|██▋       | 22/83 [00:00<00:00, 105.81it/s] 40%|███▉      | 33/83 [00:00<00:00, 106.18it/s] 53%|█████▎    | 44/83 [00:00<00:00, 106.51it/s] 66%|██████▋   | 55/83 [00:00<00:00, 106.39it/s] 80%|███████▉  | 66/83 [00:00<00:00, 106.59it/s] 93%|█████████▎| 77/83 [00:00<00:00, 106.44it/s]100%|██████████| 83/83 [00:00<00:00, 106.35it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:05:26,947 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:05:26,949 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:05:27,260 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 15%|█▍        | 12/82 [00:00<00:00, 119.20it/s] 29%|██▉       | 24/82 [00:00<00:00, 109.91it/s] 44%|████▍     | 36/82 [00:00<00:00, 107.70it/s] 57%|█████▋    | 47/82 [00:00<00:00, 108.54it/s] 71%|███████   | 58/82 [00:00<00:00, 108.79it/s] 84%|████████▍ | 69/82 [00:00<00:00, 108.21it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
100%|██████████| 82/82 [00:00<00:00, 114.50it/s]100%|██████████| 82/82 [00:00<00:00, 111.52it/s]
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:05:28,045 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:05:28,047 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:05:28,413 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 141.39it/s] 36%|███▌      | 30/83 [00:00<00:00, 141.69it/s] 54%|█████▍    | 45/83 [00:00<00:00, 141.71it/s] 72%|███████▏  | 60/83 [00:00<00:00, 141.65it/s] 90%|█████████ | 75/83 [00:00<00:00, 141.49it/s]100%|██████████| 83/83 [00:00<00:00, 141.58it/s]
2024-06-06:13:05:32,819 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:05:32,819 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:05:32,819 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:05:32,819 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:05:32,819 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:05:32,819 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:05:32,819 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:05:32,819 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:25<35:07, 25.70s/it]Running generate_until requests:   2%|▏         | 2/83 [01:00<42:06, 31.20s/it]Running generate_until requests:   4%|▎         | 3/83 [01:12<29:57, 22.47s/it]Running generate_until requests:   5%|▍         | 4/83 [01:25<24:23, 18.53s/it]Running generate_until requests:   6%|▌         | 5/83 [01:40<22:20, 17.18s/it]Running generate_until requests:   7%|▋         | 6/83 [01:51<19:20, 15.07s/it]Running generate_until requests:   8%|▊         | 7/83 [02:23<26:13, 20.70s/it]Running generate_until requests:  10%|▉         | 8/83 [02:38<23:32, 18.83s/it]Running generate_until requests:  11%|█         | 9/83 [02:58<23:48, 19.31s/it]Running generate_until requests:  12%|█▏        | 10/83 [03:10<20:47, 17.09s/it]Running generate_until requests:  13%|█▎        | 11/83 [03:29<21:13, 17.69s/it]Running generate_until requests:  14%|█▍        | 12/83 [03:53<23:15, 19.65s/it]Running generate_until requests:  16%|█▌        | 13/83 [04:10<21:51, 18.74s/it]Running generate_until requests:  17%|█▋        | 14/83 [04:29<21:46, 18.94s/it]Running generate_until requests:  18%|█▊        | 15/83 [05:08<28:19, 25.00s/it]Running generate_until requests:  19%|█▉        | 16/83 [05:37<28:57, 25.93s/it]Running generate_until requests:  20%|██        | 17/83 [05:55<25:56, 23.59s/it]Running generate_until requests:  22%|██▏       | 18/83 [06:06<21:31, 19.87s/it]Running generate_until requests:  23%|██▎       | 19/83 [06:26<21:09, 19.83s/it]Running generate_until requests:  24%|██▍       | 20/83 [06:56<24:08, 22.99s/it]Running generate_until requests:  25%|██▌       | 21/83 [07:18<23:24, 22.65s/it]Running generate_until requests:  27%|██▋       | 22/83 [07:33<20:37, 20.28s/it]Running generate_until requests:  28%|██▊       | 23/83 [08:05<24:01, 24.03s/it]Running generate_until requests:  29%|██▉       | 24/83 [08:20<20:52, 21.23s/it]Running generate_until requests:  30%|███       | 25/83 [08:47<22:03, 22.81s/it]Running generate_until requests:  31%|███▏      | 26/83 [09:01<19:20, 20.36s/it]Running generate_until requests:  33%|███▎      | 27/83 [09:40<24:11, 25.92s/it]Running generate_until requests:  34%|███▎      | 28/83 [09:52<19:53, 21.71s/it]Running generate_until requests:  35%|███▍      | 29/83 [10:08<18:01, 20.03s/it]Running generate_until requests:  36%|███▌      | 30/83 [10:32<18:39, 21.12s/it]Running generate_until requests:  37%|███▋      | 31/83 [10:58<19:40, 22.69s/it]Running generate_until requests:  39%|███▊      | 32/83 [11:19<18:51, 22.19s/it]Running generate_until requests:  40%|███▉      | 33/83 [11:31<15:54, 19.10s/it]Running generate_until requests:  41%|████      | 34/83 [12:06<19:28, 23.86s/it]Running generate_until requests:  42%|████▏     | 35/83 [12:24<17:36, 22.00s/it]Running generate_until requests:  43%|████▎     | 36/83 [12:43<16:30, 21.08s/it]Running generate_until requests:  45%|████▍     | 37/83 [13:02<15:47, 20.59s/it]Running generate_until requests:  46%|████▌     | 38/83 [13:30<16:59, 22.66s/it]Running generate_until requests:  47%|████▋     | 39/83 [13:40<13:52, 18.92s/it]Running generate_until requests:  48%|████▊     | 40/83 [13:54<12:34, 17.56s/it]Running generate_until requests:  49%|████▉     | 41/83 [14:14<12:51, 18.37s/it]Running generate_until requests:  51%|█████     | 42/83 [14:50<16:03, 23.49s/it]Running generate_until requests:  52%|█████▏    | 43/83 [15:02<13:26, 20.17s/it]Running generate_until requests:  53%|█████▎    | 44/83 [15:17<12:06, 18.63s/it]Running generate_until requests:  54%|█████▍    | 45/83 [15:28<10:17, 16.25s/it]Running generate_until requests:  55%|█████▌    | 46/83 [15:41<09:26, 15.31s/it]Running generate_until requests:  57%|█████▋    | 47/83 [15:57<09:22, 15.64s/it]Running generate_until requests:  58%|█████▊    | 48/83 [16:18<09:55, 17.00s/it]Running generate_until requests:  59%|█████▉    | 49/83 [16:27<08:22, 14.79s/it]Running generate_until requests:  60%|██████    | 50/83 [16:48<09:04, 16.51s/it]Running generate_until requests:  61%|██████▏   | 51/83 [17:06<09:00, 16.89s/it]Running generate_until requests:  63%|██████▎   | 52/83 [17:15<07:32, 14.59s/it]Running generate_until requests:  64%|██████▍   | 53/83 [17:27<06:56, 13.88s/it]Running generate_until requests:  65%|██████▌   | 54/83 [17:46<07:25, 15.36s/it]Running generate_until requests:  66%|██████▋   | 55/83 [18:03<07:28, 16.03s/it]Running generate_until requests:  67%|██████▋   | 56/83 [18:50<11:23, 25.31s/it]Running generate_until requests:  69%|██████▊   | 57/83 [19:31<12:58, 29.94s/it]Running generate_until requests:  70%|██████▉   | 58/83 [19:39<09:46, 23.45s/it]Running generate_until requests:  71%|███████   | 59/83 [19:47<07:31, 18.80s/it]Running generate_until requests:  72%|███████▏  | 60/83 [20:02<06:42, 17.52s/it]Running generate_until requests:  73%|███████▎  | 61/83 [20:17<06:08, 16.73s/it]Running generate_until requests:  75%|███████▍  | 62/83 [20:32<05:40, 16.21s/it]Running generate_until requests:  76%|███████▌  | 63/83 [20:45<05:07, 15.39s/it]Running generate_until requests:  77%|███████▋  | 64/83 [21:06<05:21, 16.91s/it]Running generate_until requests:  78%|███████▊  | 65/83 [21:16<04:30, 15.04s/it]Running generate_until requests:  80%|███████▉  | 66/83 [21:31<04:12, 14.84s/it]Running generate_until requests:  81%|████████  | 67/83 [21:42<03:37, 13.60s/it]Running generate_until requests:  82%|████████▏ | 68/83 [21:55<03:22, 13.51s/it]Running generate_until requests:  83%|████████▎ | 69/83 [22:23<04:09, 17.79s/it]Running generate_until requests:  84%|████████▍ | 70/83 [22:40<03:49, 17.68s/it]Running generate_until requests:  86%|████████▌ | 71/83 [22:47<02:53, 14.46s/it]Running generate_until requests:  87%|████████▋ | 72/83 [23:03<02:45, 15.04s/it]Running generate_until requests:  88%|████████▊ | 73/83 [23:16<02:22, 14.22s/it]Running generate_until requests:  89%|████████▉ | 74/83 [23:32<02:12, 14.76s/it]Running generate_until requests:  90%|█████████ | 75/83 [23:56<02:22, 17.76s/it]Running generate_until requests:  92%|█████████▏| 76/83 [24:14<02:04, 17.73s/it]Running generate_until requests:  93%|█████████▎| 77/83 [24:30<01:43, 17.32s/it]Running generate_until requests:  94%|█████████▍| 78/83 [24:41<01:16, 15.33s/it]Running generate_until requests:  95%|█████████▌| 79/83 [24:56<01:00, 15.09s/it]Running generate_until requests:  96%|█████████▋| 80/83 [25:08<00:42, 14.32s/it]Running generate_until requests:  98%|█████████▊| 81/83 [25:22<00:28, 14.08s/it]Running generate_until requests:  99%|█████████▉| 82/83 [25:34<00:13, 13.45s/it]Running generate_until requests: 100%|██████████| 83/83 [25:46<00:00, 13.04s/it]Running generate_until requests: 100%|██████████| 83/83 [25:46<00:00, 18.63s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-06:13:33:58,347 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:33:58,348 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:33:58,404 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:33:58,566 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:33:58,628 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:33:58,705 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:33:59,605 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:33:59,685 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:13:34:05,262 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:34:05,263 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:34:05,268 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:34:05,268 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-06:13:34:05,386 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:34:05,387 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:34:05,392 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:34:05,393 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-06:13:34:05,449 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:34:05,450 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:34:05,455 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:34:05,455 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-06:13:34:05,892 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:34:05,892 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:34:05,896 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:34:05,896 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-06:13:34:05,984 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:34:05,985 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:34:05,989 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:34:05,989 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-06:13:34:06,003 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:34:06,004 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:34:06,008 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:34:06,008 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.3}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-06:13:34:08,188 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:34:08,188 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:13:34:08,189 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:34:08,189 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:13:34:08,193 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:34:08,194 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-06:13:34:08,194 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:13:34:08,194 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.3}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.07s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.14s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.18s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.96s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.92s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.14s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.17s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.87s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.13s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.75s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.54s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.77s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.55s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.78s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.86s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.42s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.37s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.34s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.29s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.02s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.19s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:35:04,761 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:35:04,764 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:35:05,000 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 19%|█▉        | 16/83 [00:00<00:00, 150.27it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:35:05,217 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:35:05,219 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 39%|███▊      | 32/83 [00:00<00:00, 151.95it/s] 58%|█████▊    | 48/83 [00:00<00:00, 151.15it/s]2024-06-06:13:35:05,431 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
 77%|███████▋  | 64/83 [00:00<00:00, 151.04it/s]  0%|          | 0/83 [00:00<?, ?it/s] 96%|█████████▋| 80/83 [00:00<00:00, 150.99it/s] 18%|█▊        | 15/83 [00:00<00:00, 147.69it/s]100%|██████████| 83/83 [00:00<00:00, 151.04it/s]
 37%|███▋      | 31/83 [00:00<00:00, 149.76it/s] 57%|█████▋    | 47/83 [00:00<00:00, 151.07it/s] 76%|███████▌  | 63/83 [00:00<00:00, 149.79it/s] 94%|█████████▍| 78/83 [00:00<00:00, 148.97it/s]100%|██████████| 83/83 [00:00<00:00, 149.24it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:35:26,356 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:35:26,358 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:35:26,566 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:35:26,662 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:35:26,664 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 20%|█▉        | 16/82 [00:00<00:00, 152.58it/s] 39%|███▉      | 32/82 [00:00<00:00, 152.82it/s]2024-06-06:13:35:26,870 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 59%|█████▊    | 48/82 [00:00<00:00, 153.42it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 19%|█▉        | 16/83 [00:00<00:00, 150.52it/s] 78%|███████▊  | 64/82 [00:00<00:00, 153.47it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:35:27,066 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:35:27,068 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 39%|███▊      | 32/83 [00:00<00:00, 151.43it/s] 98%|█████████▊| 80/82 [00:00<00:00, 153.44it/s]2024-06-06:13:35:27,106 INFO     [xhuggingface.py:315] Using 8 devices with data parallelism
100%|██████████| 82/82 [00:00<00:00, 153.27it/s]
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:35:27,183 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:35:27,185 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 58%|█████▊    | 48/83 [00:00<00:00, 153.27it/s]2024-06-06:13:35:27,292 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
 77%|███████▋  | 64/83 [00:00<00:00, 153.03it/s]  0%|          | 0/82 [00:00<?, ?it/s]2024-06-06:13:35:27,386 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 96%|█████████▋| 80/83 [00:00<00:00, 152.81it/s] 20%|█▉        | 16/82 [00:00<00:00, 153.50it/s]100%|██████████| 83/83 [00:00<00:00, 152.59it/s]
 19%|█▉        | 16/83 [00:00<00:00, 155.86it/s] 39%|███▉      | 32/82 [00:00<00:00, 155.45it/s] 39%|███▊      | 32/83 [00:00<00:00, 156.23it/s] 59%|█████▊    | 48/82 [00:00<00:00, 156.35it/s] 58%|█████▊    | 48/83 [00:00<00:00, 157.05it/s] 78%|███████▊  | 64/82 [00:00<00:00, 156.53it/s] 77%|███████▋  | 64/83 [00:00<00:00, 157.33it/s] 98%|█████████▊| 80/82 [00:00<00:00, 156.86it/s]100%|██████████| 82/82 [00:00<00:00, 156.39it/s]
 96%|█████████▋| 80/83 [00:00<00:00, 157.76it/s]100%|██████████| 83/83 [00:00<00:00, 157.41it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:35:43,048 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:35:43,051 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:13:35:43,217 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:35:43,220 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:13:35:43,358 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 12%|█▏        | 10/82 [00:00<00:00, 98.80it/s] 24%|██▍       | 20/82 [00:00<00:00, 99.23it/s]2024-06-06:13:35:43,612 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 38%|███▊      | 31/82 [00:00<00:00, 99.99it/s] 13%|█▎        | 11/82 [00:00<00:00, 100.52it/s] 51%|█████     | 42/82 [00:00<00:00, 100.34it/s] 27%|██▋       | 22/82 [00:00<00:00, 100.68it/s] 65%|██████▍   | 53/82 [00:00<00:00, 100.48it/s] 40%|████      | 33/82 [00:00<00:00, 100.74it/s] 78%|███████▊  | 64/82 [00:00<00:00, 100.56it/s] 54%|█████▎    | 44/82 [00:00<00:00, 100.73it/s] 91%|█████████▏| 75/82 [00:00<00:00, 100.56it/s] 67%|██████▋   | 55/82 [00:00<00:00, 100.68it/s]100%|██████████| 82/82 [00:00<00:00, 100.34it/s]
 80%|████████  | 66/82 [00:00<00:00, 102.24it/s] 94%|█████████▍| 77/82 [00:00<00:00, 101.22it/s]100%|██████████| 82/82 [00:00<00:00, 100.77it/s]
2024-06-06:13:35:48,722 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:35:48,722 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:35:48,722 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:35:48,722 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:35:48,722 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:35:48,727 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:35:48,729 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:13:35:48,734 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:28<39:16, 28.74s/it]Running generate_until requests:   2%|▏         | 2/83 [01:05<44:58, 33.32s/it]Running generate_until requests:   4%|▎         | 3/83 [01:17<31:42, 23.78s/it]Running generate_until requests:   5%|▍         | 4/83 [01:30<25:39, 19.49s/it]Running generate_until requests:   6%|▌         | 5/83 [01:45<23:05, 17.76s/it]Running generate_until requests:   7%|▋         | 6/83 [01:55<19:40, 15.33s/it]Running generate_until requests:   8%|▊         | 7/83 [02:36<29:52, 23.58s/it]Running generate_until requests:  10%|▉         | 8/83 [02:51<25:59, 20.80s/it]Running generate_until requests:  11%|█         | 9/83 [03:12<25:45, 20.89s/it]Running generate_until requests:  12%|█▏        | 10/83 [03:24<22:09, 18.21s/it]Running generate_until requests:  13%|█▎        | 11/83 [03:45<22:41, 18.90s/it]Running generate_until requests:  14%|█▍        | 12/83 [04:09<24:14, 20.49s/it]Running generate_until requests:  16%|█▌        | 13/83 [04:25<22:30, 19.30s/it]Running generate_until requests:  17%|█▋        | 14/83 [04:46<22:35, 19.64s/it]Running generate_until requests:  18%|█▊        | 15/83 [05:13<24:53, 21.97s/it]Running generate_until requests:  19%|█▉        | 16/83 [05:56<31:39, 28.35s/it]Running generate_until requests:  20%|██        | 17/83 [06:16<28:29, 25.90s/it]Running generate_until requests:  22%|██▏       | 18/83 [06:28<23:19, 21.54s/it]Running generate_until requests:  23%|██▎       | 19/83 [06:50<23:10, 21.73s/it]Running generate_until requests:  24%|██▍       | 20/83 [07:22<26:09, 24.92s/it]Running generate_until requests:  25%|██▌       | 21/83 [07:45<24:54, 24.10s/it]Running generate_until requests:  27%|██▋       | 22/83 [07:59<21:31, 21.17s/it]Running generate_until requests:  28%|██▊       | 23/83 [08:40<27:15, 27.25s/it]Running generate_until requests:  29%|██▉       | 24/83 [08:55<23:09, 23.55s/it]Running generate_until requests:  30%|███       | 25/83 [09:14<21:21, 22.09s/it]Running generate_until requests:  31%|███▏      | 26/83 [09:29<18:52, 19.88s/it]Running generate_until requests:  33%|███▎      | 27/83 [09:57<20:49, 22.32s/it]Running generate_until requests:  34%|███▎      | 28/83 [10:12<18:30, 20.20s/it]Running generate_until requests:  35%|███▍      | 29/83 [10:35<19:02, 21.15s/it]Running generate_until requests:  36%|███▌      | 30/83 [10:59<19:20, 21.89s/it]Running generate_until requests:  37%|███▋      | 31/83 [11:42<24:27, 28.22s/it]Running generate_until requests:  39%|███▊      | 32/83 [12:08<23:22, 27.49s/it]Running generate_until requests:  40%|███▉      | 33/83 [12:21<19:26, 23.33s/it]Running generate_until requests:  41%|████      | 34/83 [12:57<22:10, 27.16s/it]Running generate_until requests:  42%|████▏     | 35/83 [13:17<19:49, 24.78s/it]Running generate_until requests:  43%|████▎     | 36/83 [13:43<19:41, 25.13s/it]Running generate_until requests:  45%|████▍     | 37/83 [14:03<18:15, 23.81s/it]Running generate_until requests:  46%|████▌     | 38/83 [14:32<18:53, 25.18s/it]Running generate_until requests:  47%|████▋     | 39/83 [14:42<15:11, 20.71s/it]Running generate_until requests:  48%|████▊     | 40/83 [15:07<15:42, 21.91s/it]Running generate_until requests:  49%|████▉     | 41/83 [15:27<15:00, 21.44s/it]Running generate_until requests:  51%|█████     | 42/83 [15:53<15:37, 22.87s/it]Running generate_until requests:  52%|█████▏    | 43/83 [16:10<13:56, 20.90s/it]Running generate_until requests:  53%|█████▎    | 44/83 [16:26<12:40, 19.51s/it]Running generate_until requests:  54%|█████▍    | 45/83 [16:39<11:12, 17.70s/it]Running generate_until requests:  55%|█████▌    | 46/83 [16:53<10:15, 16.65s/it]Running generate_until requests:  57%|█████▋    | 47/83 [17:13<10:31, 17.54s/it]Running generate_until requests:  58%|█████▊    | 48/83 [17:33<10:40, 18.30s/it]Running generate_until requests:  59%|█████▉    | 49/83 [17:46<09:24, 16.61s/it]Running generate_until requests:  60%|██████    | 50/83 [18:09<10:17, 18.71s/it]Running generate_until requests:  61%|██████▏   | 51/83 [18:37<11:21, 21.31s/it]Running generate_until requests:  63%|██████▎   | 52/83 [18:53<10:12, 19.75s/it]Running generate_until requests:  64%|██████▍   | 53/83 [19:12<09:45, 19.51s/it]Running generate_until requests:  65%|██████▌   | 54/83 [19:32<09:34, 19.81s/it]Running generate_until requests:  66%|██████▋   | 55/83 [19:50<08:57, 19.21s/it]Running generate_until requests:  67%|██████▋   | 56/83 [20:33<11:50, 26.32s/it]Running generate_until requests:  69%|██████▊   | 57/83 [21:21<14:13, 32.83s/it]Running generate_until requests:  70%|██████▉   | 58/83 [21:30<10:39, 25.58s/it]Running generate_until requests:  71%|███████   | 59/83 [21:39<08:19, 20.81s/it]Running generate_until requests:  72%|███████▏  | 60/83 [21:58<07:45, 20.25s/it]Running generate_until requests:  73%|███████▎  | 61/83 [22:22<07:45, 21.15s/it]Running generate_until requests:  75%|███████▍  | 62/83 [22:36<06:43, 19.19s/it]Running generate_until requests:  76%|███████▌  | 63/83 [22:47<05:31, 16.56s/it]Running generate_until requests:  77%|███████▋  | 64/83 [23:02<05:10, 16.32s/it]Running generate_until requests:  78%|███████▊  | 65/83 [23:11<04:10, 13.91s/it]Running generate_until requests:  80%|███████▉  | 66/83 [23:19<03:25, 12.11s/it]Running generate_until requests:  81%|████████  | 67/83 [23:31<03:14, 12.15s/it]Running generate_until requests:  82%|████████▏ | 68/83 [23:44<03:06, 12.42s/it]Running generate_until requests:  83%|████████▎ | 69/83 [24:15<04:11, 17.99s/it]Running generate_until requests:  84%|████████▍ | 70/83 [24:30<03:42, 17.08s/it]Running generate_until requests:  86%|████████▌ | 71/83 [24:37<02:49, 14.09s/it]Running generate_until requests:  87%|████████▋ | 72/83 [24:57<02:54, 15.88s/it]Running generate_until requests:  88%|████████▊ | 73/83 [25:10<02:29, 14.90s/it]Running generate_until requests:  89%|████████▉ | 74/83 [25:25<02:14, 14.92s/it]Running generate_until requests:  90%|█████████ | 75/83 [25:55<02:36, 19.59s/it]Running generate_until requests:  92%|█████████▏| 76/83 [26:05<01:57, 16.76s/it]Running generate_until requests:  93%|█████████▎| 77/83 [26:23<01:42, 17.02s/it]Running generate_until requests:  94%|█████████▍| 78/83 [26:35<01:18, 15.61s/it]Running generate_until requests:  95%|█████████▌| 79/83 [26:51<01:03, 15.76s/it]Running generate_until requests:  96%|█████████▋| 80/83 [27:04<00:44, 14.96s/it]Running generate_until requests:  98%|█████████▊| 81/83 [27:19<00:29, 14.85s/it]Running generate_until requests:  99%|█████████▉| 82/83 [27:27<00:12, 12.87s/it]Running generate_until requests: 100%|██████████| 83/83 [27:35<00:00, 11.46s/it]Running generate_until requests: 100%|██████████| 83/83 [27:35<00:00, 19.95s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-06:14:08:22,756 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:08:22,758 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:08:22,758 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:08:22,823 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:08:22,885 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:08:22,893 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:08:22,922 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:08:23,220 INFO     [main.py:288] Verbosity set to INFO
2024-06-06:14:08:29,668 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:08:29,669 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:08:29,671 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:08:29,672 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:08:29,673 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:08:29,673 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-06:14:08:29,676 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:08:29,676 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-06:14:08:29,999 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:08:30,000 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:08:30,004 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:08:30,004 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-06:14:08:30,060 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:08:30,061 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:08:30,065 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:08:30,065 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-06:14:08:30,096 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:08:30,097 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:08:30,102 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:08:30,102 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-06:14:08:30,140 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:08:30,141 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:08:30,144 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:08:30,144 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-06:14:08:30,820 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:08:30,821 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:08:30,826 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:08:30,826 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-06:14:08:31,259 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-06:14:08:31,260 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-06:14:08:31,264 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-06:14:08:31,264 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.4}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.22s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.17s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.91s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.92s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.92s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.96s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.18s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:05,  2.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.28s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.12s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.74s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.80s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.08s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.21s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.20s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.21s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.94s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.38s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.42s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:09:31,991 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:09:31,993 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:09:32,321 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 107.78it/s] 28%|██▊       | 23/82 [00:00<00:00, 109.46it/s] 41%|████▏     | 34/82 [00:00<00:00, 109.60it/s] 55%|█████▍    | 45/82 [00:00<00:00, 109.52it/s] 68%|██████▊   | 56/82 [00:00<00:00, 109.69it/s] 83%|████████▎ | 68/82 [00:00<00:00, 109.89it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:09:33,041 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:09:33,043 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 98%|█████████▊| 80/82 [00:00<00:00, 110.27it/s]100%|██████████| 82/82 [00:00<00:00, 109.84it/s]
2024-06-06:14:09:33,251 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 20%|█▉        | 16/82 [00:00<00:00, 151.37it/s] 39%|███▉      | 32/82 [00:00<00:00, 152.19it/s] 59%|█████▊    | 48/82 [00:00<00:00, 152.59it/s] 78%|███████▊  | 64/82 [00:00<00:00, 152.60it/s] 98%|█████████▊| 80/82 [00:00<00:00, 152.23it/s]100%|██████████| 82/82 [00:00<00:00, 152.19it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:09:49,163 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:09:49,165 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:09:49,469 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 14%|█▍        | 12/83 [00:00<00:00, 113.14it/s] 31%|███▏      | 26/83 [00:00<00:00, 124.08it/s] 51%|█████     | 42/83 [00:00<00:00, 137.75it/s] 70%|██████▉   | 58/83 [00:00<00:00, 144.55it/s] 89%|████████▉ | 74/83 [00:00<00:00, 148.27it/s]100%|██████████| 83/83 [00:00<00:00, 143.10it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:09:50,609 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:09:50,611 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:09:50,719 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:09:50,721 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:09:50,817 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-06:14:09:50,927 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
 20%|█▉        | 16/82 [00:00<00:00, 154.11it/s]  0%|          | 0/83 [00:00<?, ?it/s] 39%|███▉      | 32/82 [00:00<00:00, 153.58it/s] 19%|█▉        | 16/83 [00:00<00:00, 152.35it/s] 59%|█████▊    | 48/82 [00:00<00:00, 153.35it/s] 39%|███▊      | 32/83 [00:00<00:00, 152.76it/s] 78%|███████▊  | 64/82 [00:00<00:00, 153.38it/s] 58%|█████▊    | 48/83 [00:00<00:00, 153.14it/s] 98%|█████████▊| 80/82 [00:00<00:00, 153.34it/s] 77%|███████▋  | 64/83 [00:00<00:00, 153.16it/s]100%|██████████| 82/82 [00:00<00:00, 153.39it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:09:51,447 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:09:51,449 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 96%|█████████▋| 80/83 [00:00<00:00, 154.21it/s]100%|██████████| 83/83 [00:00<00:00, 153.56it/s]
2024-06-06:14:09:51,650 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 19%|█▉        | 16/83 [00:00<00:00, 158.19it/s] 39%|███▊      | 32/83 [00:00<00:00, 158.44it/s] 58%|█████▊    | 48/83 [00:00<00:00, 158.79it/s] 77%|███████▋  | 64/83 [00:00<00:00, 158.80it/s] 96%|█████████▋| 80/83 [00:00<00:00, 158.96it/s]100%|██████████| 83/83 [00:00<00:00, 158.79it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-06:14:10:01,501 INFO     [xhuggingface.py:315] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:10:01,670 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:10:01,672 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:10:02,001 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 109.10it/s] 28%|██▊       | 23/83 [00:00<00:00, 110.43it/s] 42%|████▏     | 35/83 [00:00<00:00, 107.07it/s] 55%|█████▌    | 46/83 [00:00<00:00, 107.60it/s] 69%|██████▊   | 57/83 [00:00<00:00, 108.30it/s] 82%|████████▏ | 68/83 [00:00<00:00, 108.34it/s] 96%|█████████▋| 80/83 [00:00<00:00, 109.33it/s]100%|██████████| 83/83 [00:00<00:00, 108.79it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-06:14:10:05,280 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:10:05,282 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-06:14:10:05,678 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 12%|█▏        | 10/82 [00:00<00:00, 93.20it/s] 24%|██▍       | 20/82 [00:00<00:00, 92.34it/s] 37%|███▋      | 30/82 [00:00<00:00, 90.15it/s] 49%|████▉     | 40/82 [00:00<00:00, 89.01it/s] 60%|█████▉    | 49/82 [00:00<00:00, 88.43it/s] 71%|███████   | 58/82 [00:00<00:00, 87.09it/s] 82%|████████▏ | 67/82 [00:00<00:00, 87.12it/s] 93%|█████████▎| 76/82 [00:00<00:00, 87.12it/s]100%|██████████| 82/82 [00:00<00:00, 88.23it/s]
2024-06-06:14:10:09,790 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:14:10:09,790 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:14:10:09,790 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:14:10:09,790 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:14:10:09,791 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:14:10:09,793 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-06:14:10:09,798 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-06:14:10:09,805 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:30<41:59, 30.73s/it]Running generate_until requests:   2%|▏         | 2/83 [01:06<45:28, 33.68s/it]Running generate_until requests:   4%|▎         | 3/83 [01:18<31:58, 23.98s/it]Running generate_until requests:   5%|▍         | 4/83 [01:33<26:29, 20.11s/it]Running generate_until requests:   6%|▌         | 5/83 [01:48<23:52, 18.37s/it]Running generate_until requests:   7%|▋         | 6/83 [02:03<22:20, 17.40s/it]Running generate_until requests:   8%|▊         | 7/83 [02:43<31:06, 24.56s/it]Running generate_until requests:  10%|▉         | 8/83 [02:59<27:26, 21.95s/it]Running generate_until requests:  11%|█         | 9/83 [03:20<26:46, 21.71s/it]Running generate_until requests:  12%|█▏        | 10/83 [03:34<23:20, 19.19s/it]Running generate_until requests:  13%|█▎        | 11/83 [03:57<24:29, 20.41s/it]Running generate_until requests:  14%|█▍        | 12/83 [04:20<25:12, 21.30s/it]Running generate_until requests:  16%|█▌        | 13/83 [04:37<23:07, 19.82s/it]Running generate_until requests:  17%|█▋        | 14/83 [04:59<23:46, 20.67s/it]Running generate_until requests:  18%|█▊        | 15/83 [05:49<33:19, 29.41s/it]Running generate_until requests:  19%|█▉        | 16/83 [06:32<37:24, 33.49s/it]Running generate_until requests:  20%|██        | 17/83 [06:53<32:33, 29.59s/it]Running generate_until requests:  22%|██▏       | 18/83 [07:07<27:01, 24.95s/it]Running generate_until requests:  23%|██▎       | 19/83 [07:33<27:05, 25.40s/it]Running generate_until requests:  24%|██▍       | 20/83 [08:05<28:49, 27.45s/it]Running generate_until requests:  25%|██▌       | 21/83 [08:31<27:49, 26.94s/it]Running generate_until requests:  27%|██▋       | 22/83 [08:47<24:04, 23.68s/it]Running generate_until requests:  28%|██▊       | 23/83 [09:13<24:27, 24.45s/it]Running generate_until requests:  29%|██▉       | 24/83 [09:28<21:11, 21.54s/it]Running generate_until requests:  30%|███       | 25/83 [09:54<22:10, 22.94s/it]Running generate_until requests:  31%|███▏      | 26/83 [10:07<18:48, 19.79s/it]Running generate_until requests:  33%|███▎      | 27/83 [10:37<21:14, 22.76s/it]Running generate_until requests:  34%|███▎      | 28/83 [10:52<18:44, 20.45s/it]Running generate_until requests:  35%|███▍      | 29/83 [11:15<19:17, 21.44s/it]Running generate_until requests:  36%|███▌      | 30/83 [11:39<19:25, 21.98s/it]Running generate_until requests:  37%|███▋      | 31/83 [12:14<22:39, 26.14s/it]Running generate_until requests:  39%|███▊      | 32/83 [12:35<20:53, 24.58s/it]Running generate_until requests:  40%|███▉      | 33/83 [12:49<17:47, 21.35s/it]Running generate_until requests:  41%|████      | 34/83 [13:27<21:29, 26.31s/it]Running generate_until requests:  42%|████▏     | 35/83 [13:49<20:05, 25.12s/it]Running generate_until requests:  43%|████▎     | 36/83 [14:27<22:36, 28.87s/it]Running generate_until requests:  45%|████▍     | 37/83 [14:44<19:30, 25.44s/it]Running generate_until requests:  46%|████▌     | 38/83 [15:14<19:57, 26.62s/it]Running generate_until requests:  47%|████▋     | 39/83 [15:28<16:42, 22.79s/it]Running generate_until requests:  48%|████▊     | 40/83 [15:54<17:01, 23.76s/it]Running generate_until requests:  49%|████▉     | 41/83 [16:06<14:11, 20.28s/it]Running generate_until requests:  51%|█████     | 42/83 [16:36<15:58, 23.38s/it]Running generate_until requests:  52%|█████▏    | 43/83 [16:51<13:53, 20.83s/it]Running generate_until requests:  53%|█████▎    | 44/83 [17:08<12:43, 19.57s/it]Running generate_until requests:  54%|█████▍    | 45/83 [17:22<11:18, 17.86s/it]Running generate_until requests:  55%|█████▌    | 46/83 [17:39<10:48, 17.53s/it]Running generate_until requests:  57%|█████▋    | 47/83 [17:59<10:56, 18.24s/it]Running generate_until requests:  58%|█████▊    | 48/83 [18:20<11:07, 19.08s/it]Running generate_until requests:  59%|█████▉    | 49/83 [18:33<09:46, 17.26s/it]Running generate_until requests:  60%|██████    | 50/83 [18:57<10:36, 19.28s/it]Running generate_until requests:  61%|██████▏   | 51/83 [19:20<10:55, 20.47s/it]Running generate_until requests:  63%|██████▎   | 52/83 [19:33<09:22, 18.13s/it]Running generate_until requests:  64%|██████▍   | 53/83 [19:49<08:49, 17.67s/it]Running generate_until requests:  65%|██████▌   | 54/83 [20:10<09:03, 18.73s/it]Running generate_until requests:  66%|██████▋   | 55/83 [20:24<08:05, 17.34s/it]Running generate_until requests:  67%|██████▋   | 56/83 [21:13<12:00, 26.69s/it]Running generate_until requests:  69%|██████▊   | 57/83 [22:04<14:43, 33.98s/it]Running generate_until requests:  70%|██████▉   | 58/83 [22:16<11:25, 27.43s/it]Running generate_until requests:  71%|███████   | 59/83 [22:26<08:53, 22.22s/it]Running generate_until requests:  72%|███████▏  | 60/83 [22:45<08:07, 21.18s/it]Running generate_until requests:  73%|███████▎  | 61/83 [23:12<08:23, 22.89s/it]Running generate_until requests:  75%|███████▍  | 62/83 [23:24<06:52, 19.62s/it]Running generate_until requests:  76%|███████▌  | 63/83 [23:34<05:38, 16.90s/it]Running generate_until requests:  77%|███████▋  | 64/83 [24:04<06:35, 20.84s/it]Running generate_until requests:  78%|███████▊  | 65/83 [24:13<05:07, 17.10s/it]Running generate_until requests:  80%|███████▉  | 66/83 [24:20<03:58, 14.05s/it]Running generate_until requests:  81%|████████  | 67/83 [24:30<03:26, 12.93s/it]Running generate_until requests:  82%|████████▏ | 68/83 [24:43<03:12, 12.82s/it]Running generate_until requests:  83%|████████▎ | 69/83 [25:16<04:25, 18.93s/it]Running generate_until requests:  84%|████████▍ | 70/83 [25:30<03:49, 17.65s/it]Running generate_until requests:  86%|████████▌ | 71/83 [25:36<02:49, 14.11s/it]Running generate_until requests:  87%|████████▋ | 72/83 [25:54<02:47, 15.26s/it]Running generate_until requests:  88%|████████▊ | 73/83 [26:14<02:46, 16.66s/it]Running generate_until requests:  89%|████████▉ | 74/83 [26:29<02:25, 16.15s/it]Running generate_until requests:  90%|█████████ | 75/83 [27:01<02:46, 20.82s/it]Running generate_until requests:  92%|█████████▏| 76/83 [27:24<02:29, 21.40s/it]Running generate_until requests:  93%|█████████▎| 77/83 [27:38<01:56, 19.40s/it]Running generate_until requests:  94%|█████████▍| 78/83 [27:49<01:23, 16.74s/it]Running generate_until requests:  95%|█████████▌| 79/83 [28:03<01:04, 16.12s/it]Running generate_until requests:  96%|█████████▋| 80/83 [28:14<00:43, 14.34s/it]Running generate_until requests:  98%|█████████▊| 81/83 [28:29<00:29, 14.76s/it]Running generate_until requests:  99%|█████████▉| 82/83 [28:38<00:12, 12.84s/it]Running generate_until requests: 100%|██████████| 83/83 [28:46<00:00, 11.45s/it]Running generate_until requests: 100%|██████████| 83/83 [28:46<00:00, 20.80s/it]
