Already on 'yangexp2'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/huggingface-cli", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py", line 51, in main
    service.run()
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/user.py", line 98, in run
    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 111, in login
    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 307, in _login
    raise ValueError("Invalid token passed!")
ValueError: Invalid token passed!
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-03:11:34:58,787 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:11:34:58,840 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:11:34:59,416 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:11:34:59,501 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:11:34:59,535 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:11:35:00,000 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:11:35:00,647 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:11:35:01,718 INFO     [main.py:288] Verbosity set to INFO
2024-06-03:11:35:04,247 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:11:35:04,254 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:11:35:04,254 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-06-03:11:35:05,062 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:11:35:05,067 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:11:35:05,067 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-06-03:11:35:05,749 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:11:35:05,755 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:11:35:05,755 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
2024-06-03:11:35:05,757 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:11:35:05,762 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:11:35:05,762 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-03:11:35:06,515 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:11:35:06,519 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:11:35:06,519 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.24s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.53s/it]2024-06-03:11:35:09,573 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:11:35:09,578 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:11:35:09,578 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  2.00s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.79s/it]2024-06-03:11:35:11,100 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:11:35:11,106 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:11:35:11,106 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.91s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.95s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.61s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.60s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.29s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
2024-06-03:11:35:15,011 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-03:11:35:15,018 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-03:11:35:15,018 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'check': True, 'kernel_size': 16, 'spr': 0.5, 'thr': 0.1}
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.09s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.03s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.19s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.57s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.15s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.73s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:17,  5.88s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.40s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.27s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.80s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-03:11:35:59,139 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-03:11:35:59,142 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-03:11:35:59,338 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/165 [00:00<?, ?it/s] 12%|█▏        | 20/165 [00:00<00:00, 196.04it/s] 24%|██▍       | 40/165 [00:00<00:00, 196.23it/s] 36%|███▋      | 60/165 [00:00<00:00, 196.78it/s] 48%|████▊     | 80/165 [00:00<00:00, 196.91it/s] 61%|██████    | 100/165 [00:00<00:00, 197.28it/s] 73%|███████▎  | 120/165 [00:00<00:00, 197.43it/s] 85%|████████▍ | 140/165 [00:00<00:00, 197.37it/s] 97%|█████████▋| 160/165 [00:00<00:00, 197.18it/s]100%|██████████| 165/165 [00:00<00:00, 197.07it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-03:11:36:18,179 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-03:11:36:18,181 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-03:11:36:18,483 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/165 [00:00<?, ?it/s]  9%|▉         | 15/165 [00:00<00:01, 143.00it/s] 18%|█▊        | 30/165 [00:00<00:00, 142.99it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 27%|██▋       | 45/165 [00:00<00:00, 142.79it/s] 39%|███▉      | 65/165 [00:00<00:00, 163.97it/s] 52%|█████▏    | 85/165 [00:00<00:00, 176.09it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-03:11:36:19,050 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-03:11:36:19,052 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 62%|██████▏   | 103/165 [00:00<00:00, 159.62it/s] 73%|███████▎  | 120/165 [00:00<00:00, 147.99it/s]2024-06-03:11:36:19,367 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/165 [00:00<?, ?it/s] 82%|████████▏ | 136/165 [00:00<00:00, 141.64it/s]  8%|▊         | 14/165 [00:00<00:01, 131.79it/s] 92%|█████████▏| 151/165 [00:01<00:00, 138.62it/s] 17%|█▋        | 28/165 [00:00<00:01, 131.70it/s]100%|██████████| 165/165 [00:01<00:00, 146.25it/s]
 28%|██▊       | 47/165 [00:00<00:00, 155.33it/s] 41%|████      | 68/165 [00:00<00:00, 173.01it/s] 54%|█████▍    | 89/165 [00:00<00:00, 183.18it/s] 67%|██████▋   | 110/165 [00:00<00:00, 189.35it/s] 79%|███████▉  | 131/165 [00:00<00:00, 193.06it/s] 92%|█████████▏| 152/165 [00:00<00:00, 195.68it/s]100%|██████████| 165/165 [00:00<00:00, 182.78it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-03:11:36:27,501 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-03:11:36:27,503 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-03:11:36:27,707 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/165 [00:00<?, ?it/s] 13%|█▎        | 21/165 [00:00<00:00, 204.88it/s] 25%|██▌       | 42/165 [00:00<00:00, 204.72it/s] 38%|███▊      | 63/165 [00:00<00:00, 204.57it/s] 51%|█████     | 84/165 [00:00<00:00, 204.85it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 64%|██████▎   | 105/165 [00:00<00:00, 204.70it/s] 76%|███████▋  | 126/165 [00:00<00:00, 205.59it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-03:11:36:28,422 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-03:11:36:28,424 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 89%|████████▉ | 147/165 [00:00<00:00, 206.28it/s]100%|██████████| 165/165 [00:00<00:00, 205.28it/s]
2024-06-03:11:36:28,586 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/165 [00:00<?, ?it/s] 13%|█▎        | 21/165 [00:00<00:00, 206.84it/s] 25%|██▌       | 42/165 [00:00<00:00, 208.16it/s] 38%|███▊      | 63/165 [00:00<00:00, 208.09it/s] 51%|█████     | 84/165 [00:00<00:00, 208.34it/s] 64%|██████▎   | 105/165 [00:00<00:00, 208.54it/s] 76%|███████▋  | 126/165 [00:00<00:00, 208.64it/s] 89%|████████▉ | 147/165 [00:00<00:00, 208.75it/s]100%|██████████| 165/165 [00:00<00:00, 207.96it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-03:11:36:40,947 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-03:11:36:40,952 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-03:11:36:41,616 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/164 [00:00<?, ?it/s]  4%|▎         | 6/164 [00:00<00:03, 50.70it/s]  7%|▋         | 12/164 [00:00<00:02, 51.09it/s] 11%|█         | 18/164 [00:00<00:02, 51.37it/s] 15%|█▍        | 24/164 [00:00<00:02, 51.61it/s] 18%|█▊        | 30/164 [00:00<00:02, 51.54it/s] 22%|██▏       | 36/164 [00:00<00:02, 51.62it/s] 26%|██▌       | 42/164 [00:00<00:02, 51.69it/s] 29%|██▉       | 48/164 [00:00<00:02, 51.64it/s] 33%|███▎      | 54/164 [00:01<00:02, 51.74it/s] 37%|███▋      | 60/164 [00:01<00:02, 51.88it/s] 40%|████      | 66/164 [00:01<00:01, 51.96it/s] 44%|████▍     | 72/164 [00:01<00:01, 51.92it/s] 48%|████▊     | 78/164 [00:01<00:01, 51.14it/s] 51%|█████     | 84/164 [00:01<00:01, 52.49it/s] 55%|█████▍    | 90/164 [00:01<00:01, 53.52it/s] 59%|█████▊    | 96/164 [00:01<00:01, 54.20it/s] 62%|██████▏   | 102/164 [00:01<00:01, 54.71it/s] 66%|██████▌   | 108/164 [00:02<00:01, 55.06it/s] 70%|██████▉   | 114/164 [00:02<00:00, 54.09it/s] 73%|███████▎  | 120/164 [00:02<00:00, 53.25it/s] 77%|███████▋  | 126/164 [00:02<00:00, 52.74it/s] 80%|████████  | 132/164 [00:02<00:00, 52.38it/s] 84%|████████▍ | 138/164 [00:02<00:00, 52.06it/s] 88%|████████▊ | 144/164 [00:02<00:00, 51.85it/s] 91%|█████████▏| 150/164 [00:02<00:00, 51.72it/s] 95%|█████████▌| 156/164 [00:02<00:00, 51.69it/s] 99%|█████████▉| 162/164 [00:03<00:00, 51.62it/s]100%|██████████| 164/164 [00:03<00:00, 52.26it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-03:11:37:15,690 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-03:11:37:15,692 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-03:11:37:16,074 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/165 [00:00<?, ?it/s]  8%|▊         | 13/165 [00:00<00:01, 123.49it/s] 16%|█▌        | 26/165 [00:00<00:01, 124.06it/s] 24%|██▎       | 39/165 [00:00<00:01, 124.26it/s] 32%|███▏      | 52/165 [00:00<00:00, 124.41it/s] 39%|███▉      | 65/165 [00:00<00:00, 124.39it/s] 47%|████▋     | 78/165 [00:00<00:00, 124.42it/s] 55%|█████▌    | 91/165 [00:00<00:00, 124.39it/s] 63%|██████▎   | 104/165 [00:00<00:00, 124.53it/s] 71%|███████   | 117/165 [00:00<00:00, 124.38it/s] 79%|███████▉  | 130/165 [00:01<00:00, 124.46it/s] 87%|████████▋ | 143/165 [00:01<00:00, 124.50it/s] 95%|█████████▍| 156/165 [00:01<00:00, 124.57it/s]100%|██████████| 165/165 [00:01<00:00, 124.40it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-03:11:37:39,679 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-03:11:37:39,930 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-03:11:37:39,934 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-03:11:37:40,310 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/165 [00:00<?, ?it/s]  8%|▊         | 13/165 [00:00<00:01, 126.05it/s] 16%|█▌        | 26/165 [00:00<00:01, 126.30it/s] 24%|██▎       | 39/165 [00:00<00:00, 126.27it/s] 32%|███▏      | 52/165 [00:00<00:00, 126.56it/s] 39%|███▉      | 65/165 [00:00<00:00, 126.43it/s] 47%|████▋     | 78/165 [00:00<00:00, 126.39it/s] 55%|█████▌    | 91/165 [00:00<00:00, 126.35it/s] 63%|██████▎   | 104/165 [00:00<00:00, 126.42it/s] 71%|███████   | 117/165 [00:00<00:00, 126.23it/s] 79%|███████▉  | 130/165 [00:01<00:00, 126.32it/s] 87%|████████▋ | 143/165 [00:01<00:00, 126.17it/s] 95%|█████████▍| 156/165 [00:01<00:00, 126.18it/s]100%|██████████| 165/165 [00:01<00:00, 126.26it/s]
2024-06-03:11:37:52,131 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-03:11:37:52,131 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-03:11:37:52,131 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-03:11:37:52,131 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-03:11:37:52,131 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-03:11:37:52,132 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-03:11:37:52,132 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-03:11:37:52,132 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/165 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/165 [00:26<1:12:17, 26.45s/it]Running generate_until requests:   1%|          | 2/165 [00:39<50:15, 18.50s/it]  Running generate_until requests:   2%|▏         | 3/165 [00:58<51:07, 18.94s/it]Running generate_until requests:   2%|▏         | 4/165 [01:21<54:47, 20.42s/it]Running generate_until requests:   3%|▎         | 5/165 [01:31<44:00, 16.50s/it]Running generate_until requests:   4%|▎         | 6/165 [01:51<47:30, 17.93s/it]Running generate_until requests:   4%|▍         | 7/165 [01:57<36:54, 14.02s/it]Running generate_until requests:   5%|▍         | 8/165 [02:07<33:23, 12.76s/it]Running generate_until requests:   5%|▌         | 9/165 [02:14<28:07, 10.82s/it]Running generate_until requests:   6%|▌         | 10/165 [02:25<28:08, 10.89s/it]Running generate_until requests:   7%|▋         | 11/165 [02:36<27:50, 10.85s/it]Running generate_until requests:   7%|▋         | 12/165 [02:42<24:19,  9.54s/it]Running generate_until requests:   8%|▊         | 13/165 [02:54<25:45, 10.17s/it]Running generate_until requests:   8%|▊         | 14/165 [03:03<24:31,  9.74s/it]Running generate_until requests:   9%|▉         | 15/165 [03:16<26:52, 10.75s/it]Running generate_until requests:  10%|▉         | 16/165 [03:25<25:29, 10.27s/it]Running generate_until requests:  10%|█         | 17/165 [03:36<26:14, 10.64s/it]Running generate_until requests:  11%|█         | 18/165 [03:51<28:52, 11.79s/it]Running generate_until requests:  12%|█▏        | 19/165 [03:59<25:50, 10.62s/it]Running generate_until requests:  12%|█▏        | 20/165 [04:07<23:46,  9.84s/it]Running generate_until requests:  13%|█▎        | 21/165 [04:25<29:54, 12.46s/it]Running generate_until requests:  13%|█▎        | 22/165 [04:39<30:42, 12.89s/it]Running generate_until requests:  14%|█▍        | 23/165 [04:46<25:58, 10.98s/it]Running generate_until requests:  15%|█▍        | 24/165 [04:54<23:58, 10.20s/it]Running generate_until requests:  15%|█▌        | 25/165 [05:06<24:52, 10.66s/it]Running generate_until requests:  16%|█▌        | 26/165 [05:16<24:37, 10.63s/it]Running generate_until requests:  16%|█▋        | 27/165 [05:34<29:36, 12.87s/it]Running generate_until requests:  17%|█▋        | 28/165 [05:42<25:34, 11.20s/it]Running generate_until requests:  18%|█▊        | 29/165 [05:57<28:18, 12.49s/it]Running generate_until requests:  18%|█▊        | 30/165 [06:09<27:28, 12.21s/it]Running generate_until requests:  19%|█▉        | 31/165 [06:22<27:43, 12.41s/it]Running generate_until requests:  19%|█▉        | 32/165 [06:29<23:47, 10.73s/it]Running generate_until requests:  20%|██        | 33/165 [06:37<21:48,  9.91s/it]Running generate_until requests:  21%|██        | 34/165 [06:48<22:56, 10.51s/it]Running generate_until requests:  21%|██        | 35/165 [07:05<26:49, 12.38s/it]Running generate_until requests:  22%|██▏       | 36/165 [07:20<27:58, 13.01s/it]Running generate_until requests:  22%|██▏       | 37/165 [07:29<25:27, 11.94s/it]Running generate_until requests:  23%|██▎       | 38/165 [07:41<25:19, 11.97s/it]Running generate_until requests:  24%|██▎       | 39/165 [08:04<31:41, 15.09s/it]Running generate_until requests:  24%|██▍       | 40/165 [08:14<28:39, 13.76s/it]Running generate_until requests:  25%|██▍       | 41/165 [08:23<25:28, 12.33s/it]Running generate_until requests:  25%|██▌       | 42/165 [08:31<22:44, 11.10s/it]Running generate_until requests:  26%|██▌       | 43/165 [08:45<24:22, 11.99s/it]Running generate_until requests:  27%|██▋       | 44/165 [08:59<24:51, 12.33s/it]Running generate_until requests:  27%|██▋       | 45/165 [09:08<23:07, 11.57s/it]Running generate_until requests:  28%|██▊       | 46/165 [09:15<20:16, 10.22s/it]Running generate_until requests:  28%|██▊       | 47/165 [09:23<18:47,  9.55s/it]Running generate_until requests:  29%|██▉       | 48/165 [09:38<21:45, 11.16s/it]Running generate_until requests:  30%|██▉       | 49/165 [09:52<23:06, 11.95s/it]Running generate_until requests:  30%|███       | 50/165 [10:01<21:05, 11.00s/it]Running generate_until requests:  31%|███       | 51/165 [10:20<25:28, 13.41s/it]Running generate_until requests:  32%|███▏      | 52/165 [10:23<19:37, 10.42s/it]Running generate_until requests:  32%|███▏      | 53/165 [10:36<20:37, 11.05s/it]Running generate_until requests:  33%|███▎      | 54/165 [10:48<21:00, 11.35s/it]Running generate_until requests:  33%|███▎      | 55/165 [10:57<19:16, 10.51s/it]Running generate_until requests:  34%|███▍      | 56/165 [11:04<17:37,  9.70s/it]Running generate_until requests:  35%|███▍      | 57/165 [11:18<19:20, 10.74s/it]Running generate_until requests:  35%|███▌      | 58/165 [11:26<17:41,  9.92s/it]Running generate_until requests:  36%|███▌      | 59/165 [11:34<16:57,  9.60s/it]Running generate_until requests:  36%|███▋      | 60/165 [11:42<15:33,  8.89s/it]Running generate_until requests:  37%|███▋      | 61/165 [11:54<17:22, 10.02s/it]Running generate_until requests:  38%|███▊      | 62/165 [12:03<16:26,  9.58s/it]Running generate_until requests:  38%|███▊      | 63/165 [12:12<15:51,  9.32s/it]Running generate_until requests:  39%|███▉      | 64/165 [12:18<14:18,  8.50s/it]Running generate_until requests:  39%|███▉      | 65/165 [12:24<12:41,  7.62s/it]Running generate_until requests:  40%|████      | 66/165 [12:31<12:23,  7.51s/it]Running generate_until requests:  41%|████      | 67/165 [12:40<13:08,  8.05s/it]Running generate_until requests:  41%|████      | 68/165 [12:50<13:42,  8.48s/it]Running generate_until requests:  42%|████▏     | 69/165 [13:03<15:59, 10.00s/it]Running generate_until requests:  42%|████▏     | 70/165 [13:11<14:46,  9.33s/it]Running generate_until requests:  43%|████▎     | 71/165 [13:19<13:51,  8.84s/it]Running generate_until requests:  44%|████▎     | 72/165 [13:30<14:51,  9.59s/it]Running generate_until requests:  44%|████▍     | 73/165 [13:38<14:00,  9.13s/it]Running generate_until requests:  45%|████▍     | 74/165 [13:52<15:56, 10.51s/it]Running generate_until requests:  45%|████▌     | 75/165 [14:06<17:23, 11.59s/it]Running generate_until requests:  46%|████▌     | 76/165 [14:17<16:43, 11.27s/it]Running generate_until requests:  47%|████▋     | 77/165 [14:27<16:06, 10.99s/it]Running generate_until requests:  47%|████▋     | 78/165 [14:43<18:12, 12.56s/it]Running generate_until requests:  48%|████▊     | 79/165 [14:53<16:57, 11.83s/it]Running generate_until requests:  48%|████▊     | 80/165 [14:59<14:23, 10.16s/it]Running generate_until requests:  49%|████▉     | 81/165 [15:08<13:42,  9.79s/it]Running generate_until requests:  50%|████▉     | 82/165 [15:19<13:48,  9.98s/it]Running generate_until requests:  50%|█████     | 83/165 [15:30<14:17, 10.46s/it]Running generate_until requests:  51%|█████     | 84/165 [15:47<16:48, 12.45s/it]Running generate_until requests:  52%|█████▏    | 85/165 [15:54<14:11, 10.65s/it]Running generate_until requests:  52%|█████▏    | 86/165 [16:03<13:17, 10.10s/it]Running generate_until requests:  53%|█████▎    | 87/165 [16:13<13:09, 10.12s/it]Running generate_until requests:  53%|█████▎    | 88/165 [16:20<11:47,  9.18s/it]Running generate_until requests:  54%|█████▍    | 89/165 [16:27<10:59,  8.67s/it]Running generate_until requests:  55%|█████▍    | 90/165 [16:37<11:17,  9.03s/it]Running generate_until requests:  55%|█████▌    | 91/165 [16:42<09:36,  7.78s/it]Running generate_until requests:  56%|█████▌    | 92/165 [16:48<08:53,  7.31s/it]Running generate_until requests:  56%|█████▋    | 93/165 [16:57<09:24,  7.84s/it]Running generate_until requests:  57%|█████▋    | 94/165 [17:06<09:28,  8.01s/it]Running generate_until requests:  58%|█████▊    | 95/165 [17:15<09:36,  8.24s/it]Running generate_until requests:  58%|█████▊    | 96/165 [17:22<09:13,  8.03s/it]Running generate_until requests:  59%|█████▉    | 97/165 [17:30<08:59,  7.94s/it]Running generate_until requests:  59%|█████▉    | 98/165 [17:38<09:03,  8.11s/it]Running generate_until requests:  60%|██████    | 99/165 [17:46<08:51,  8.06s/it]Running generate_until requests:  61%|██████    | 100/165 [17:53<08:25,  7.78s/it]Running generate_until requests:  61%|██████    | 101/165 [18:03<08:45,  8.21s/it]Running generate_until requests:  62%|██████▏   | 102/165 [18:10<08:15,  7.87s/it]Running generate_until requests:  62%|██████▏   | 103/165 [18:22<09:22,  9.07s/it]Running generate_until requests:  63%|██████▎   | 104/165 [18:34<10:14, 10.08s/it]Running generate_until requests:  64%|██████▎   | 105/165 [18:46<10:35, 10.59s/it]Running generate_until requests:  64%|██████▍   | 106/165 [18:51<08:47,  8.94s/it]Running generate_until requests:  65%|██████▍   | 107/165 [18:59<08:19,  8.61s/it]Running generate_until requests:  65%|██████▌   | 108/165 [19:11<09:15,  9.75s/it]Running generate_until requests:  66%|██████▌   | 109/165 [19:18<08:20,  8.95s/it]Running generate_until requests:  67%|██████▋   | 110/165 [19:34<10:05, 11.02s/it]Running generate_until requests:  67%|██████▋   | 111/165 [19:41<08:54,  9.90s/it]Running generate_until requests:  68%|██████▊   | 112/165 [19:49<08:12,  9.30s/it]Running generate_until requests:  68%|██████▊   | 113/165 [20:00<08:28,  9.79s/it]Running generate_until requests:  69%|██████▉   | 114/165 [20:06<07:18,  8.60s/it]Running generate_until requests:  70%|██████▉   | 115/165 [20:13<06:39,  8.00s/it]Running generate_until requests:  70%|███████   | 116/165 [20:22<06:47,  8.32s/it]Running generate_until requests:  71%|███████   | 117/165 [20:28<06:10,  7.71s/it]Running generate_until requests:  72%|███████▏  | 118/165 [20:35<05:54,  7.55s/it]Running generate_until requests:  72%|███████▏  | 119/165 [20:45<06:21,  8.29s/it]Running generate_until requests:  73%|███████▎  | 120/165 [21:03<08:28, 11.29s/it]Running generate_until requests:  73%|███████▎  | 121/165 [21:18<08:58, 12.24s/it]Running generate_until requests:  74%|███████▍  | 122/165 [21:36<10:07, 14.14s/it]Running generate_until requests:  75%|███████▍  | 123/165 [21:54<10:34, 15.11s/it]Running generate_until requests:  75%|███████▌  | 124/165 [22:02<08:58, 13.12s/it]Running generate_until requests:  76%|███████▌  | 125/165 [22:15<08:40, 13.02s/it]Running generate_until requests:  76%|███████▋  | 126/165 [22:22<07:11, 11.07s/it]Running generate_until requests:  77%|███████▋  | 127/165 [22:28<06:06,  9.64s/it]Running generate_until requests:  78%|███████▊  | 128/165 [22:38<06:02,  9.79s/it]Running generate_until requests:  78%|███████▊  | 129/165 [22:46<05:28,  9.12s/it]Running generate_until requests:  79%|███████▉  | 130/165 [22:53<05:05,  8.73s/it]Running generate_until requests:  79%|███████▉  | 131/165 [23:06<05:41, 10.03s/it]Running generate_until requests:  80%|████████  | 132/165 [23:17<05:35, 10.16s/it]Running generate_until requests:  81%|████████  | 133/165 [23:24<04:54,  9.21s/it]Running generate_until requests:  81%|████████  | 134/165 [23:30<04:18,  8.34s/it]Running generate_until requests:  82%|████████▏ | 135/165 [23:37<03:52,  7.77s/it]Running generate_until requests:  82%|████████▏ | 136/165 [23:41<03:13,  6.68s/it]Running generate_until requests:  83%|████████▎ | 137/165 [23:49<03:23,  7.27s/it]Running generate_until requests:  84%|████████▎ | 138/165 [24:02<03:58,  8.82s/it]Running generate_until requests:  84%|████████▍ | 139/165 [24:10<03:40,  8.49s/it]Running generate_until requests:  85%|████████▍ | 140/165 [24:22<04:02,  9.68s/it]Running generate_until requests:  85%|████████▌ | 141/165 [24:29<03:35,  8.96s/it]Running generate_until requests:  86%|████████▌ | 142/165 [24:41<03:42,  9.67s/it]Running generate_until requests:  87%|████████▋ | 143/165 [24:47<03:07,  8.54s/it]Running generate_until requests:  87%|████████▋ | 144/165 [24:55<02:58,  8.51s/it]Running generate_until requests:  88%|████████▊ | 145/165 [25:03<02:44,  8.22s/it]Running generate_until requests:  88%|████████▊ | 146/165 [25:11<02:39,  8.39s/it]Running generate_until requests:  89%|████████▉ | 147/165 [25:19<02:24,  8.03s/it]Running generate_until requests:  90%|████████▉ | 148/165 [25:26<02:13,  7.83s/it]Running generate_until requests:  90%|█████████ | 149/165 [25:33<01:59,  7.49s/it]Running generate_until requests:  91%|█████████ | 150/165 [25:39<01:49,  7.28s/it]Running generate_until requests:  92%|█████████▏| 151/165 [25:54<02:12,  9.48s/it]Running generate_until requests:  92%|█████████▏| 152/165 [26:00<01:49,  8.40s/it]Running generate_until requests:  93%|█████████▎| 153/165 [26:10<01:46,  8.87s/it]Running generate_until requests:  93%|█████████▎| 154/165 [26:18<01:36,  8.80s/it]Running generate_until requests:  94%|█████████▍| 155/165 [26:28<01:30,  9.06s/it]Running generate_until requests:  95%|█████████▍| 156/165 [26:39<01:27,  9.68s/it]Running generate_until requests:  95%|█████████▌| 157/165 [26:45<01:08,  8.51s/it]Running generate_until requests:  96%|█████████▌| 158/165 [26:53<00:57,  8.20s/it]Running generate_until requests:  96%|█████████▋| 159/165 [27:09<01:03, 10.66s/it]Running generate_until requests:  97%|█████████▋| 160/165 [27:16<00:47,  9.59s/it]Running generate_until requests:  98%|█████████▊| 161/165 [27:26<00:38,  9.66s/it]Running generate_until requests:  98%|█████████▊| 162/165 [27:32<00:25,  8.65s/it]Running generate_until requests:  99%|█████████▉| 163/165 [27:37<00:15,  7.55s/it]Running generate_until requests:  99%|█████████▉| 164/165 [27:43<00:07,  7.11s/it]Running generate_until requests: 100%|██████████| 165/165 [27:50<00:00,  6.95s/it]Running generate_until requests: 100%|██████████| 165/165 [27:50<00:00, 10.12s/it]
