Already up to date.
Obtaining file:///opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Installing backend dependencies: started
  Installing backend dependencies: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: filelock in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (3.13.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (0.21.4)
Requirement already satisfied: numpy>=1.17 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (1.26.4)
Requirement already satisfied: packaging>=20.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (23.2)
Requirement already satisfied: pyyaml>=5.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (6.0.1)
Requirement already satisfied: regex!=2019.12.17 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (2023.12.25)
Requirement already satisfied: requests in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (2.31.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (0.15.2)
Requirement already satisfied: safetensors>=0.3.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (0.4.2)
Requirement already satisfied: tqdm>=4.27 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (4.66.2)
Requirement already satisfied: fsspec>=2023.5.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (2023.10.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (4.9.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (1.26.18)
Requirement already satisfied: certifi>=2017.4.17 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (2024.2.2)
Building wheels for collected packages: transformers
  Building editable for transformers (pyproject.toml): started
  Building editable for transformers (pyproject.toml): finished with status 'done'
  Created wheel for transformers: filename=transformers-4.36.0.dev0-0.editable-py3-none-any.whl size=40509 sha256=e5edf5fa0d3f48894cc7cc40594da3e6701afa289c554aba29a9be399d4ca772
  Stored in directory: /tmp/pip-ephem-wheel-cache-ozfcooju/wheels/b7/36/f7/419f9185aae64b34b34c9ec7bc0c4910b3433f825ab8b94ec7
Successfully built transformers
Installing collected packages: transformers
  Attempting uninstall: transformers
    Found existing installation: transformers 4.36.0.dev0
    Uninstalling transformers-4.36.0.dev0:
      Successfully uninstalled transformers-4.36.0.dev0
Successfully installed transformers-4.36.0.dev0
Requirement already satisfied: termcolor in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (2.4.0)
Requirement already satisfied: wandb in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (0.16.3)
Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from wandb) (8.1.7)
Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from wandb) (3.1.41)
Requirement already satisfied: requests<3,>=2.0.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from wandb) (2.31.0)
Requirement already satisfied: psutil>=5.0.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from wandb) (5.9.8)
Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from wandb) (1.40.4)
Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from wandb) (0.4.0)
Requirement already satisfied: PyYAML in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from wandb) (6.0.1)
Requirement already satisfied: setproctitle in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from wandb) (1.3.3)
Requirement already satisfied: setuptools in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from wandb) (69.0.3)
Requirement already satisfied: appdirs>=1.4.3 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from wandb) (1.4.4)
Requirement already satisfied: typing-extensions in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from wandb) (4.9.0)
Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from wandb) (4.25.2)
Requirement already satisfied: six>=1.4.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)
Requirement already satisfied: certifi>=2017.4.17 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)
Requirement already satisfied: smmap<6,>=3.0.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)
Requirement already satisfied: datasets in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (2.17.0)
Requirement already satisfied: filelock in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from datasets) (3.13.1)
Requirement already satisfied: numpy>=1.17 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from datasets) (1.26.4)
Requirement already satisfied: pyarrow>=12.0.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from datasets) (15.0.0)
Requirement already satisfied: pyarrow-hotfix in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from datasets) (0.6)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from datasets) (0.3.8)
Requirement already satisfied: pandas in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from datasets) (2.2.0)
Requirement already satisfied: requests>=2.19.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from datasets) (2.31.0)
Requirement already satisfied: tqdm>=4.62.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from datasets) (4.66.2)
Requirement already satisfied: xxhash in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from datasets) (3.4.1)
Requirement already satisfied: multiprocess in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from datasets) (0.70.16)
Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)
Requirement already satisfied: aiohttp in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from datasets) (3.9.3)
Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from datasets) (0.21.4)
Requirement already satisfied: packaging in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from datasets) (23.2)
Requirement already satisfied: pyyaml>=5.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from datasets) (6.0.1)
Requirement already satisfied: aiosignal>=1.1.2 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)
Requirement already satisfied: attrs>=17.3.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)
Requirement already satisfied: frozenlist>=1.1.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)
Requirement already satisfied: multidict<7.0,>=4.5 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)
Requirement already satisfied: yarl<2.0,>=1.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)
Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.18)
Requirement already satisfied: certifi>=2017.4.17 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2024.2.2)
Requirement already satisfied: python-dateutil>=2.8.2 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from pandas->datasets) (2024.1)
Requirement already satisfied: tzdata>=2022.7 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from pandas->datasets) (2024.1)
Requirement already satisfied: six>=1.5 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)
Requirement already satisfied: accelerate in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (0.27.0)
Requirement already satisfied: numpy>=1.17 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from accelerate) (1.26.4)
Requirement already satisfied: packaging>=20.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from accelerate) (23.2)
Requirement already satisfied: psutil in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from accelerate) (5.9.8)
Requirement already satisfied: pyyaml in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from accelerate) (6.0.1)
Requirement already satisfied: torch>=1.10.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from accelerate) (2.1.0)
Requirement already satisfied: huggingface-hub in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from accelerate) (0.21.4)
Requirement already satisfied: safetensors>=0.3.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from accelerate) (0.4.2)
Requirement already satisfied: filelock in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.13.1)
Requirement already satisfied: typing-extensions in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.9.0)
Requirement already satisfied: sympy in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.12)
Requirement already satisfied: networkx in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.2.1)
Requirement already satisfied: jinja2 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.3)
Requirement already satisfied: fsspec in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.18.1)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)
Requirement already satisfied: triton==2.1.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.1.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)
Requirement already satisfied: requests in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.66.2)
Requirement already satisfied: MarkupSafe>=2.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)
Requirement already satisfied: certifi>=2017.4.17 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)
Requirement already satisfied: mpmath>=0.19 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)
Requirement already satisfied: huggingface_hub[cli] in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (0.21.4)
Requirement already satisfied: filelock in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from huggingface_hub[cli]) (3.13.1)
Requirement already satisfied: fsspec>=2023.5.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from huggingface_hub[cli]) (2023.10.0)
Requirement already satisfied: requests in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from huggingface_hub[cli]) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from huggingface_hub[cli]) (4.66.2)
Requirement already satisfied: pyyaml>=5.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from huggingface_hub[cli]) (6.0.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from huggingface_hub[cli]) (4.9.0)
Requirement already satisfied: packaging>=20.9 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from huggingface_hub[cli]) (23.2)
Requirement already satisfied: InquirerPy==0.3.4 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from huggingface_hub[cli]) (0.3.4)
Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)
Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.43)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests->huggingface_hub[cli]) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests->huggingface_hub[cli]) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests->huggingface_hub[cli]) (1.26.18)
Requirement already satisfied: certifi>=2017.4.17 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from requests->huggingface_hub[cli]) (2024.2.2)
Requirement already satisfied: wcwidth in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)
Requirement already satisfied: matplotlib in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (3.8.3)
Requirement already satisfied: contourpy>=1.0.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from matplotlib) (1.2.0)
Requirement already satisfied: cycler>=0.10 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from matplotlib) (0.12.1)
Requirement already satisfied: fonttools>=4.22.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from matplotlib) (4.49.0)
Requirement already satisfied: kiwisolver>=1.3.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from matplotlib) (1.4.5)
Requirement already satisfied: numpy<2,>=1.21 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from matplotlib) (1.26.4)
Requirement already satisfied: packaging>=20.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from matplotlib) (23.2)
Requirement already satisfied: pillow>=8 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from matplotlib) (9.4.0)
Requirement already satisfied: pyparsing>=2.3.1 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from matplotlib) (3.1.1)
Requirement already satisfied: python-dateutil>=2.7 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from matplotlib) (2.8.2)
Requirement already satisfied: importlib-resources>=3.2.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from matplotlib) (6.1.2)
Requirement already satisfied: zipp>=3.1.0 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)
Requirement already satisfied: six>=1.5 in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)
Requirement already satisfied: sentencepiece in /opt/hpcaas/.mounts/fs-0565f60d669b6a2d3/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages (0.1.99)
/data/home/beidic/anaconda3/envs/yangllm2/bin/python
/data/home/beidic/anaconda3/envs/yangllm2/bin/python
[1m[31mERROR! `huggingface-cli login` uses an outdated login mechanism that is not compatible with the Hugging Face Hub backend anymore. Please use `huggingface-cli login instead.[0m
Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /data/home/beidic/.cache/huggingface/token
Login successful
the commit hash is 780a8e3
the hash of time is 822795
Hostname: a100-st-p4de24xlarge-1070
the commit hash is 780a8e3
the hash of time is 823073
Hostname: a100-st-p4de24xlarge-1070
the commit hash is 780a8e3
the hash of time is 823871
Hostname: a100-st-p4de24xlarge-1070
the commit hash is 780a8e3
the hash of time is 866846
Hostname: a100-st-p4de24xlarge-1070
the commit hash is 780a8e3
the hash of time is 913674
Hostname: a100-st-p4de24xlarge-1070
the commit hash is 780a8e3
the hash of time is 936938
Hostname: a100-st-p4de24xlarge-1070
the commit hash is 780a8e3
the hash of time is 946897
Hostname: a100-st-p4de24xlarge-1070
the commit hash is 780a8e3
the hash of time is 959975
Hostname: a100-st-p4de24xlarge-1070
We now use eos_token as pad token
We now use eos_token as pad token
the max length for processing the dataset is 260
hostname is a100-st-p4de24xlarge-1070
loading training set from 0 to 9
We now use eos_token as pad token
We now use eos_token as pad token
the max length for processing the dataset is 260
hostname is a100-st-p4de24xlarge-1070
loading training set from 0 to 9
We now use eos_token as pad token
We now use eos_token as pad token
the max length for processing the dataset is 260
hostname is a100-st-p4de24xlarge-1070
loading training set from 0 to 9
We now use eos_token as pad token
We now use eos_token as pad token
the max length for processing the dataset is 260
hostname is a100-st-p4de24xlarge-1070
loading training set from 0 to 9
We now use eos_token as pad token
We now use eos_token as pad token
the max length for processing the dataset is 260
hostname is a100-st-p4de24xlarge-1070
loading training set from 0 to 9
We now use eos_token as pad token
We now use eos_token as pad token
the max length for processing the dataset is 260
hostname is a100-st-p4de24xlarge-1070
loading training set from 0 to 9
We now use eos_token as pad token
We now use eos_token as pad token
the max length for processing the dataset is 260
hostname is a100-st-p4de24xlarge-1070
loading training set from 0 to 9
We now use eos_token as pad token
We now use eos_token as pad token
the max length for processing the dataset is 260
hostname is a100-st-p4de24xlarge-1070
loading training set from 0 to 9
hostname is a100-st-p4de24xlarge-1070
hostname is a100-st-p4de24xlarge-1070
hostname is a100-st-p4de24xlarge-1070
hostname is a100-st-p4de24xlarge-1070
hostname is a100-st-p4de24xlarge-1070
hostname is a100-st-p4de24xlarge-1070
hostname is a100-st-p4de24xlarge-1070
hostname is a100-st-p4de24xlarge-1070
The input ids is tensor([    1, 22222,  8525,  4127,  1090,  6480, 23136, 29892,   322,  9792,
          515,   594,  3901, 18845, 29889,    13,  4013,   338,   278,  8405,
          310, 24205, 29936,  5936,  5281,   393,  1749, 12463,  1532, 29899,
          915,   292,   338, 19632,  3368,  2861,   304,   278,   671,   310,
        27231,  5391,   470,   916,  5960,  2925, 29889,  2398, 29892,  3763,
         4788,  1403,  1218,  1438,  5960,  2925,   526,   451,  3307,   363,
          278, 27231,  5391,   293,   470,   788,   919, 29889,   383,   542,
         1558,   292,   373,   278, 18066,   267, 29892,  1020, 10859, 29892,
          470,   916, 21420,   310,   270,   952,  2220,   338,   451,  3307,
        29889,   512,  1797,   304,  8072,  9792,   591,  1818, 18720,   393,
         1749, 22986, 29933, 15202,   338,  1828,   792,   304,  1472,  1840,
        15331,   322,  6095,  5589,   358, 29889,    13,  4013,   338,   278,
         1346, 29956,   340,   383,  7168, 30024,   393,  1784,  2305,  3052,
        29889,  2688,  4049,  8569, 14419,   368,   373, 27826,   825,   338,
         9391,   322,  4418,   304, 13389,   278,  3256,  1235,  7432,  2874,
          263,  5434,   363,   263,  9150,  3234,   573,  2834, 29889,   910,
          338,   278,  8405,   310,   825,   306,  4658,   756,  8126,   592,
        12789,  1558,   287,  1549, 24081,   322,  1623, 29879, 29889, 17044,
        10672, 29892,  3632,  6393,  2264, 29892, 25074,   346,   408,  1532,
          408,   599,   310,   278,  2551,   267,   306,   505,  1063,  1999,
        11517,   411, 29892,   599])The input ids is tensor([    1, 22222,  8525,  4127,  1090,  6480, 23136, 29892,   322,  9792,
          515,   594,  3901, 18845, 29889,    13,  4013,   338,   278,  8405,
          310, 24205, 29936,  5936,  5281,   393,  1749, 12463,  1532, 29899,
          915,   292,   338, 19632,  3368,  2861,   304,   278,   671,   310,
        27231,  5391,   470,   916,  5960,  2925, 29889,  2398, 29892,  3763,
         4788,  1403,  1218,  1438,  5960,  2925,   526,   451,  3307,   363,
          278, 27231,  5391,   293,   470,   788,   919, 29889,   383,   542,
         1558,   292,   373,   278, 18066,   267, 29892,  1020, 10859, 29892,
          470,   916, 21420,   310,   270,   952,  2220,   338,   451,  3307,
        29889,   512,  1797,   304,  8072,  9792,   591,  1818, 18720,   393,
         1749, 22986, 29933, 15202,   338,  1828,   792,   304,  1472,  1840,
        15331,   322,  6095,  5589,   358, 29889,    13,  4013,   338,   278,
         1346, 29956,   340,   383,  7168, 30024,   393,  1784,  2305,  3052,
        29889,  2688,  4049,  8569, 14419,   368,   373, 27826,   825,   338,
         9391,   322,  4418,   304, 13389,   278,  3256,  1235,  7432,  2874,
          263,  5434,   363,   263,  9150,  3234,   573,  2834, 29889,   910,
          338,   278,  8405,   310,   825,   306,  4658,   756,  8126,   592,
        12789,  1558,   287,  1549, 24081,   322,  1623, 29879, 29889, 17044,
        10672, 29892,  3632,  6393,  2264, 29892, 25074,   346,   408,  1532,
          408,   599,   310,   278,  2551,   267,   306,   505,  1063,  1999,
        11517,   411, 29892,   599])

The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])

The input ids is tensor([    1, 22222,  8525,  4127,  1090,  6480, 23136, 29892,   322,  9792,
          515,   594,  3901, 18845, 29889,    13,  4013,   338,   278,  8405,
          310, 24205, 29936,  5936,  5281,   393,  1749, 12463,  1532, 29899,
          915,   292,   338, 19632,  3368,  2861,   304,   278,   671,   310,
        27231,  5391,   470,   916,  5960,  2925, 29889,  2398, 29892,  3763,
         4788,  1403,  1218,  1438,  5960,  2925,   526,   451,  3307,   363,
          278, 27231,  5391,   293,   470,   788,   919, 29889,   383,   542,
         1558,   292,   373,   278, 18066,   267, 29892,  1020, 10859, 29892,
          470,   916, 21420,   310,   270,   952,  2220,   338,   451,  3307,
        29889,   512,  1797,   304,  8072,  9792,   591,  1818, 18720,   393,
         1749, 22986, 29933, 15202,   338,  1828,   792,   304,  1472,  1840,
        15331,   322,  6095,  5589,   358, 29889,    13,  4013,   338,   278,
         1346, 29956,   340,   383,  7168, 30024,   393,  1784,  2305,  3052,
        29889,  2688,  4049,  8569, 14419,   368,   373, 27826,   825,   338,
         9391,   322,  4418,   304, 13389,   278,  3256,  1235,  7432,  2874,
          263,  5434,   363,   263,  9150,  3234,   573,  2834, 29889,   910,
          338,   278,  8405,   310,   825,   306,  4658,   756,  8126,   592,
        12789,  1558,   287,  1549, 24081,   322,  1623, 29879, 29889, 17044,
        10672, 29892,  3632,  6393,  2264, 29892, 25074,   346,   408,  1532,
          408,   599,   310,   278,  2551,   267,   306,   505,  1063,  1999,
        11517,   411, 29892,   599])
The input ids is tensor([    1, 22222,  8525,  4127,  1090,  6480, 23136, 29892,   322,  9792,
          515,   594,  3901, 18845, 29889,    13,  4013,   338,   278,  8405,
          310, 24205, 29936,  5936,  5281,   393,  1749, 12463,  1532, 29899,
          915,   292,   338, 19632,  3368,  2861,   304,   278,   671,   310,
        27231,  5391,   470,   916,  5960,  2925, 29889,  2398, 29892,  3763,
         4788,  1403,  1218,  1438,  5960,  2925,   526,   451,  3307,   363,
          278, 27231,  5391,   293,   470,   788,   919, 29889,   383,   542,
         1558,   292,   373,   278, 18066,   267, 29892,  1020, 10859, 29892,
          470,   916, 21420,   310,   270,   952,  2220,   338,   451,  3307,
        29889,   512,  1797,   304,  8072,  9792,   591,  1818, 18720,   393,
         1749, 22986, 29933, 15202,   338,  1828,   792,   304,  1472,  1840,
        15331,   322,  6095,  5589,   358, 29889,    13,  4013,   338,   278,
         1346, 29956,   340,   383,  7168, 30024,   393,  1784,  2305,  3052,
        29889,  2688,  4049,  8569, 14419,   368,   373, 27826,   825,   338,
         9391,   322,  4418,   304, 13389,   278,  3256,  1235,  7432,  2874,
          263,  5434,   363,   263,  9150,  3234,   573,  2834, 29889,   910,
          338,   278,  8405,   310,   825,   306,  4658,   756,  8126,   592,
        12789,  1558,   287,  1549, 24081,   322,  1623, 29879, 29889, 17044,
        10672, 29892,  3632,  6393,  2264, 29892, 25074,   346,   408,  1532,
          408,   599,   310,   278,  2551,   267,   306,   505,  1063,  1999,
        11517,   411, 29892,   599])The input ids is tensor([    1, 22222,  8525,  4127,  1090,  6480, 23136, 29892,   322,  9792,
          515,   594,  3901, 18845, 29889,    13,  4013,   338,   278,  8405,
          310, 24205, 29936,  5936,  5281,   393,  1749, 12463,  1532, 29899,
          915,   292,   338, 19632,  3368,  2861,   304,   278,   671,   310,
        27231,  5391,   470,   916,  5960,  2925, 29889,  2398, 29892,  3763,
         4788,  1403,  1218,  1438,  5960,  2925,   526,   451,  3307,   363,
          278, 27231,  5391,   293,   470,   788,   919, 29889,   383,   542,
         1558,   292,   373,   278, 18066,   267, 29892,  1020, 10859, 29892,
          470,   916, 21420,   310,   270,   952,  2220,   338,   451,  3307,
        29889,   512,  1797,   304,  8072,  9792,   591,  1818, 18720,   393,
         1749, 22986, 29933, 15202,   338,  1828,   792,   304,  1472,  1840,
        15331,   322,  6095,  5589,   358, 29889,    13,  4013,   338,   278,
         1346, 29956,   340,   383,  7168, 30024,   393,  1784,  2305,  3052,
        29889,  2688,  4049,  8569, 14419,   368,   373, 27826,   825,   338,
         9391,   322,  4418,   304, 13389,   278,  3256,  1235,  7432,  2874,
          263,  5434,   363,   263,  9150,  3234,   573,  2834, 29889,   910,
          338,   278,  8405,   310,   825,   306,  4658,   756,  8126,   592,
        12789,  1558,   287,  1549, 24081,   322,  1623, 29879, 29889, 17044,
        10672, 29892,  3632,  6393,  2264, 29892, 25074,   346,   408,  1532,
          408,   599,   310,   278,  2551,   267,   306,   505,  1063,  1999,
        11517,   411, 29892,   599])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])

The input ids is tensor([    1, 22222,  8525,  4127,  1090,  6480, 23136, 29892,   322,  9792,
          515,   594,  3901, 18845, 29889,    13,  4013,   338,   278,  8405,
          310, 24205, 29936,  5936,  5281,   393,  1749, 12463,  1532, 29899,
          915,   292,   338, 19632,  3368,  2861,   304,   278,   671,   310,
        27231,  5391,   470,   916,  5960,  2925, 29889,  2398, 29892,  3763,
         4788,  1403,  1218,  1438,  5960,  2925,   526,   451,  3307,   363,
          278, 27231,  5391,   293,   470,   788,   919, 29889,   383,   542,
         1558,   292,   373,   278, 18066,   267, 29892,  1020, 10859, 29892,
          470,   916, 21420,   310,   270,   952,  2220,   338,   451,  3307,
        29889,   512,  1797,   304,  8072,  9792,   591,  1818, 18720,   393,
         1749, 22986, 29933, 15202,   338,  1828,   792,   304,  1472,  1840,
        15331,   322,  6095,  5589,   358, 29889,    13,  4013,   338,   278,
         1346, 29956,   340,   383,  7168, 30024,   393,  1784,  2305,  3052,
        29889,  2688,  4049,  8569, 14419,   368,   373, 27826,   825,   338,
         9391,   322,  4418,   304, 13389,   278,  3256,  1235,  7432,  2874,
          263,  5434,   363,   263,  9150,  3234,   573,  2834, 29889,   910,
          338,   278,  8405,   310,   825,   306,  4658,   756,  8126,   592,
        12789,  1558,   287,  1549, 24081,   322,  1623, 29879, 29889, 17044,
        10672, 29892,  3632,  6393,  2264, 29892, 25074,   346,   408,  1532,
          408,   599,   310,   278,  2551,   267,   306,   505,  1063,  1999,
        11517,   411, 29892,   599])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])

The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The input ids is tensor([    1, 22222,  8525,  4127,  1090,  6480, 23136, 29892,   322,  9792,
          515,   594,  3901, 18845, 29889,    13,  4013,   338,   278,  8405,
          310, 24205, 29936,  5936,  5281,   393,  1749, 12463,  1532, 29899,
          915,   292,   338, 19632,  3368,  2861,   304,   278,   671,   310,
        27231,  5391,   470,   916,  5960,  2925, 29889,  2398, 29892,  3763,
         4788,  1403,  1218,  1438,  5960,  2925,   526,   451,  3307,   363,
          278, 27231,  5391,   293,   470,   788,   919, 29889,   383,   542,
         1558,   292,   373,   278, 18066,   267, 29892,  1020, 10859, 29892,
          470,   916, 21420,   310,   270,   952,  2220,   338,   451,  3307,
        29889,   512,  1797,   304,  8072,  9792,   591,  1818, 18720,   393,
         1749, 22986, 29933, 15202,   338,  1828,   792,   304,  1472,  1840,
        15331,   322,  6095,  5589,   358, 29889,    13,  4013,   338,   278,
         1346, 29956,   340,   383,  7168, 30024,   393,  1784,  2305,  3052,
        29889,  2688,  4049,  8569, 14419,   368,   373, 27826,   825,   338,
         9391,   322,  4418,   304, 13389,   278,  3256,  1235,  7432,  2874,
          263,  5434,   363,   263,  9150,  3234,   573,  2834, 29889,   910,
          338,   278,  8405,   310,   825,   306,  4658,   756,  8126,   592,
        12789,  1558,   287,  1549, 24081,   322,  1623, 29879, 29889, 17044,
        10672, 29892,  3632,  6393,  2264, 29892, 25074,   346,   408,  1532,
          408,   599,   310,   278,  2551,   267,   306,   505,  1063,  1999,
        11517,   411, 29892,   599])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The input ids is tensor([    1,  8589,   297,   278, 13851, 29892,   470,  1560,   514,   278,
        14652,   267,   297,  6709, 29892,   727, 30010, 29879,   263, 14089,
          363,   366,   297, 14299, 11653, 29889,  3617,   714,   310,   278,
         3699,   445,  4723,   355,   322,  1423,   714,   777,   310,  1749,
        25448,   610,  2039,   297,   278,  4038,  1434,  6421,   297,   363,
          596,  2446,  3423,  5738, 25114, 17623,   545,  1244,   472, 25114,
         3423,  5738, 13822, 29888,  1165, 29991,    13,  3644,   366,   864,
          304,  5110,  1048,   278,  4955,   310,   278, 12886,  3362,   297,
        14299, 11653, 29892,  2343,   304, 13402, 30010, 29879,  3164,  3096,
        12788,  2671,  4815, 29889,   910, 14089, 12955,   278,  3268,   310,
          697,   310,   278, 24842, 12886,  3362,  8957,   793,   297,   365,
         2736,   265,  5127, 29889,  3645,  3786,  1549,  3979, 29892,   278,
        14089,   756,  2318,   260,  2470,   373, 24211,   322, 16340,   472,
        29871, 29896, 29896, 13862,   322, 29871, 29896, 11278, 29889,  4525,
          260,  2470,   526, 18043,   491, 27886,   261,  5133,  2153,  1058,
          508,  1234,   738,  5155,   393,   366,   505,  1048,   278, 10555,
         2671, 29889,   887,   508,   884,  6282,   278, 10555,  2671,   373,
          596,  1914,   322, 18864,   931,  3063,   472,   278, 14089, 30010,
        29879, 22586, 29889,  2860,  6282,   292,   278, 10555,  2671, 29892,
         1095,   596,  2462,   411,   263,  7251,   446,   373,   697,   310,
          278, 14089, 30010, 29879])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The input ids is tensor([    1,  8589,   297,   278, 13851, 29892,   470,  1560,   514,   278,
        14652,   267,   297,  6709, 29892,   727, 30010, 29879,   263, 14089,
          363,   366,   297, 14299, 11653, 29889,  3617,   714,   310,   278,
         3699,   445,  4723,   355,   322,  1423,   714,   777,   310,  1749,
        25448,   610,  2039,   297,   278,  4038,  1434,  6421,   297,   363,
          596,  2446,  3423,  5738, 25114, 17623,   545,  1244,   472, 25114,
         3423,  5738, 13822, 29888,  1165, 29991,    13,  3644,   366,   864,
          304,  5110,  1048,   278,  4955,   310,   278, 12886,  3362,   297,
        14299, 11653, 29892,  2343,   304, 13402, 30010, 29879,  3164,  3096,
        12788,  2671,  4815, 29889,   910, 14089, 12955,   278,  3268,   310,
          697,   310,   278, 24842, 12886,  3362,  8957,   793,   297,   365,
         2736,   265,  5127, 29889,  3645,  3786,  1549,  3979, 29892,   278,
        14089,   756,  2318,   260,  2470,   373, 24211,   322, 16340,   472,
        29871, 29896, 29896, 13862,   322, 29871, 29896, 11278, 29889,  4525,
          260,  2470,   526, 18043,   491, 27886,   261,  5133,  2153,  1058,
          508,  1234,   738,  5155,   393,   366,   505,  1048,   278, 10555,
         2671, 29889,   887,   508,   884,  6282,   278, 10555,  2671,   373,
          596,  1914,   322, 18864,   931,  3063,   472,   278, 14089, 30010,
        29879, 22586, 29889,  2860,  6282,   292,   278, 10555,  2671, 29892,
         1095,   596,  2462,   411,   263,  7251,   446,   373,   697,   310,
          278, 14089, 30010, 29879])
The input ids is tensor([    1,  8589,   297,   278, 13851, 29892,   470,  1560,   514,   278,
        14652,   267,   297,  6709, 29892,   727, 30010, 29879,   263, 14089,
          363,   366,   297, 14299, 11653, 29889,  3617,   714,   310,   278,
         3699,   445,  4723,   355,   322,  1423,   714,   777,   310,  1749,
        25448,   610,  2039,   297,   278,  4038,  1434,  6421,   297,   363,
          596,  2446,  3423,  5738, 25114, 17623,   545,  1244,   472, 25114,
         3423,  5738, 13822, 29888,  1165, 29991,    13,  3644,   366,   864,
          304,  5110,  1048,   278,  4955,   310,   278, 12886,  3362,   297,
        14299, 11653, 29892,  2343,   304, 13402, 30010, 29879,  3164,  3096,
        12788,  2671,  4815, 29889,   910, 14089, 12955,   278,  3268,   310,
          697,   310,   278, 24842, 12886,  3362,  8957,   793,   297,   365,
         2736,   265,  5127, 29889,  3645,  3786,  1549,  3979, 29892,   278,
        14089,   756,  2318,   260,  2470,   373, 24211,   322, 16340,   472,
        29871, 29896, 29896, 13862,   322, 29871, 29896, 11278, 29889,  4525,
          260,  2470,   526, 18043,   491, 27886,   261,  5133,  2153,  1058,
          508,  1234,   738,  5155,   393,   366,   505,  1048,   278, 10555,
         2671, 29889,   887,   508,   884,  6282,   278, 10555,  2671,   373,
          596,  1914,   322, 18864,   931,  3063,   472,   278, 14089, 30010,
        29879, 22586, 29889,  2860,  6282,   292,   278, 10555,  2671, 29892,
         1095,   596,  2462,   411,   263,  7251,   446,   373,   697,   310,
          278, 14089, 30010, 29879])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])

The input ids is tensor([    1,  8589,   297,   278, 13851, 29892,   470,  1560,   514,   278,
        14652,   267,   297,  6709, 29892,   727, 30010, 29879,   263, 14089,
          363,   366,   297, 14299, 11653, 29889,  3617,   714,   310,   278,
         3699,   445,  4723,   355,   322,  1423,   714,   777,   310,  1749,
        25448,   610,  2039,   297,   278,  4038,  1434,  6421,   297,   363,
          596,  2446,  3423,  5738, 25114, 17623,   545,  1244,   472, 25114,
         3423,  5738, 13822, 29888,  1165, 29991,    13,  3644,   366,   864,
          304,  5110,  1048,   278,  4955,   310,   278, 12886,  3362,   297,
        14299, 11653, 29892,  2343,   304, 13402, 30010, 29879,  3164,  3096,
        12788,  2671,  4815, 29889,   910, 14089, 12955,   278,  3268,   310,
          697,   310,   278, 24842, 12886,  3362,  8957,   793,   297,   365,
         2736,   265,  5127, 29889,  3645,  3786,  1549,  3979, 29892,   278,
        14089,   756,  2318,   260,  2470,   373, 24211,   322, 16340,   472,
        29871, 29896, 29896, 13862,   322, 29871, 29896, 11278, 29889,  4525,
          260,  2470,   526, 18043,   491, 27886,   261,  5133,  2153,  1058,
          508,  1234,   738,  5155,   393,   366,   505,  1048,   278, 10555,
         2671, 29889,   887,   508,   884,  6282,   278, 10555,  2671,   373,
          596,  1914,   322, 18864,   931,  3063,   472,   278, 14089, 30010,
        29879, 22586, 29889,  2860,  6282,   292,   278, 10555,  2671, 29892,
         1095,   596,  2462,   411,   263,  7251,   446,   373,   697,   310,
          278, 14089, 30010, 29879])
The input ids is tensor([    1,  8589,   297,   278, 13851, 29892,   470,  1560,   514,   278,
        14652,   267,   297,  6709, 29892,   727, 30010, 29879,   263, 14089,
          363,   366,   297, 14299, 11653, 29889,  3617,   714,   310,   278,
         3699,   445,  4723,   355,   322,  1423,   714,   777,   310,  1749,
        25448,   610,  2039,   297,   278,  4038,  1434,  6421,   297,   363,
          596,  2446,  3423,  5738, 25114, 17623,   545,  1244,   472, 25114,
         3423,  5738, 13822, 29888,  1165, 29991,    13,  3644,   366,   864,
          304,  5110,  1048,   278,  4955,   310,   278, 12886,  3362,   297,
        14299, 11653, 29892,  2343,   304, 13402, 30010, 29879,  3164,  3096,
        12788,  2671,  4815, 29889,   910, 14089, 12955,   278,  3268,   310,
          697,   310,   278, 24842, 12886,  3362,  8957,   793,   297,   365,
         2736,   265,  5127, 29889,  3645,  3786,  1549,  3979, 29892,   278,
        14089,   756,  2318,   260,  2470,   373, 24211,   322, 16340,   472,
        29871, 29896, 29896, 13862,   322, 29871, 29896, 11278, 29889,  4525,
          260,  2470,   526, 18043,   491, 27886,   261,  5133,  2153,  1058,
          508,  1234,   738,  5155,   393,   366,   505,  1048,   278, 10555,
         2671, 29889,   887,   508,   884,  6282,   278, 10555,  2671,   373,
          596,  1914,   322, 18864,   931,  3063,   472,   278, 14089, 30010,
        29879, 22586, 29889,  2860,  6282,   292,   278, 10555,  2671, 29892,
         1095,   596,  2462,   411,   263,  7251,   446,   373,   697,   310,
          278, 14089, 30010, 29879])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The input ids is tensor([    1,  8589,   297,   278, 13851, 29892,   470,  1560,   514,   278,
        14652,   267,   297,  6709, 29892,   727, 30010, 29879,   263, 14089,
          363,   366,   297, 14299, 11653, 29889,  3617,   714,   310,   278,
         3699,   445,  4723,   355,   322,  1423,   714,   777,   310,  1749,
        25448,   610,  2039,   297,   278,  4038,  1434,  6421,   297,   363,
          596,  2446,  3423,  5738, 25114, 17623,   545,  1244,   472, 25114,
         3423,  5738, 13822, 29888,  1165, 29991,    13,  3644,   366,   864,
          304,  5110,  1048,   278,  4955,   310,   278, 12886,  3362,   297,
        14299, 11653, 29892,  2343,   304, 13402, 30010, 29879,  3164,  3096,
        12788,  2671,  4815, 29889,   910, 14089, 12955,   278,  3268,   310,
          697,   310,   278, 24842, 12886,  3362,  8957,   793,   297,   365,
         2736,   265,  5127, 29889,  3645,  3786,  1549,  3979, 29892,   278,
        14089,   756,  2318,   260,  2470,   373, 24211,   322, 16340,   472,
        29871, 29896, 29896, 13862,   322, 29871, 29896, 11278, 29889,  4525,
          260,  2470,   526, 18043,   491, 27886,   261,  5133,  2153,  1058,
          508,  1234,   738,  5155,   393,   366,   505,  1048,   278, 10555,
         2671, 29889,   887,   508,   884,  6282,   278, 10555,  2671,   373,
          596,  1914,   322, 18864,   931,  3063,   472,   278, 14089, 30010,
        29879, 22586, 29889,  2860,  6282,   292,   278, 10555,  2671, 29892,
         1095,   596,  2462,   411,   263,  7251,   446,   373,   697,   310,
          278, 14089, 30010, 29879])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The input ids is tensor([    1,  8589,   297,   278, 13851, 29892,   470,  1560,   514,   278,
        14652,   267,   297,  6709, 29892,   727, 30010, 29879,   263, 14089,
          363,   366,   297, 14299, 11653, 29889,  3617,   714,   310,   278,
         3699,   445,  4723,   355,   322,  1423,   714,   777,   310,  1749,
        25448,   610,  2039,   297,   278,  4038,  1434,  6421,   297,   363,
          596,  2446,  3423,  5738, 25114, 17623,   545,  1244,   472, 25114,
         3423,  5738, 13822, 29888,  1165, 29991,    13,  3644,   366,   864,
          304,  5110,  1048,   278,  4955,   310,   278, 12886,  3362,   297,
        14299, 11653, 29892,  2343,   304, 13402, 30010, 29879,  3164,  3096,
        12788,  2671,  4815, 29889,   910, 14089, 12955,   278,  3268,   310,
          697,   310,   278, 24842, 12886,  3362,  8957,   793,   297,   365,
         2736,   265,  5127, 29889,  3645,  3786,  1549,  3979, 29892,   278,
        14089,   756,  2318,   260,  2470,   373, 24211,   322, 16340,   472,
        29871, 29896, 29896, 13862,   322, 29871, 29896, 11278, 29889,  4525,
          260,  2470,   526, 18043,   491, 27886,   261,  5133,  2153,  1058,
          508,  1234,   738,  5155,   393,   366,   505,  1048,   278, 10555,
         2671, 29889,   887,   508,   884,  6282,   278, 10555,  2671,   373,
          596,  1914,   322, 18864,   931,  3063,   472,   278, 14089, 30010,
        29879, 22586, 29889,  2860,  6282,   292,   278, 10555,  2671, 29892,
         1095,   596,  2462,   411,   263,  7251,   446,   373,   697,   310,
          278, 14089, 30010, 29879])
The input ids is tensor([    1,   278,   527,  1971,  2153, 29892,   322,   373,   393,  1298,
          306,   437,   451, 16193,   304,  1827,  1407,  1568, 29889,  1954,
         4011,  6602, 11233,   414, 29892,   322,   306,  1073,   393,   565,
         3099,   338, 17432,   491,   278,  6682,   607,  6602, 29879,   278,
         3353,   419, 29899,   869,   286,  6997, 29892,   372,   674,  1407,
         4720,   367, 10551,   287, 29889,   306,   626,  1407,  1568,   901,
        15041,   411,   278,  2779,   607,   278,  6682,  1122,   505,   373,
         1749, 29586, 29889,   306,   437,   451,  4658,   297,   278, 10901,
          310,   376, 22110,   338,  6721,  1454,  1366,  1397,  7497,   362,
         1577, 29908,   607,   338,  6091,   577,  4049,   746,   263,   716,
         5645,   338,  9129, 29889,   960,   591,   892,   304,  4480,   363,
          738,  4004,   310,   278,   970,   304,  2244,   363,  4266, 13332,
          362, 29892,   591,   881,  3117,   505,  1407,  2846,   350,  6090,
          304,  2050, 29889,  1932,  4266, 13332,   362,   338,  4433,   363,
          372,   338,  4049,  3307,  7429,   363,   263,  1583,   728,   322,
          451, 19148,  1781,  6437, 29889,  2180,   278,  1021,   931, 29892,
          746,   591,   526, 16743,   411,   263,  4004,   310,   278,  7881,
          607,   338,  4825,  1711,  2319, 29902,  2737,   304,   278,  1391,
        22543, 29892, 12012,   332,   414, 29892,   322, 29930,  1518,   272,
         2153,   448,   591,  1795,  1407,  2769,  2197,  1018,   304,   408,
        14082,  3692,   372,   553])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The input ids is tensor([    1,   278,   527,  1971,  2153, 29892,   322,   373,   393,  1298,
          306,   437,   451, 16193,   304,  1827,  1407,  1568, 29889,  1954,
         4011,  6602, 11233,   414, 29892,   322,   306,  1073,   393,   565,
         3099,   338, 17432,   491,   278,  6682,   607,  6602, 29879,   278,
         3353,   419, 29899,   869,   286,  6997, 29892,   372,   674,  1407,
         4720,   367, 10551,   287, 29889,   306,   626,  1407,  1568,   901,
        15041,   411,   278,  2779,   607,   278,  6682,  1122,   505,   373,
         1749, 29586, 29889,   306,   437,   451,  4658,   297,   278, 10901,
          310,   376, 22110,   338,  6721,  1454,  1366,  1397,  7497,   362,
         1577, 29908,   607,   338,  6091,   577,  4049,   746,   263,   716,
         5645,   338,  9129, 29889,   960,   591,   892,   304,  4480,   363,
          738,  4004,   310,   278,   970,   304,  2244,   363,  4266, 13332,
          362, 29892,   591,   881,  3117,   505,  1407,  2846,   350,  6090,
          304,  2050, 29889,  1932,  4266, 13332,   362,   338,  4433,   363,
          372,   338,  4049,  3307,  7429,   363,   263,  1583,   728,   322,
          451, 19148,  1781,  6437, 29889,  2180,   278,  1021,   931, 29892,
          746,   591,   526, 16743,   411,   263,  4004,   310,   278,  7881,
          607,   338,  4825,  1711,  2319, 29902,  2737,   304,   278,  1391,
        22543, 29892, 12012,   332,   414, 29892,   322, 29930,  1518,   272,
         2153,   448,   591,  1795,  1407,  2769,  2197,  1018,   304,   408,
        14082,  3692,   372,   553])
The input ids is tensor([    1,   278,   527,  1971,  2153, 29892,   322,   373,   393,  1298,
          306,   437,   451, 16193,   304,  1827,  1407,  1568, 29889,  1954,
         4011,  6602, 11233,   414, 29892,   322,   306,  1073,   393,   565,
         3099,   338, 17432,   491,   278,  6682,   607,  6602, 29879,   278,
         3353,   419, 29899,   869,   286,  6997, 29892,   372,   674,  1407,
         4720,   367, 10551,   287, 29889,   306,   626,  1407,  1568,   901,
        15041,   411,   278,  2779,   607,   278,  6682,  1122,   505,   373,
         1749, 29586, 29889,   306,   437,   451,  4658,   297,   278, 10901,
          310,   376, 22110,   338,  6721,  1454,  1366,  1397,  7497,   362,
         1577, 29908,   607,   338,  6091,   577,  4049,   746,   263,   716,
         5645,   338,  9129, 29889,   960,   591,   892,   304,  4480,   363,
          738,  4004,   310,   278,   970,   304,  2244,   363,  4266, 13332,
          362, 29892,   591,   881,  3117,   505,  1407,  2846,   350,  6090,
          304,  2050, 29889,  1932,  4266, 13332,   362,   338,  4433,   363,
          372,   338,  4049,  3307,  7429,   363,   263,  1583,   728,   322,
          451, 19148,  1781,  6437, 29889,  2180,   278,  1021,   931, 29892,
          746,   591,   526, 16743,   411,   263,  4004,   310,   278,  7881,
          607,   338,  4825,  1711,  2319, 29902,  2737,   304,   278,  1391,
        22543, 29892, 12012,   332,   414, 29892,   322, 29930,  1518,   272,
         2153,   448,   591,  1795,  1407,  2769,  2197,  1018,   304,   408,
        14082,  3692,   372,   553])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The input ids is tensor([    1,   304, 24354, 29915, 29879, 18873, 29889,    13,  8263, 19021,
         4810, 29906, 21104,  2894,   293, 18755,   414, 29892,   278, 18187,
          338,  1754,   338, 17362,   515,   367,  5309,  8112, 19356,   411,
          288, 28596, 29899,  6707, 15483, 29889,   450,  1121,   338,  3682,
          635,   321, 21553,   936,   322, 22747, 29885,   873,  4964, 29889,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
The input ids is tensor([    1,   304, 24354, 29915, 29879, 18873, 29889,    13,  8263, 19021,
         4810, 29906, 21104,  2894,   293, 18755,   414, 29892,   278, 18187,
          338,  1754,   338, 17362,   515,   367,  5309,  8112, 19356,   411,
          288, 28596, 29899,  6707, 15483, 29889,   450,  1121,   338,  3682,
          635,   321, 21553,   936,   322, 22747, 29885,   873,  4964, 29889,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])
The input ids is tensor([    1,   304, 24354, 29915, 29879, 18873, 29889,    13,  8263, 19021,
         4810, 29906, 21104,  2894,   293, 18755,   414, 29892,   278, 18187,
          338,  1754,   338, 17362,   515,   367,  5309,  8112, 19356,   411,
          288, 28596, 29899,  6707, 15483, 29889,   450,  1121,   338,  3682,
          635,   321, 21553,   936,   322, 22747, 29885,   873,  4964, 29889,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
The input ids is tensor([    1, 29871, 29896, 29929, 29941, 29900, 30010, 29879, 23826,  4006,
         3956,  6150,  3732,   278,  4922, 26681,   287,  8608,   515,   263,
          491, 29887,   650,  5046,   363,   596, 12561, 14837,  8497,   322,
          674,  1106, 13568,  6288,   297,   278, 14837,  8497, 20612, 29889,
           13, 29908,  3492,   505,  2289,  2715,   304,   278,  3353, 12463,
         7271,   491,  1641, 23279, 29892, 24803,  1218,   322, 14401,   368,
        10257,  3447, 14294,   322,  4780,  2675, 29889,  1334,  2609,  7726,
         1880,  3307,   310,   366, 18239, 29991,   376,  9511, 11276, 29889,
           13,  1433,  3881,   304,  1776,  1749, 14837,  8497, 18647,   322,
         5353,   738,  4266, 11780,   363,   596, 14837,  8497,  2462,   411,
          502, 29889,    13, 29949,   332, 14837,  8497, 18647,  2041, 17151,
         2168,   297, 18130, 29890,   787, 29892,   289,  1242,   322,  1652,
        11251, 10200,   800,   304,  1993,   596,  1914, 12384, 11380, 29889,
           13,  4806,   508,   884,  5957,   263, 18480,  5889,   304,   579,
         1156,   596, 25672,   304,  1207,   596,  2462,  1584,   901,  4266,
        29889,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
The input ids is tensor([    1, 29871, 29896, 29929, 29941, 29900, 30010, 29879, 23826,  4006,
         3956,  6150,  3732,   278,  4922, 26681,   287,  8608,   515,   263,
          491, 29887,   650,  5046,   363,   596, 12561, 14837,  8497,   322,
          674,  1106, 13568,  6288,   297,   278, 14837,  8497, 20612, 29889,
           13, 29908,  3492,   505,  2289,  2715,   304,   278,  3353, 12463,
         7271,   491,  1641, 23279, 29892, 24803,  1218,   322, 14401,   368,
        10257,  3447, 14294,   322,  4780,  2675, 29889,  1334,  2609,  7726,
         1880,  3307,   310,   366, 18239, 29991,   376,  9511, 11276, 29889,
           13,  1433,  3881,   304,  1776,  1749, 14837,  8497, 18647,   322,
         5353,   738,  4266, 11780,   363,   596, 14837,  8497,  2462,   411,
          502, 29889,    13, 29949,   332, 14837,  8497, 18647,  2041, 17151,
         2168,   297, 18130, 29890,   787, 29892,   289,  1242,   322,  1652,
        11251, 10200,   800,   304,  1993,   596,  1914, 12384, 11380, 29889,
           13,  4806,   508,   884,  5957,   263, 18480,  5889,   304,   579,
         1156,   596, 25672,   304,  1207,   596,  2462,  1584,   901,  4266,
        29889,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])
The input ids is tensor([    1, 29871, 29896, 29929, 29941, 29900, 30010, 29879, 23826,  4006,
         3956,  6150,  3732,   278,  4922, 26681,   287,  8608,   515,   263,
          491, 29887,   650,  5046,   363,   596, 12561, 14837,  8497,   322,
          674,  1106, 13568,  6288,   297,   278, 14837,  8497, 20612, 29889,
           13, 29908,  3492,   505,  2289,  2715,   304,   278,  3353, 12463,
         7271,   491,  1641, 23279, 29892, 24803,  1218,   322, 14401,   368,
        10257,  3447, 14294,   322,  4780,  2675, 29889,  1334,  2609,  7726,
         1880,  3307,   310,   366, 18239, 29991,   376,  9511, 11276, 29889,
           13,  1433,  3881,   304,  1776,  1749, 14837,  8497, 18647,   322,
         5353,   738,  4266, 11780,   363,   596, 14837,  8497,  2462,   411,
          502, 29889,    13, 29949,   332, 14837,  8497, 18647,  2041, 17151,
         2168,   297, 18130, 29890,   787, 29892,   289,  1242,   322,  1652,
        11251, 10200,   800,   304,  1993,   596,  1914, 12384, 11380, 29889,
           13,  4806,   508,   884,  5957,   263, 18480,  5889,   304,   579,
         1156,   596, 25672,   304,  1207,   596,  2462,  1584,   901,  4266,
        29889,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
The input ids is tensor([    1, 22222,  8525,  4127,  1090,  6480, 23136, 29892,   322,  9792,
          515,   594,  3901, 18845, 29889,    13,  4013,   338,   278,  8405,
          310, 24205, 29936,  5936,  5281,   393,  1749, 12463,  1532, 29899,
          915,   292,   338, 19632,  3368,  2861,   304,   278,   671,   310,
        27231,  5391,   470,   916,  5960,  2925, 29889,  2398, 29892,  3763,
         4788,  1403,  1218,  1438,  5960,  2925,   526,   451,  3307,   363,
          278, 27231,  5391,   293,   470,   788,   919, 29889,   383,   542,
         1558,   292,   373,   278, 18066,   267, 29892,  1020, 10859, 29892,
          470,   916, 21420,   310,   270,   952,  2220,   338,   451,  3307,
        29889,   512,  1797,   304,  8072,  9792,   591,  1818, 18720,   393,
         1749, 22986, 29933, 15202,   338,  1828,   792,   304,  1472,  1840,
        15331,   322,  6095,  5589,   358, 29889,    13,  4013,   338,   278,
         1346, 29956,   340,   383,  7168, 30024,   393,  1784,  2305,  3052,
        29889,  2688,  4049,  8569, 14419,   368,   373, 27826,   825,   338,
         9391,   322,  4418,   304, 13389,   278,  3256,  1235,  7432,  2874,
          263,  5434,   363,   263,  9150,  3234,   573,  2834, 29889,   910,
          338,   278,  8405,   310,   825,   306,  4658,   756,  8126,   592,
        12789,  1558,   287,  1549, 24081,   322,  1623, 29879, 29889, 17044,
        10672, 29892,  3632,  6393,  2264, 29892, 25074,   346,   408,  1532,
          408,   599,   310,   278,  2551,   267,   306,   505,  1063,  1999,
        11517,   411, 29892,   599])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The input ids is tensor([    1,   278,   527,  1971,  2153, 29892,   322,   373,   393,  1298,
          306,   437,   451, 16193,   304,  1827,  1407,  1568, 29889,  1954,
         4011,  6602, 11233,   414, 29892,   322,   306,  1073,   393,   565,
         3099,   338, 17432,   491,   278,  6682,   607,  6602, 29879,   278,
         3353,   419, 29899,   869,   286,  6997, 29892,   372,   674,  1407,
         4720,   367, 10551,   287, 29889,   306,   626,  1407,  1568,   901,
        15041,   411,   278,  2779,   607,   278,  6682,  1122,   505,   373,
         1749, 29586, 29889,   306,   437,   451,  4658,   297,   278, 10901,
          310,   376, 22110,   338,  6721,  1454,  1366,  1397,  7497,   362,
         1577, 29908,   607,   338,  6091,   577,  4049,   746,   263,   716,
         5645,   338,  9129, 29889,   960,   591,   892,   304,  4480,   363,
          738,  4004,   310,   278,   970,   304,  2244,   363,  4266, 13332,
          362, 29892,   591,   881,  3117,   505,  1407,  2846,   350,  6090,
          304,  2050, 29889,  1932,  4266, 13332,   362,   338,  4433,   363,
          372,   338,  4049,  3307,  7429,   363,   263,  1583,   728,   322,
          451, 19148,  1781,  6437, 29889,  2180,   278,  1021,   931, 29892,
          746,   591,   526, 16743,   411,   263,  4004,   310,   278,  7881,
          607,   338,  4825,  1711,  2319, 29902,  2737,   304,   278,  1391,
        22543, 29892, 12012,   332,   414, 29892,   322, 29930,  1518,   272,
         2153,   448,   591,  1795,  1407,  2769,  2197,  1018,   304,   408,
        14082,  3692,   372,   553])
The input ids is tensor([    1,   278,   527,  1971,  2153, 29892,   322,   373,   393,  1298,
          306,   437,   451, 16193,   304,  1827,  1407,  1568, 29889,  1954,
         4011,  6602, 11233,   414, 29892,   322,   306,  1073,   393,   565,
         3099,   338, 17432,   491,   278,  6682,   607,  6602, 29879,   278,
         3353,   419, 29899,   869,   286,  6997, 29892,   372,   674,  1407,
         4720,   367, 10551,   287, 29889,   306,   626,  1407,  1568,   901,
        15041,   411,   278,  2779,   607,   278,  6682,  1122,   505,   373,
         1749, 29586, 29889,   306,   437,   451,  4658,   297,   278, 10901,
          310,   376, 22110,   338,  6721,  1454,  1366,  1397,  7497,   362,
         1577, 29908,   607,   338,  6091,   577,  4049,   746,   263,   716,
         5645,   338,  9129, 29889,   960,   591,   892,   304,  4480,   363,
          738,  4004,   310,   278,   970,   304,  2244,   363,  4266, 13332,
          362, 29892,   591,   881,  3117,   505,  1407,  2846,   350,  6090,
          304,  2050, 29889,  1932,  4266, 13332,   362,   338,  4433,   363,
          372,   338,  4049,  3307,  7429,   363,   263,  1583,   728,   322,
          451, 19148,  1781,  6437, 29889,  2180,   278,  1021,   931, 29892,
          746,   591,   526, 16743,   411,   263,  4004,   310,   278,  7881,
          607,   338,  4825,  1711,  2319, 29902,  2737,   304,   278,  1391,
        22543, 29892, 12012,   332,   414, 29892,   322, 29930,  1518,   272,
         2153,   448,   591,  1795,  1407,  2769,  2197,  1018,   304,   408,
        14082,  3692,   372,   553])
The input ids is tensor([    1,   278,   527,  1971,  2153, 29892,   322,   373,   393,  1298,
          306,   437,   451, 16193,   304,  1827,  1407,  1568, 29889,  1954,
         4011,  6602, 11233,   414, 29892,   322,   306,  1073,   393,   565,
         3099,   338, 17432,   491,   278,  6682,   607,  6602, 29879,   278,
         3353,   419, 29899,   869,   286,  6997, 29892,   372,   674,  1407,
         4720,   367, 10551,   287, 29889,   306,   626,  1407,  1568,   901,
        15041,   411,   278,  2779,   607,   278,  6682,  1122,   505,   373,
         1749, 29586, 29889,   306,   437,   451,  4658,   297,   278, 10901,
          310,   376, 22110,   338,  6721,  1454,  1366,  1397,  7497,   362,
         1577, 29908,   607,   338,  6091,   577,  4049,   746,   263,   716,
         5645,   338,  9129, 29889,   960,   591,   892,   304,  4480,   363,
          738,  4004,   310,   278,   970,   304,  2244,   363,  4266, 13332,
          362, 29892,   591,   881,  3117,   505,  1407,  2846,   350,  6090,
          304,  2050, 29889,  1932,  4266, 13332,   362,   338,  4433,   363,
          372,   338,  4049,  3307,  7429,   363,   263,  1583,   728,   322,
          451, 19148,  1781,  6437, 29889,  2180,   278,  1021,   931, 29892,
          746,   591,   526, 16743,   411,   263,  4004,   310,   278,  7881,
          607,   338,  4825,  1711,  2319, 29902,  2737,   304,   278,  1391,
        22543, 29892, 12012,   332,   414, 29892,   322, 29930,  1518,   272,
         2153,   448,   591,  1795,  1407,  2769,  2197,  1018,   304,   408,
        14082,  3692,   372,   553])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The input ids is tensor([    1,   304, 24354, 29915, 29879, 18873, 29889,    13,  8263, 19021,
         4810, 29906, 21104,  2894,   293, 18755,   414, 29892,   278, 18187,
          338,  1754,   338, 17362,   515,   367,  5309,  8112, 19356,   411,
          288, 28596, 29899,  6707, 15483, 29889,   450,  1121,   338,  3682,
          635,   321, 21553,   936,   322, 22747, 29885,   873,  4964, 29889,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])The input ids is tensor([    1,   304, 24354, 29915, 29879, 18873, 29889,    13,  8263, 19021,
         4810, 29906, 21104,  2894,   293, 18755,   414, 29892,   278, 18187,
          338,  1754,   338, 17362,   515,   367,  5309,  8112, 19356,   411,
          288, 28596, 29899,  6707, 15483, 29889,   450,  1121,   338,  3682,
          635,   321, 21553,   936,   322, 22747, 29885,   873,  4964, 29889,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])

The input ids is tensor([    1,   304, 24354, 29915, 29879, 18873, 29889,    13,  8263, 19021,
         4810, 29906, 21104,  2894,   293, 18755,   414, 29892,   278, 18187,
          338,  1754,   338, 17362,   515,   367,  5309,  8112, 19356,   411,
          288, 28596, 29899,  6707, 15483, 29889,   450,  1121,   338,  3682,
          635,   321, 21553,   936,   322, 22747, 29885,   873,  4964, 29889,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
The input ids is tensor([    1,  8589,   297,   278, 13851, 29892,   470,  1560,   514,   278,
        14652,   267,   297,  6709, 29892,   727, 30010, 29879,   263, 14089,
          363,   366,   297, 14299, 11653, 29889,  3617,   714,   310,   278,
         3699,   445,  4723,   355,   322,  1423,   714,   777,   310,  1749,
        25448,   610,  2039,   297,   278,  4038,  1434,  6421,   297,   363,
          596,  2446,  3423,  5738, 25114, 17623,   545,  1244,   472, 25114,
         3423,  5738, 13822, 29888,  1165, 29991,    13,  3644,   366,   864,
          304,  5110,  1048,   278,  4955,   310,   278, 12886,  3362,   297,
        14299, 11653, 29892,  2343,   304, 13402, 30010, 29879,  3164,  3096,
        12788,  2671,  4815, 29889,   910, 14089, 12955,   278,  3268,   310,
          697,   310,   278, 24842, 12886,  3362,  8957,   793,   297,   365,
         2736,   265,  5127, 29889,  3645,  3786,  1549,  3979, 29892,   278,
        14089,   756,  2318,   260,  2470,   373, 24211,   322, 16340,   472,
        29871, 29896, 29896, 13862,   322, 29871, 29896, 11278, 29889,  4525,
          260,  2470,   526, 18043,   491, 27886,   261,  5133,  2153,  1058,
          508,  1234,   738,  5155,   393,   366,   505,  1048,   278, 10555,
         2671, 29889,   887,   508,   884,  6282,   278, 10555,  2671,   373,
          596,  1914,   322, 18864,   931,  3063,   472,   278, 14089, 30010,
        29879, 22586, 29889,  2860,  6282,   292,   278, 10555,  2671, 29892,
         1095,   596,  2462,   411,   263,  7251,   446,   373,   697,   310,
          278, 14089, 30010, 29879])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The input ids is tensor([    1, 29871, 29896, 29929, 29941, 29900, 30010, 29879, 23826,  4006,
         3956,  6150,  3732,   278,  4922, 26681,   287,  8608,   515,   263,
          491, 29887,   650,  5046,   363,   596, 12561, 14837,  8497,   322,
          674,  1106, 13568,  6288,   297,   278, 14837,  8497, 20612, 29889,
           13, 29908,  3492,   505,  2289,  2715,   304,   278,  3353, 12463,
         7271,   491,  1641, 23279, 29892, 24803,  1218,   322, 14401,   368,
        10257,  3447, 14294,   322,  4780,  2675, 29889,  1334,  2609,  7726,
         1880,  3307,   310,   366, 18239, 29991,   376,  9511, 11276, 29889,
           13,  1433,  3881,   304,  1776,  1749, 14837,  8497, 18647,   322,
         5353,   738,  4266, 11780,   363,   596, 14837,  8497,  2462,   411,
          502, 29889,    13, 29949,   332, 14837,  8497, 18647,  2041, 17151,
         2168,   297, 18130, 29890,   787, 29892,   289,  1242,   322,  1652,
        11251, 10200,   800,   304,  1993,   596,  1914, 12384, 11380, 29889,
           13,  4806,   508,   884,  5957,   263, 18480,  5889,   304,   579,
         1156,   596, 25672,   304,  1207,   596,  2462,  1584,   901,  4266,
        29889,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])The input ids is tensor([    1, 29871, 29896, 29929, 29941, 29900, 30010, 29879, 23826,  4006,
         3956,  6150,  3732,   278,  4922, 26681,   287,  8608,   515,   263,
          491, 29887,   650,  5046,   363,   596, 12561, 14837,  8497,   322,
          674,  1106, 13568,  6288,   297,   278, 14837,  8497, 20612, 29889,
           13, 29908,  3492,   505,  2289,  2715,   304,   278,  3353, 12463,
         7271,   491,  1641, 23279, 29892, 24803,  1218,   322, 14401,   368,
        10257,  3447, 14294,   322,  4780,  2675, 29889,  1334,  2609,  7726,
         1880,  3307,   310,   366, 18239, 29991,   376,  9511, 11276, 29889,
           13,  1433,  3881,   304,  1776,  1749, 14837,  8497, 18647,   322,
         5353,   738,  4266, 11780,   363,   596, 14837,  8497,  2462,   411,
          502, 29889,    13, 29949,   332, 14837,  8497, 18647,  2041, 17151,
         2168,   297, 18130, 29890,   787, 29892,   289,  1242,   322,  1652,
        11251, 10200,   800,   304,  1993,   596,  1914, 12384, 11380, 29889,
           13,  4806,   508,   884,  5957,   263, 18480,  5889,   304,   579,
         1156,   596, 25672,   304,  1207,   596,  2462,  1584,   901,  4266,
        29889,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])

The input ids is tensor([    1, 29871, 29896, 29929, 29941, 29900, 30010, 29879, 23826,  4006,
         3956,  6150,  3732,   278,  4922, 26681,   287,  8608,   515,   263,
          491, 29887,   650,  5046,   363,   596, 12561, 14837,  8497,   322,
          674,  1106, 13568,  6288,   297,   278, 14837,  8497, 20612, 29889,
           13, 29908,  3492,   505,  2289,  2715,   304,   278,  3353, 12463,
         7271,   491,  1641, 23279, 29892, 24803,  1218,   322, 14401,   368,
        10257,  3447, 14294,   322,  4780,  2675, 29889,  1334,  2609,  7726,
         1880,  3307,   310,   366, 18239, 29991,   376,  9511, 11276, 29889,
           13,  1433,  3881,   304,  1776,  1749, 14837,  8497, 18647,   322,
         5353,   738,  4266, 11780,   363,   596, 14837,  8497,  2462,   411,
          502, 29889,    13, 29949,   332, 14837,  8497, 18647,  2041, 17151,
         2168,   297, 18130, 29890,   787, 29892,   289,  1242,   322,  1652,
        11251, 10200,   800,   304,  1993,   596,  1914, 12384, 11380, 29889,
           13,  4806,   508,   884,  5957,   263, 18480,  5889,   304,   579,
         1156,   596, 25672,   304,  1207,   596,  2462,  1584,   901,  4266,
        29889,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
The input ids is tensor([    1,   278,   527,  1971,  2153, 29892,   322,   373,   393,  1298,
          306,   437,   451, 16193,   304,  1827,  1407,  1568, 29889,  1954,
         4011,  6602, 11233,   414, 29892,   322,   306,  1073,   393,   565,
         3099,   338, 17432,   491,   278,  6682,   607,  6602, 29879,   278,
         3353,   419, 29899,   869,   286,  6997, 29892,   372,   674,  1407,
         4720,   367, 10551,   287, 29889,   306,   626,  1407,  1568,   901,
        15041,   411,   278,  2779,   607,   278,  6682,  1122,   505,   373,
         1749, 29586, 29889,   306,   437,   451,  4658,   297,   278, 10901,
          310,   376, 22110,   338,  6721,  1454,  1366,  1397,  7497,   362,
         1577, 29908,   607,   338,  6091,   577,  4049,   746,   263,   716,
         5645,   338,  9129, 29889,   960,   591,   892,   304,  4480,   363,
          738,  4004,   310,   278,   970,   304,  2244,   363,  4266, 13332,
          362, 29892,   591,   881,  3117,   505,  1407,  2846,   350,  6090,
          304,  2050, 29889,  1932,  4266, 13332,   362,   338,  4433,   363,
          372,   338,  4049,  3307,  7429,   363,   263,  1583,   728,   322,
          451, 19148,  1781,  6437, 29889,  2180,   278,  1021,   931, 29892,
          746,   591,   526, 16743,   411,   263,  4004,   310,   278,  7881,
          607,   338,  4825,  1711,  2319, 29902,  2737,   304,   278,  1391,
        22543, 29892, 12012,   332,   414, 29892,   322, 29930,  1518,   272,
         2153,   448,   591,  1795,  1407,  2769,  2197,  1018,   304,   408,
        14082,  3692,   372,   553])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The input ids is tensor([    1,   304, 24354, 29915, 29879, 18873, 29889,    13,  8263, 19021,
         4810, 29906, 21104,  2894,   293, 18755,   414, 29892,   278, 18187,
          338,  1754,   338, 17362,   515,   367,  5309,  8112, 19356,   411,
          288, 28596, 29899,  6707, 15483, 29889,   450,  1121,   338,  3682,
          635,   321, 21553,   936,   322, 22747, 29885,   873,  4964, 29889,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
The input ids is tensor([    1, 29871, 29896, 29929, 29941, 29900, 30010, 29879, 23826,  4006,
         3956,  6150,  3732,   278,  4922, 26681,   287,  8608,   515,   263,
          491, 29887,   650,  5046,   363,   596, 12561, 14837,  8497,   322,
          674,  1106, 13568,  6288,   297,   278, 14837,  8497, 20612, 29889,
           13, 29908,  3492,   505,  2289,  2715,   304,   278,  3353, 12463,
         7271,   491,  1641, 23279, 29892, 24803,  1218,   322, 14401,   368,
        10257,  3447, 14294,   322,  4780,  2675, 29889,  1334,  2609,  7726,
         1880,  3307,   310,   366, 18239, 29991,   376,  9511, 11276, 29889,
           13,  1433,  3881,   304,  1776,  1749, 14837,  8497, 18647,   322,
         5353,   738,  4266, 11780,   363,   596, 14837,  8497,  2462,   411,
          502, 29889,    13, 29949,   332, 14837,  8497, 18647,  2041, 17151,
         2168,   297, 18130, 29890,   787, 29892,   289,  1242,   322,  1652,
        11251, 10200,   800,   304,  1993,   596,  1914, 12384, 11380, 29889,
           13,  4806,   508,   884,  5957,   263, 18480,  5889,   304,   579,
         1156,   596, 25672,   304,  1207,   596,  2462,  1584,   901,  4266,
        29889,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
The input ids is tensor([    1,   278,   527,  1971,  2153, 29892,   322,   373,   393,  1298,
          306,   437,   451, 16193,   304,  1827,  1407,  1568, 29889,  1954,
         4011,  6602, 11233,   414, 29892,   322,   306,  1073,   393,   565,
         3099,   338, 17432,   491,   278,  6682,   607,  6602, 29879,   278,
         3353,   419, 29899,   869,   286,  6997, 29892,   372,   674,  1407,
         4720,   367, 10551,   287, 29889,   306,   626,  1407,  1568,   901,
        15041,   411,   278,  2779,   607,   278,  6682,  1122,   505,   373,
         1749, 29586, 29889,   306,   437,   451,  4658,   297,   278, 10901,
          310,   376, 22110,   338,  6721,  1454,  1366,  1397,  7497,   362,
         1577, 29908,   607,   338,  6091,   577,  4049,   746,   263,   716,
         5645,   338,  9129, 29889,   960,   591,   892,   304,  4480,   363,
          738,  4004,   310,   278,   970,   304,  2244,   363,  4266, 13332,
          362, 29892,   591,   881,  3117,   505,  1407,  2846,   350,  6090,
          304,  2050, 29889,  1932,  4266, 13332,   362,   338,  4433,   363,
          372,   338,  4049,  3307,  7429,   363,   263,  1583,   728,   322,
          451, 19148,  1781,  6437, 29889,  2180,   278,  1021,   931, 29892,
          746,   591,   526, 16743,   411,   263,  4004,   310,   278,  7881,
          607,   338,  4825,  1711,  2319, 29902,  2737,   304,   278,  1391,
        22543, 29892, 12012,   332,   414, 29892,   322, 29930,  1518,   272,
         2153,   448,   591,  1795,  1407,  2769,  2197,  1018,   304,   408,
        14082,  3692,   372,   553])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
The input ids is tensor([    1,   304, 24354, 29915, 29879, 18873, 29889,    13,  8263, 19021,
         4810, 29906, 21104,  2894,   293, 18755,   414, 29892,   278, 18187,
          338,  1754,   338, 17362,   515,   367,  5309,  8112, 19356,   411,
          288, 28596, 29899,  6707, 15483, 29889,   450,  1121,   338,  3682,
          635,   321, 21553,   936,   322, 22747, 29885,   873,  4964, 29889,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
The input ids is tensor([    1, 29871, 29896, 29929, 29941, 29900, 30010, 29879, 23826,  4006,
         3956,  6150,  3732,   278,  4922, 26681,   287,  8608,   515,   263,
          491, 29887,   650,  5046,   363,   596, 12561, 14837,  8497,   322,
          674,  1106, 13568,  6288,   297,   278, 14837,  8497, 20612, 29889,
           13, 29908,  3492,   505,  2289,  2715,   304,   278,  3353, 12463,
         7271,   491,  1641, 23279, 29892, 24803,  1218,   322, 14401,   368,
        10257,  3447, 14294,   322,  4780,  2675, 29889,  1334,  2609,  7726,
         1880,  3307,   310,   366, 18239, 29991,   376,  9511, 11276, 29889,
           13,  1433,  3881,   304,  1776,  1749, 14837,  8497, 18647,   322,
         5353,   738,  4266, 11780,   363,   596, 14837,  8497,  2462,   411,
          502, 29889,    13, 29949,   332, 14837,  8497, 18647,  2041, 17151,
         2168,   297, 18130, 29890,   787, 29892,   289,  1242,   322,  1652,
        11251, 10200,   800,   304,  1993,   596,  1914, 12384, 11380, 29889,
           13,  4806,   508,   884,  5957,   263, 18480,  5889,   304,   579,
         1156,   596, 25672,   304,  1207,   596,  2462,  1584,   901,  4266,
        29889,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2])
The attention mask is tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
not using pretrained small model
not using pretrained small model
not using pretrained small model
not using pretrained small model
not using pretrained small model
not using pretrained small model
not using pretrained small model
not using pretrained small model
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
norm.weight
got here found the following key lm_head.weight
lm_head.weight
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
norm.weight
got here found the following key lm_head.weight
lm_head.weight
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
norm.weight
got here found the following key lm_head.weight
lm_head.weight
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
norm.weight
got here found the following key lm_head.weight
lm_head.weight
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
norm.weight
got here found the following key lm_head.weight
lm_head.weight
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
norm.weight
got here found the following key lm_head.weight
lm_head.weight
Error(s) in loading state_dict for SimpleSmallModel:
	Missing key(s) in state_dict: "embed_projection.weight". 
Error(s) in loading state_dict for SimpleSmallModel:
	Missing key(s) in state_dict: "embed_projection.weight". 
Error(s) in loading state_dict for SimpleSmallModel:
	Missing key(s) in state_dict: "embed_projection.weight". 
Error(s) in loading state_dict for SimpleSmallModel:
	Missing key(s) in state_dict: "embed_projection.weight". 
Error(s) in loading state_dict for SimpleSmallModel:
	Missing key(s) in state_dict: "embed_projection.weight". 
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
norm.weight
got here found the following key lm_head.weight
lm_head.weight
Error(s) in loading state_dict for SimpleSmallModel:
	Missing key(s) in state_dict: "embed_projection.weight". 
Error(s) in loading state_dict for SimpleSmallModel:
	Missing key(s) in state_dict: "embed_projection.weight". 
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
norm.weight
got here found the following key lm_head.weight
lm_head.weight
Error(s) in loading state_dict for SimpleSmallModel:
	Missing key(s) in state_dict: "embed_projection.weight". 
using new small model checkpoint
using new small model checkpoint
using new small model checkpoint
using new small model checkpoint
using new small model checkpoint
using new small model checkpoint
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
norm.weight
got here found the following key lm_head.weight
lm_head.weight
Error(s) in loading state_dict for SimpleSmallModel:
	Missing key(s) in state_dict: "embed_projection.weight". 
sliding_window_length 4 sliding_window_length 7
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
norm.weight
got here found the following key lm_head.weight
lm_head.weight
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
norm.weight
got here found the following key lm_head.weight
lm_head.weight
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
norm.weight
got here found the following key lm_head.weight
lm_head.weight
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
norm.weight
got here found the following key lm_head.weight
lm_head.weight
embed_tokens.weight
layers.0.self_attn.q_proj.weight
layers.0.self_attn.k_proj.weight
layers.0.self_attn.v_proj.weight
layers.0.self_attn.o_proj.weight
layers.0.mlp.gate_proj.weight
layers.0.mlp.up_proj.weight
layers.0.mlp.down_proj.weight
layers.0.input_layernorm.weight
layers.0.post_attention_layernorm.weight
layers.1.self_attn.q_proj.weight
layers.1.self_attn.k_proj.weight
layers.1.self_attn.v_proj.weight
layers.1.self_attn.o_proj.weight
layers.1.mlp.gate_proj.weight
layers.1.mlp.up_proj.weight
layers.1.mlp.down_proj.weight
layers.1.input_layernorm.weight
layers.1.post_attention_layernorm.weight
layers.2.self_attn.q_proj.weight
layers.2.self_attn.k_proj.weight
layers.2.self_attn.v_proj.weight
layers.2.self_attn.o_proj.weight
layers.2.mlp.gate_proj.weight
layers.2.mlp.up_proj.weight
layers.2.mlp.down_proj.weight
layers.2.input_layernorm.weight
layers.2.post_attention_layernorm.weight
layers.3.self_attn.q_proj.weight
layers.3.self_attn.k_proj.weight
layers.3.self_attn.v_proj.weight
layers.3.self_attn.o_proj.weight
layers.3.mlp.gate_proj.weight
layers.3.mlp.up_proj.weight
layers.3.mlp.down_proj.weight
layers.3.input_layernorm.weight
layers.3.post_attention_layernorm.weight
layers.4.self_attn.q_proj.weight
layers.4.self_attn.k_proj.weight
layers.4.self_attn.v_proj.weight
layers.4.self_attn.o_proj.weight
layers.4.mlp.gate_proj.weight
layers.4.mlp.up_proj.weight
layers.4.mlp.down_proj.weight
layers.4.input_layernorm.weight
layers.4.post_attention_layernorm.weight
layers.5.self_attn.q_proj.weight
layers.5.self_attn.k_proj.weight
layers.5.self_attn.v_proj.weight
layers.5.self_attn.o_proj.weight
layers.5.mlp.gate_proj.weight
layers.5.mlp.up_proj.weight
layers.5.mlp.down_proj.weight
layers.5.input_layernorm.weight
layers.5.post_attention_layernorm.weight
layers.6.self_attn.q_proj.weight
layers.6.self_attn.k_proj.weight
layers.6.self_attn.v_proj.weight
layers.6.self_attn.o_proj.weight
layers.6.mlp.gate_proj.weight
layers.6.mlp.up_proj.weight
layers.6.mlp.down_proj.weight
layers.6.input_layernorm.weight
layers.6.post_attention_layernorm.weight
layers.7.self_attn.q_proj.weight
layers.7.self_attn.k_proj.weight
layers.7.self_attn.v_proj.weight
layers.7.self_attn.o_proj.weight
layers.7.mlp.gate_proj.weight
layers.7.mlp.up_proj.weight
layers.7.mlp.down_proj.weight
layers.7.input_layernorm.weight
layers.7.post_attention_layernorm.weight
layers.8.self_attn.q_proj.weight
layers.8.self_attn.k_proj.weight
layers.8.self_attn.v_proj.weight
layers.8.self_attn.o_proj.weight
layers.8.mlp.gate_proj.weight
layers.8.mlp.up_proj.weight
layers.8.mlp.down_proj.weight
layers.8.input_layernorm.weight
layers.8.post_attention_layernorm.weight
layers.9.self_attn.q_proj.weight
layers.9.self_attn.k_proj.weight
layers.9.self_attn.v_proj.weight
layers.9.self_attn.o_proj.weight
layers.9.mlp.gate_proj.weight
layers.9.mlp.up_proj.weight
layers.9.mlp.down_proj.weight
layers.9.input_layernorm.weight
layers.9.post_attention_layernorm.weight
layers.10.self_attn.q_proj.weight
layers.10.self_attn.k_proj.weight
layers.10.self_attn.v_proj.weight
layers.10.self_attn.o_proj.weight
layers.10.mlp.gate_proj.weight
layers.10.mlp.up_proj.weight
layers.10.mlp.down_proj.weight
layers.10.input_layernorm.weight
layers.10.post_attention_layernorm.weight
layers.11.self_attn.q_proj.weight
layers.11.self_attn.k_proj.weight
layers.11.self_attn.v_proj.weight
layers.11.self_attn.o_proj.weight
layers.11.mlp.gate_proj.weight
layers.11.mlp.up_proj.weight
layers.11.mlp.down_proj.weight
layers.11.input_layernorm.weight
layers.11.post_attention_layernorm.weight
norm.weight
got here found the following key lm_head.weight
lm_head.weight
Error(s) in loading state_dict for SimpleSmallModel:
	Missing key(s) in state_dict: "embed_projection.weight". 
sliding_window_length 4 sliding_window_length 7
Error(s) in loading state_dict for SimpleSmallModel:
	Missing key(s) in state_dict: "embed_projection.weight". 
sliding_window_length 4 sliding_window_length 7
Error(s) in loading state_dict for SimpleSmallModel:
	Missing key(s) in state_dict: "embed_projection.weight". 
sliding_window_length 4 sliding_window_length 7
Error(s) in loading state_dict for SimpleSmallModel:
	Missing key(s) in state_dict: "embed_projection.weight". 
sliding_window_length 4 sliding_window_length 7
Error(s) in loading state_dict for SimpleSmallModel:
	Missing key(s) in state_dict: "embed_projection.weight". 
sliding_window_length 4 sliding_window_length 7
using new small model checkpoint
using new small model checkpoint
