Already on 'yangexp2'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/huggingface-cli", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py", line 51, in main
    service.run()
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/user.py", line 98, in run
    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 111, in login
    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 307, in _login
    raise ValueError("Invalid token passed!")
ValueError: Invalid token passed!
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:01:30:55,286 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:30:55,320 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:30:55,436 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:30:55,509 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:30:55,649 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:30:55,732 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:30:55,823 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:30:56,008 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:00,814 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:00,816 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:00,822 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:00,822 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.1, 'check': False}
2024-06-04:01:31:00,848 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:00,850 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:00,854 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:00,854 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.1, 'check': False}
2024-06-04:01:31:01,664 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:01,665 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:01,670 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:01,670 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.1, 'check': False}
2024-06-04:01:31:02,067 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:02,068 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:02,072 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:02,072 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.1, 'check': False}
2024-06-04:01:31:02,197 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:02,198 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:02,202 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:02,202 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.1, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:01:31:02,628 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:02,629 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:02,634 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:02,634 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.1, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:01:31:02,701 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:02,703 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:02,707 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:02,708 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.1, 'check': False}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:01:31:02,781 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:02,782 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:02,786 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:02,786 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.1, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:34<01:42, 34.30s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:35<01:47, 35.82s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:36<01:49, 36.34s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:37<01:51, 37.13s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:35<01:47, 35.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:36<01:49, 36.38s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:37<01:51, 37.20s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:37<01:51, 37.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:07<01:07, 33.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:09<01:08, 34.32s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:09<01:09, 34.51s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:09<01:08, 34.38s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:09<01:09, 34.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:10<01:09, 34.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:10<01:09, 34.88s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:10<01:09, 34.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:38<00:31, 31.96s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:40<00:32, 32.87s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:40<00:32, 32.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:40<00:32, 32.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:40<00:32, 32.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:41<00:32, 32.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:39<00:32, 32.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:39<00:32, 32.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:45<00:00, 21.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:45<00:00, 26.29s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:46<00:00, 22.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:46<00:00, 26.59s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:46<00:00, 22.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:46<00:00, 26.59s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:45<00:00, 21.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:45<00:00, 26.41s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:45<00:00, 22.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:45<00:00, 26.30s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:44<00:00, 22.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:45<00:00, 21.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:44<00:00, 26.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:45<00:00, 26.29s/it]

Loading checkpoint shards: 100%|██████████| 4/4 [01:46<00:00, 22.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:46<00:00, 26.54s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:29,588 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:29,590 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:29,781 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 206.68it/s] 51%|█████     | 42/83 [00:00<00:00, 206.93it/s] 76%|███████▌  | 63/83 [00:00<00:00, 205.83it/s]100%|██████████| 83/83 [00:00<00:00, 205.66it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:30,576 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:30,578 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:30,864 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 136.23it/s] 34%|███▍      | 28/82 [00:00<00:00, 137.10it/s] 51%|█████     | 42/82 [00:00<00:00, 137.65it/s] 68%|██████▊   | 56/82 [00:00<00:00, 137.80it/s] 85%|████████▌ | 70/82 [00:00<00:00, 137.89it/s]100%|██████████| 82/82 [00:00<00:00, 137.77it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:48,548 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:48,550 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:48,643 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:48,645 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:48,715 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:01:33:48,817 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 203.44it/s] 26%|██▌       | 21/82 [00:00<00:00, 205.23it/s] 51%|█████     | 42/82 [00:00<00:00, 204.21it/s] 51%|█████     | 42/82 [00:00<00:00, 205.78it/s] 77%|███████▋  | 63/82 [00:00<00:00, 204.44it/s]100%|██████████| 82/82 [00:00<00:00, 204.45it/s]
 77%|███████▋  | 63/82 [00:00<00:00, 205.99it/s]100%|██████████| 82/82 [00:00<00:00, 206.75it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:50,202 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:50,204 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:50,417 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 24%|██▍       | 20/83 [00:00<00:00, 198.31it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:50,557 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:50,559 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 48%|████▊     | 40/83 [00:00<00:00, 196.99it/s] 72%|███████▏  | 60/83 [00:00<00:00, 197.13it/s] 96%|█████████▋| 80/83 [00:00<00:00, 197.51it/s]2024-06-04:01:33:50,845 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
100%|██████████| 83/83 [00:00<00:00, 197.39it/s]
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 142.42it/s] 36%|███▌      | 30/83 [00:00<00:00, 142.90it/s] 54%|█████▍    | 45/83 [00:00<00:00, 143.51it/s] 72%|███████▏  | 60/83 [00:00<00:00, 143.78it/s] 90%|█████████ | 75/83 [00:00<00:00, 143.91it/s]100%|██████████| 83/83 [00:00<00:00, 143.75it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:01:33:58,998 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:59,070 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:59,072 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:59,247 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 205.93it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:59,440 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:59,442 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 51%|█████     | 42/83 [00:00<00:00, 207.67it/s] 76%|███████▌  | 63/83 [00:00<00:00, 207.84it/s]2024-06-04:01:33:59,612 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s]100%|██████████| 83/83 [00:00<00:00, 208.14it/s]
 21%|██        | 17/82 [00:00<00:00, 168.43it/s] 41%|████▏     | 34/82 [00:00<00:00, 135.04it/s] 59%|█████▊    | 48/82 [00:00<00:00, 127.66it/s] 74%|███████▍  | 61/82 [00:00<00:00, 122.12it/s] 90%|█████████ | 74/82 [00:00<00:00, 115.81it/s]100%|██████████| 82/82 [00:00<00:00, 121.20it/s]
2024-06-04:01:34:11,926 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:34:11,927 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:34:11,927 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:34:11,927 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:34:11,927 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:34:11,927 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:34:11,927 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:34:11,927 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:16<22:54, 16.76s/it]Running generate_until requests:   2%|▏         | 2/83 [00:32<21:28, 15.91s/it]Running generate_until requests:   4%|▎         | 3/83 [00:47<20:50, 15.63s/it]Running generate_until requests:   5%|▍         | 4/83 [01:02<20:22, 15.47s/it]Running generate_until requests:   6%|▌         | 5/83 [01:17<20:01, 15.40s/it]Running generate_until requests:   7%|▋         | 6/83 [01:33<19:38, 15.31s/it]Running generate_until requests:   8%|▊         | 7/83 [01:48<19:20, 15.26s/it]Running generate_until requests:  10%|▉         | 8/83 [02:03<19:03, 15.24s/it]Running generate_until requests:  11%|█         | 9/83 [02:18<18:48, 15.25s/it]Running generate_until requests:  12%|█▏        | 10/83 [02:33<18:31, 15.22s/it]Running generate_until requests:  13%|█▎        | 11/83 [02:48<18:13, 15.18s/it]Running generate_until requests:  14%|█▍        | 12/83 [03:03<17:55, 15.14s/it]Running generate_until requests:  16%|█▌        | 13/83 [03:18<17:37, 15.10s/it]Running generate_until requests:  17%|█▋        | 14/83 [03:33<17:18, 15.05s/it]Running generate_until requests:  18%|█▊        | 15/83 [03:48<17:00, 15.01s/it]Running generate_until requests:  19%|█▉        | 16/83 [04:03<16:43, 14.98s/it]Running generate_until requests:  20%|██        | 17/83 [04:18<16:26, 14.95s/it]Running generate_until requests:  22%|██▏       | 18/83 [04:33<16:10, 14.93s/it]Running generate_until requests:  23%|██▎       | 19/83 [04:48<15:55, 14.94s/it]Running generate_until requests:  24%|██▍       | 20/83 [05:03<15:40, 14.93s/it]Running generate_until requests:  25%|██▌       | 21/83 [05:18<15:22, 14.87s/it]Running generate_until requests:  27%|██▋       | 22/83 [05:32<15:05, 14.85s/it]Running generate_until requests:  28%|██▊       | 23/83 [05:47<14:49, 14.82s/it]Running generate_until requests:  29%|██▉       | 24/83 [06:02<14:34, 14.81s/it]Running generate_until requests:  30%|███       | 25/83 [06:17<14:18, 14.79s/it]Running generate_until requests:  31%|███▏      | 26/83 [06:31<14:02, 14.78s/it]Running generate_until requests:  33%|███▎      | 27/83 [06:46<13:46, 14.76s/it]Running generate_until requests:  34%|███▎      | 28/83 [07:01<13:31, 14.75s/it]Running generate_until requests:  35%|███▍      | 29/83 [07:16<13:16, 14.75s/it]Running generate_until requests:  36%|███▌      | 30/83 [07:30<13:01, 14.74s/it]Running generate_until requests:  37%|███▋      | 31/83 [07:45<12:46, 14.73s/it]Running generate_until requests:  39%|███▊      | 32/83 [08:00<12:31, 14.73s/it]Running generate_until requests:  40%|███▉      | 33/83 [08:14<12:16, 14.73s/it]Running generate_until requests:  41%|████      | 34/83 [08:29<12:01, 14.72s/it]Running generate_until requests:  42%|████▏     | 35/83 [08:44<11:46, 14.73s/it]Running generate_until requests:  43%|████▎     | 36/83 [08:59<11:32, 14.72s/it]Running generate_until requests:  45%|████▍     | 37/83 [09:13<11:16, 14.72s/it]Running generate_until requests:  46%|████▌     | 38/83 [09:28<11:01, 14.70s/it]Running generate_until requests:  47%|████▋     | 39/83 [09:43<10:46, 14.69s/it]Running generate_until requests:  48%|████▊     | 40/83 [09:57<10:31, 14.68s/it]Running generate_until requests:  49%|████▉     | 41/83 [10:12<10:16, 14.67s/it]Running generate_until requests:  51%|█████     | 42/83 [10:27<10:01, 14.67s/it]Running generate_until requests:  52%|█████▏    | 43/83 [10:41<09:46, 14.66s/it]Running generate_until requests:  53%|█████▎    | 44/83 [10:56<09:31, 14.66s/it]Running generate_until requests:  54%|█████▍    | 45/83 [11:11<09:16, 14.65s/it]Running generate_until requests:  55%|█████▌    | 46/83 [11:25<09:01, 14.64s/it]Running generate_until requests:  57%|█████▋    | 47/83 [11:40<08:47, 14.64s/it]Running generate_until requests:  58%|█████▊    | 48/83 [11:54<08:32, 14.64s/it]Running generate_until requests:  59%|█████▉    | 49/83 [12:09<08:17, 14.64s/it]Running generate_until requests:  60%|██████    | 50/83 [12:24<08:02, 14.62s/it]Running generate_until requests:  61%|██████▏   | 51/83 [12:38<07:47, 14.62s/it]Running generate_until requests:  63%|██████▎   | 52/83 [12:53<07:33, 14.61s/it]Running generate_until requests:  64%|██████▍   | 53/83 [13:07<07:18, 14.61s/it]Running generate_until requests:  65%|██████▌   | 54/83 [13:22<07:03, 14.60s/it]Running generate_until requests:  66%|██████▋   | 55/83 [13:37<06:48, 14.60s/it]Running generate_until requests:  67%|██████▋   | 56/83 [13:51<06:34, 14.60s/it]Running generate_until requests:  69%|██████▊   | 57/83 [14:06<06:19, 14.60s/it]Running generate_until requests:  70%|██████▉   | 58/83 [14:20<06:04, 14.59s/it]Running generate_until requests:  71%|███████   | 59/83 [14:35<05:50, 14.60s/it]Running generate_until requests:  72%|███████▏  | 60/83 [14:50<05:36, 14.61s/it]Running generate_until requests:  73%|███████▎  | 61/83 [15:04<05:21, 14.61s/it]Running generate_until requests:  75%|███████▍  | 62/83 [15:19<05:06, 14.61s/it]Running generate_until requests:  76%|███████▌  | 63/83 [15:34<04:52, 14.60s/it]Running generate_until requests:  77%|███████▋  | 64/83 [15:48<04:37, 14.60s/it]Running generate_until requests:  78%|███████▊  | 65/83 [16:03<04:22, 14.60s/it]Running generate_until requests:  80%|███████▉  | 66/83 [16:17<04:08, 14.60s/it]Running generate_until requests:  81%|████████  | 67/83 [16:32<03:53, 14.60s/it]Running generate_until requests:  82%|████████▏ | 68/83 [16:46<03:38, 14.58s/it]Running generate_until requests:  83%|████████▎ | 69/83 [17:01<03:23, 14.56s/it]Running generate_until requests:  84%|████████▍ | 70/83 [17:16<03:09, 14.56s/it]Running generate_until requests:  86%|████████▌ | 71/83 [17:30<02:54, 14.55s/it]Running generate_until requests:  87%|████████▋ | 72/83 [17:45<02:39, 14.54s/it]Running generate_until requests:  88%|████████▊ | 73/83 [17:59<02:25, 14.53s/it]Running generate_until requests:  89%|████████▉ | 74/83 [18:14<02:10, 14.53s/it]Running generate_until requests:  90%|█████████ | 75/83 [18:28<01:56, 14.52s/it]Running generate_until requests:  92%|█████████▏| 76/83 [18:43<01:41, 14.49s/it]Running generate_until requests:  93%|█████████▎| 77/83 [18:57<01:26, 14.48s/it]Running generate_until requests:  94%|█████████▍| 78/83 [19:11<01:12, 14.47s/it]Running generate_until requests:  95%|█████████▌| 79/83 [19:26<00:57, 14.46s/it]Running generate_until requests:  96%|█████████▋| 80/83 [19:40<00:43, 14.46s/it]Running generate_until requests:  98%|█████████▊| 81/83 [19:55<00:28, 14.46s/it]Running generate_until requests:  99%|█████████▉| 82/83 [20:09<00:14, 14.46s/it]Running generate_until requests: 100%|██████████| 83/83 [20:23<00:00, 14.40s/it]Running generate_until requests: 100%|██████████| 83/83 [20:23<00:00, 14.75s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:01:59:40,365 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:40,366 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:40,366 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:40,538 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:40,555 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:40,797 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:41,011 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:41,247 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:45,023 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:45,024 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:45,028 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:45,028 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.2, 'check': False}
2024-06-04:01:59:45,445 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:45,447 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:45,451 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:45,451 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.2, 'check': False}
2024-06-04:01:59:45,940 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:45,941 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:45,945 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:45,945 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.2, 'check': False}
2024-06-04:01:59:46,087 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:46,088 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:46,093 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:46,093 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.2, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:01:59:47,798 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:47,800 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:47,804 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:47,805 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.2, 'check': False}
2024-06-04:01:59:48,096 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:48,097 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:48,102 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:48,102 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.2, 'check': False}
2024-06-04:01:59:48,288 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:48,289 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:48,292 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:48,292 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.2, 'check': False}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.95s/it]2024-06-04:01:59:48,490 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:48,491 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:48,495 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:48,495 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.2, 'check': False}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.89s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.90s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.83s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.88s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.59s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.54s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.55s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.21s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.06s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.07s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.09s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:00:35,721 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:00:35,722 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:00:35,937 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 194.94it/s] 49%|████▉     | 40/82 [00:00<00:00, 195.68it/s] 73%|███████▎  | 60/82 [00:00<00:00, 195.58it/s] 98%|█████████▊| 80/82 [00:00<00:00, 195.68it/s]100%|██████████| 82/82 [00:00<00:00, 195.54it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:00:37,878 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:00:37,880 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:00:38,048 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 207.47it/s] 51%|█████     | 42/82 [00:00<00:00, 207.70it/s] 77%|███████▋  | 63/82 [00:00<00:00, 207.96it/s]100%|██████████| 82/82 [00:00<00:00, 208.03it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:00:57,686 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:00:57,687 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:00:57,888 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 206.22it/s] 51%|█████     | 42/82 [00:00<00:00, 207.47it/s] 77%|███████▋  | 63/82 [00:00<00:00, 207.93it/s]100%|██████████| 82/82 [00:00<00:00, 208.14it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:01:00,691 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:01:00,767 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:00,769 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:00,936 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 209.86it/s] 52%|█████▏    | 43/83 [00:00<00:00, 210.49it/s] 78%|███████▊  | 65/83 [00:00<00:00, 210.62it/s]100%|██████████| 83/83 [00:00<00:00, 210.56it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:01:13,322 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:13,324 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:01:13,398 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:13,400 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:01:13,447 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:13,450 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:13,604 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:02:01:13,706 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
 18%|█▊        | 15/83 [00:00<00:00, 147.74it/s]  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:02:01:13,767 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 36%|███▌      | 30/83 [00:00<00:00, 145.40it/s] 17%|█▋        | 14/83 [00:00<00:00, 133.15it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 17%|█▋        | 14/82 [00:00<00:00, 135.45it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:01:13,923 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:13,925 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 58%|█████▊    | 48/83 [00:00<00:00, 159.90it/s] 34%|███▎      | 28/83 [00:00<00:00, 134.23it/s] 34%|███▍      | 28/82 [00:00<00:00, 134.50it/s] 51%|█████     | 42/83 [00:00<00:00, 134.86it/s] 78%|███████▊  | 65/83 [00:00<00:00, 151.12it/s] 51%|█████     | 42/82 [00:00<00:00, 136.02it/s] 67%|██████▋   | 56/83 [00:00<00:00, 136.05it/s] 98%|█████████▊| 81/83 [00:00<00:00, 143.70it/s]100%|██████████| 83/83 [00:00<00:00, 146.76it/s]
 68%|██████▊   | 56/82 [00:00<00:00, 136.15it/s] 84%|████████▍ | 70/83 [00:00<00:00, 135.74it/s]2024-06-04:02:01:14,258 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 85%|████████▌ | 70/82 [00:00<00:00, 135.67it/s]100%|██████████| 83/83 [00:00<00:00, 135.35it/s]
 17%|█▋        | 14/83 [00:00<00:00, 132.14it/s]100%|██████████| 82/82 [00:00<00:00, 135.12it/s]
 34%|███▎      | 28/83 [00:00<00:00, 123.86it/s] 49%|████▉     | 41/83 [00:00<00:00, 121.47it/s] 65%|██████▌   | 54/83 [00:00<00:00, 109.84it/s] 80%|███████▉  | 66/83 [00:00<00:00, 109.52it/s] 94%|█████████▍| 78/83 [00:00<00:00, 109.33it/s]100%|██████████| 83/83 [00:00<00:00, 112.71it/s]
2024-06-04:02:01:26,635 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:01:26,635 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:01:26,635 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:01:26,635 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:01:26,636 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:01:26,636 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:02:01:26,636 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:01:26,638 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:11<15:18, 11.20s/it]Running generate_until requests:   2%|▏         | 2/83 [00:21<14:21, 10.64s/it]Running generate_until requests:   4%|▎         | 3/83 [00:25<10:08,  7.60s/it]Running generate_until requests:   5%|▍         | 4/83 [00:35<11:21,  8.62s/it]Running generate_until requests:   6%|▌         | 5/83 [00:45<11:56,  9.18s/it]Running generate_until requests:   7%|▋         | 6/83 [00:55<12:10,  9.49s/it]Running generate_until requests:   8%|▊         | 7/83 [01:05<12:15,  9.68s/it]Running generate_until requests:  10%|▉         | 8/83 [01:16<12:15,  9.80s/it]Running generate_until requests:  11%|█         | 9/83 [01:21<10:25,  8.46s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:27<09:20,  7.68s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:37<10:05,  8.41s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:47<10:33,  8.92s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:50<08:15,  7.08s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:00<09:09,  7.97s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:03<07:29,  6.62s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:13<08:31,  7.63s/it]Running generate_until requests:  20%|██        | 17/83 [02:23<09:10,  8.34s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:33<09:34,  8.84s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:43<09:47,  9.18s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:53<09:53,  9.43s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:58<08:10,  7.91s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:08<08:38,  8.50s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:17<08:53,  8.90s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:27<09:00,  9.17s/it]Running generate_until requests:  30%|███       | 25/83 [03:37<09:03,  9.37s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:47<09:01,  9.50s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:57<08:57,  9.60s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:00<06:57,  7.60s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:09<07:26,  8.26s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:19<07:42,  8.72s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:29<07:50,  9.05s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:39<07:53,  9.28s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:49<07:52,  9.44s/it]Running generate_until requests:  41%|████      | 34/83 [04:59<07:47,  9.55s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:02<06:07,  7.66s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:12<06:30,  8.30s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:21<06:42,  8.75s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:26<05:32,  7.38s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:35<05:56,  8.09s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:45<06:09,  8.58s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:55<06:14,  8.92s/it]Running generate_until requests:  51%|█████     | 42/83 [06:04<06:15,  9.17s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:14<06:13,  9.34s/it]Running generate_until requests:  53%|█████▎    | 44/83 [06:19<05:07,  7.88s/it]Running generate_until requests:  54%|█████▍    | 45/83 [06:28<05:20,  8.43s/it]Running generate_until requests:  55%|█████▌    | 46/83 [06:38<05:26,  8.82s/it]Running generate_until requests:  57%|█████▋    | 47/83 [06:48<05:27,  9.09s/it]Running generate_until requests:  58%|█████▊    | 48/83 [06:58<05:24,  9.28s/it]Running generate_until requests:  59%|█████▉    | 49/83 [07:07<05:19,  9.41s/it]Running generate_until requests:  60%|██████    | 50/83 [07:11<04:10,  7.59s/it]Running generate_until requests:  61%|██████▏   | 51/83 [07:20<04:22,  8.22s/it]Running generate_until requests:  63%|██████▎   | 52/83 [07:30<04:28,  8.67s/it]Running generate_until requests:  64%|██████▍   | 53/83 [07:40<04:29,  8.97s/it]Running generate_until requests:  65%|██████▌   | 54/83 [07:43<03:29,  7.23s/it]Running generate_until requests:  66%|██████▋   | 55/83 [07:53<03:42,  7.96s/it]Running generate_until requests:  67%|██████▋   | 56/83 [08:02<03:48,  8.47s/it]Running generate_until requests:  69%|██████▊   | 57/83 [08:12<03:49,  8.84s/it]Running generate_until requests:  70%|██████▉   | 58/83 [08:22<03:47,  9.09s/it]Running generate_until requests:  71%|███████   | 59/83 [08:31<03:42,  9.26s/it]Running generate_until requests:  72%|███████▏  | 60/83 [08:41<03:36,  9.40s/it]Running generate_until requests:  73%|███████▎  | 61/83 [08:51<03:28,  9.49s/it]Running generate_until requests:  75%|███████▍  | 62/83 [08:53<02:31,  7.23s/it]Running generate_until requests:  76%|███████▌  | 63/83 [09:02<02:39,  7.96s/it]Running generate_until requests:  77%|███████▋  | 64/83 [09:12<02:41,  8.47s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:22<02:39,  8.83s/it]Running generate_until requests:  80%|███████▉  | 66/83 [09:31<02:34,  9.08s/it]Running generate_until requests:  81%|████████  | 67/83 [09:41<02:28,  9.26s/it]Running generate_until requests:  82%|████████▏ | 68/83 [09:43<01:48,  7.23s/it]Running generate_until requests:  83%|████████▎ | 69/83 [09:53<01:51,  7.94s/it]Running generate_until requests:  84%|████████▍ | 70/83 [10:03<01:49,  8.43s/it]Running generate_until requests:  86%|████████▌ | 71/83 [10:06<01:22,  6.85s/it]Running generate_until requests:  87%|████████▋ | 72/83 [10:15<01:24,  7.67s/it]Running generate_until requests:  88%|████████▊ | 73/83 [10:25<01:22,  8.25s/it]Running generate_until requests:  89%|████████▉ | 74/83 [10:35<01:17,  8.66s/it]Running generate_until requests:  90%|█████████ | 75/83 [10:37<00:54,  6.78s/it]Running generate_until requests:  92%|█████████▏| 76/83 [10:47<00:53,  7.60s/it]Running generate_until requests:  93%|█████████▎| 77/83 [10:56<00:49,  8.17s/it]Running generate_until requests:  94%|█████████▍| 78/83 [10:58<00:32,  6.42s/it]Running generate_until requests:  95%|█████████▌| 79/83 [11:08<00:29,  7.34s/it]Running generate_until requests:  96%|█████████▋| 80/83 [11:17<00:23,  7.99s/it]Running generate_until requests:  98%|█████████▊| 81/83 [11:27<00:16,  8.44s/it]Running generate_until requests:  99%|█████████▉| 82/83 [11:36<00:08,  8.75s/it]Running generate_until requests: 100%|██████████| 83/83 [11:46<00:00,  8.97s/it]Running generate_until requests: 100%|██████████| 83/83 [11:46<00:00,  8.51s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:02:21:56,630 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:21:56,634 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:21:56,644 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:21:56,683 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:21:56,810 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:21:56,829 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:21:56,832 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:21:56,901 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:22:01,545 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:22:01,546 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:22:01,551 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:22:01,551 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.3, 'check': False}
2024-06-04:02:22:01,714 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:22:01,716 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:22:01,721 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:22:01,721 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.3, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:22:03,522 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:22:03,523 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:22:03,528 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:22:03,528 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.3, 'check': False}
2024-06-04:02:22:03,606 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:22:03,608 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:22:03,612 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:22:03,612 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.3, 'check': False}
2024-06-04:02:22:03,664 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:22:03,665 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:22:03,670 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:22:03,670 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.3, 'check': False}
2024-06-04:02:22:03,701 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:22:03,702 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:22:03,706 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:22:03,706 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.3, 'check': False}
2024-06-04:02:22:03,803 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:22:03,804 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:22:03,808 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:22:03,808 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.3, 'check': False}
2024-06-04:02:22:03,871 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:22:03,872 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:22:03,875 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:22:03,875 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.3, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.98s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  2.00s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.89s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.62s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  3.00s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.70s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.59s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.61s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.53s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.49s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.23s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.15s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.03s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.04s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.11s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:22:49,741 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:22:49,743 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:22:49,921 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 201.46it/s] 51%|█████     | 42/83 [00:00<00:00, 202.66it/s] 76%|███████▌  | 63/83 [00:00<00:00, 202.61it/s]100%|██████████| 83/83 [00:00<00:00, 202.71it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:22:56,018 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:22:56,020 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:22:56,297 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 136.98it/s] 34%|███▎      | 28/83 [00:00<00:00, 138.00it/s] 51%|█████     | 42/83 [00:00<00:00, 138.18it/s] 67%|██████▋   | 56/83 [00:00<00:00, 132.60it/s] 84%|████████▍ | 70/83 [00:00<00:00, 134.76it/s]100%|██████████| 83/83 [00:00<00:00, 135.89it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:23:06,401 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:06,403 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:06,567 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 203.66it/s] 51%|█████     | 42/82 [00:00<00:00, 204.95it/s] 77%|███████▋  | 63/82 [00:00<00:00, 205.67it/s]100%|██████████| 82/82 [00:00<00:00, 205.62it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:23:11,279 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:11,282 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:11,472 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 202.82it/s] 51%|█████     | 42/82 [00:00<00:00, 205.41it/s] 77%|███████▋  | 63/82 [00:00<00:00, 205.88it/s]100%|██████████| 82/82 [00:00<00:00, 206.02it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:23:15,330 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:23:15,374 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:15,376 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:23:15,402 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:15,404 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:15,572 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
2024-06-04:02:23:15,582 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/83 [00:00<?, ?it/s]  0%|          | 0/82 [00:00<?, ?it/s] 24%|██▍       | 20/83 [00:00<00:00, 197.07it/s] 24%|██▍       | 20/82 [00:00<00:00, 196.51it/s] 48%|████▊     | 40/83 [00:00<00:00, 196.60it/s] 49%|████▉     | 40/82 [00:00<00:00, 196.13it/s] 72%|███████▏  | 60/83 [00:00<00:00, 197.50it/s] 73%|███████▎  | 60/82 [00:00<00:00, 196.84it/s] 96%|█████████▋| 80/83 [00:00<00:00, 198.19it/s]100%|██████████| 83/83 [00:00<00:00, 197.82it/s]
 98%|█████████▊| 80/82 [00:00<00:00, 197.32it/s]100%|██████████| 82/82 [00:00<00:00, 194.90it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:23:31,543 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:31,545 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:23:31,731 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:31,733 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:31,997 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 104.45it/s]2024-06-04:02:23:32,177 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 27%|██▋       | 22/83 [00:00<00:00, 105.05it/s] 13%|█▎        | 11/82 [00:00<00:00, 106.42it/s] 40%|███▉      | 33/83 [00:00<00:00, 105.73it/s] 27%|██▋       | 22/82 [00:00<00:00, 106.83it/s] 54%|█████▍    | 45/83 [00:00<00:00, 109.86it/s] 40%|████      | 33/82 [00:00<00:00, 106.83it/s] 71%|███████   | 59/83 [00:00<00:00, 118.80it/s] 54%|█████▎    | 44/82 [00:00<00:00, 107.04it/s] 88%|████████▊ | 73/83 [00:00<00:00, 123.25it/s] 67%|██████▋   | 55/82 [00:00<00:00, 107.14it/s]100%|██████████| 83/83 [00:00<00:00, 118.43it/s]
 80%|████████  | 66/82 [00:00<00:00, 107.24it/s] 94%|█████████▍| 77/82 [00:00<00:00, 107.20it/s]100%|██████████| 82/82 [00:00<00:00, 107.00it/s]
2024-06-04:02:23:40,961 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:23:40,961 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:23:40,961 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:23:40,961 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:23:40,961 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:23:40,961 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:23:40,962 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:02:23:40,964 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:16<22:48, 16.69s/it]Running generate_until requests:   2%|▏         | 2/83 [00:29<19:23, 14.37s/it]Running generate_until requests:   4%|▎         | 3/83 [00:44<19:43, 14.79s/it]Running generate_until requests:   5%|▍         | 4/83 [00:50<14:44, 11.19s/it]Running generate_until requests:   6%|▌         | 5/83 [00:57<12:37,  9.71s/it]Running generate_until requests:   7%|▋         | 6/83 [01:12<14:47, 11.52s/it]Running generate_until requests:   8%|▊         | 7/83 [01:21<13:30, 10.66s/it]Running generate_until requests:  10%|▉         | 8/83 [01:28<11:58,  9.58s/it]Running generate_until requests:  11%|█         | 9/83 [01:35<10:34,  8.58s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:49<12:46, 10.51s/it]Running generate_until requests:  13%|█▎        | 11/83 [02:04<14:16, 11.89s/it]Running generate_until requests:  14%|█▍        | 12/83 [02:19<15:12, 12.85s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:34<15:45, 13.51s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:40<12:40, 11.03s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:46<10:52,  9.60s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:55<10:38,  9.52s/it]Running generate_until requests:  20%|██        | 17/83 [03:01<09:01,  8.21s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:08<08:42,  8.03s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:23<10:43, 10.06s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:29<09:15,  8.82s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:33<07:39,  7.41s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:40<07:19,  7.21s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:45<06:45,  6.76s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:52<06:28,  6.58s/it]Running generate_until requests:  30%|███       | 25/83 [04:06<08:43,  9.02s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:11<07:18,  7.70s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:16<06:24,  6.86s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:26<07:05,  7.73s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:30<06:00,  6.67s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:34<05:16,  5.97s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:38<04:43,  5.45s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:48<05:44,  6.75s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:58<06:22,  7.65s/it]Running generate_until requests:  41%|████      | 34/83 [05:08<06:46,  8.29s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:11<05:30,  6.88s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:16<04:51,  6.20s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:19<04:02,  5.28s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:24<03:49,  5.11s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:27<03:13,  4.40s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:30<02:50,  3.98s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:33<02:37,  3.75s/it]Running generate_until requests:  51%|█████     | 42/83 [05:42<03:47,  5.54s/it]Running generate_until requests:  52%|█████▏    | 43/83 [05:46<03:16,  4.91s/it]Running generate_until requests:  53%|█████▎    | 44/83 [05:49<02:53,  4.44s/it]Running generate_until requests:  54%|█████▍    | 45/83 [05:53<02:39,  4.19s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:56<02:20,  3.81s/it]Running generate_until requests:  57%|█████▋    | 47/83 [06:05<03:20,  5.57s/it]Running generate_until requests:  58%|█████▊    | 48/83 [06:10<03:03,  5.26s/it]Running generate_until requests:  59%|█████▉    | 49/83 [06:13<02:32,  4.48s/it]Running generate_until requests:  60%|██████    | 50/83 [06:17<02:24,  4.37s/it]Running generate_until requests:  61%|██████▏   | 51/83 [06:26<03:11,  5.97s/it]Running generate_until requests:  63%|██████▎   | 52/83 [06:29<02:34,  5.00s/it]Running generate_until requests:  64%|██████▍   | 53/83 [06:32<02:13,  4.45s/it]Running generate_until requests:  65%|██████▌   | 54/83 [06:35<01:50,  3.82s/it]Running generate_until requests:  66%|██████▋   | 55/83 [06:38<01:43,  3.71s/it]Running generate_until requests:  67%|██████▋   | 56/83 [06:48<02:28,  5.49s/it]Running generate_until requests:  69%|██████▊   | 57/83 [06:57<02:55,  6.74s/it]Running generate_until requests:  70%|██████▉   | 58/83 [07:02<02:33,  6.12s/it]Running generate_until requests:  71%|███████   | 59/83 [07:12<02:52,  7.17s/it]Running generate_until requests:  72%|███████▏  | 60/83 [07:15<02:17,  5.99s/it]Running generate_until requests:  73%|███████▎  | 61/83 [07:19<01:56,  5.28s/it]Running generate_until requests:  75%|███████▍  | 62/83 [07:21<01:34,  4.49s/it]Running generate_until requests:  76%|███████▌  | 63/83 [07:24<01:21,  4.06s/it]Running generate_until requests:  77%|███████▋  | 64/83 [07:27<01:11,  3.76s/it]Running generate_until requests:  78%|███████▊  | 65/83 [07:31<01:06,  3.69s/it]Running generate_until requests:  80%|███████▉  | 66/83 [07:35<01:06,  3.91s/it]Running generate_until requests:  81%|████████  | 67/83 [07:39<01:00,  3.76s/it]Running generate_until requests:  82%|████████▏ | 68/83 [07:48<01:22,  5.50s/it]Running generate_until requests:  83%|████████▎ | 69/83 [07:58<01:33,  6.71s/it]Running generate_until requests:  84%|████████▍ | 70/83 [08:02<01:15,  5.79s/it]Running generate_until requests:  86%|████████▌ | 71/83 [08:04<00:57,  4.83s/it]Running generate_until requests:  87%|████████▋ | 72/83 [08:07<00:46,  4.26s/it]Running generate_until requests:  88%|████████▊ | 73/83 [08:10<00:40,  4.01s/it]Running generate_until requests:  89%|████████▉ | 74/83 [08:20<00:51,  5.67s/it]Running generate_until requests:  90%|█████████ | 75/83 [08:23<00:39,  4.96s/it]Running generate_until requests:  92%|█████████▏| 76/83 [08:26<00:29,  4.21s/it]Running generate_until requests:  93%|█████████▎| 77/83 [08:29<00:23,  3.99s/it]Running generate_until requests:  94%|█████████▍| 78/83 [08:33<00:18,  3.80s/it]Running generate_until requests:  95%|█████████▌| 79/83 [08:35<00:13,  3.44s/it]Running generate_until requests:  96%|█████████▋| 80/83 [08:38<00:09,  3.30s/it]Running generate_until requests:  98%|█████████▊| 81/83 [08:42<00:07,  3.55s/it]Running generate_until requests:  99%|█████████▉| 82/83 [08:52<00:05,  5.32s/it]Running generate_until requests: 100%|██████████| 83/83 [08:56<00:00,  4.92s/it]Running generate_until requests: 100%|██████████| 83/83 [08:56<00:00,  6.46s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:02:37:06,771 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:06,772 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:06,773 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:06,784 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:07,034 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:07,214 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:07,222 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:07,338 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:11,820 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:11,821 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:11,826 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:11,826 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': False}
2024-06-04:02:37:11,986 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:11,987 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:11,992 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:11,993 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': False}
2024-06-04:02:37:12,473 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:12,475 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:12,480 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:12,480 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': False}
2024-06-04:02:37:13,134 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:13,135 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:13,140 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:13,140 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:37:14,223 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:14,225 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:14,229 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:14,229 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': False}
2024-06-04:02:37:14,453 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:14,455 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:14,458 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:14,459 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:14,459 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': False}
2024-06-04:02:37:14,460 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:14,464 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:14,464 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:37:14,702 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:14,703 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:14,707 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:14,707 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.4, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.94s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.96s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.63s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.88s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.89s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.78s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.61s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.60s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.58s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.54s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.12s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.07s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.06s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.03s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.08s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:38:01,638 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:01,640 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:38:01,815 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:38:01,885 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:01,887 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 24%|██▍       | 20/82 [00:00<00:00, 199.82it/s] 49%|████▉     | 40/82 [00:00<00:00, 198.72it/s]2024-06-04:02:38:02,091 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 73%|███████▎  | 60/82 [00:00<00:00, 198.36it/s] 24%|██▍       | 20/83 [00:00<00:00, 197.15it/s] 98%|█████████▊| 80/82 [00:00<00:00, 198.42it/s]100%|██████████| 82/82 [00:00<00:00, 198.50it/s]
 49%|████▉     | 41/83 [00:00<00:00, 201.52it/s] 75%|███████▍  | 62/83 [00:00<00:00, 204.62it/s]100%|██████████| 83/83 [00:00<00:00, 206.21it/s]100%|██████████| 83/83 [00:00<00:00, 204.59it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:38:23,057 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:23,060 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:23,231 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 205.35it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 51%|█████     | 42/82 [00:00<00:00, 205.35it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:38:23,508 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:23,510 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 77%|███████▋  | 63/82 [00:00<00:00, 207.11it/s]100%|██████████| 82/82 [00:00<00:00, 206.90it/s]
2024-06-04:02:38:23,669 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 209.82it/s] 52%|█████▏    | 43/83 [00:00<00:00, 210.16it/s] 78%|███████▊  | 65/83 [00:00<00:00, 210.55it/s]100%|██████████| 83/83 [00:00<00:00, 210.55it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:38:39,222 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:38:39,293 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:39,295 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:38:39,576 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:38:39,610 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:39,612 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 18%|█▊        | 15/83 [00:00<00:00, 142.67it/s] 36%|███▌      | 30/83 [00:00<00:00, 144.66it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:38:39,880 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 57%|█████▋    | 47/83 [00:00<00:00, 155.82it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:38:39,911 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:39,913 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 17%|█▋        | 14/82 [00:00<00:00, 139.88it/s] 76%|███████▌  | 63/83 [00:00<00:00, 147.71it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 38%|███▊      | 31/82 [00:00<00:00, 154.31it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:38:40,119 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:40,121 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 94%|█████████▍| 78/83 [00:00<00:00, 141.46it/s]100%|██████████| 83/83 [00:00<00:00, 143.44it/s]
 57%|█████▋    | 47/82 [00:00<00:00, 145.90it/s]2024-06-04:02:38:40,242 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 76%|███████▌  | 62/82 [00:00<00:00, 140.04it/s] 14%|█▍        | 12/83 [00:00<00:00, 117.83it/s]2024-06-04:02:38:40,423 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 94%|█████████▍| 77/82 [00:00<00:00, 137.08it/s] 29%|██▉       | 24/83 [00:00<00:00, 112.81it/s]100%|██████████| 82/82 [00:00<00:00, 140.19it/s]
 16%|█▌        | 13/82 [00:00<00:00, 129.55it/s] 45%|████▍     | 37/83 [00:00<00:00, 118.99it/s] 32%|███▏      | 26/82 [00:00<00:00, 117.92it/s] 61%|██████▏   | 51/83 [00:00<00:00, 126.40it/s] 46%|████▋     | 38/82 [00:00<00:00, 114.08it/s] 78%|███████▊  | 65/83 [00:00<00:00, 130.42it/s] 61%|██████    | 50/82 [00:00<00:00, 111.84it/s] 95%|█████████▌| 79/83 [00:00<00:00, 132.26it/s]100%|██████████| 83/83 [00:00<00:00, 127.85it/s]
 76%|███████▌  | 62/82 [00:00<00:00, 111.17it/s] 90%|█████████ | 74/82 [00:00<00:00, 110.75it/s]100%|██████████| 82/82 [00:00<00:00, 112.64it/s]
2024-06-04:02:38:52,693 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:38:52,693 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:38:52,693 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:38:52,694 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:38:52,694 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:38:52,694 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:02:38:52,697 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:38:52,709 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:09<12:35,  9.21s/it]Running generate_until requests:   2%|▏         | 2/83 [00:19<13:32, 10.04s/it]Running generate_until requests:   4%|▎         | 3/83 [00:26<11:08,  8.36s/it]Running generate_until requests:   5%|▍         | 4/83 [00:32<09:51,  7.49s/it]Running generate_until requests:   6%|▌         | 5/83 [00:38<09:14,  7.11s/it]Running generate_until requests:   7%|▋         | 6/83 [00:53<12:32,  9.77s/it]Running generate_until requests:   8%|▊         | 7/83 [01:01<11:31,  9.09s/it]Running generate_until requests:  10%|▉         | 8/83 [01:07<10:06,  8.09s/it]Running generate_until requests:  11%|█         | 9/83 [01:14<09:32,  7.74s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:29<12:04,  9.92s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:35<10:44,  8.95s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:45<10:43,  9.06s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:48<08:39,  7.42s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:53<07:37,  6.64s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:02<08:06,  7.15s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:12<09:04,  8.13s/it]Running generate_until requests:  20%|██        | 17/83 [02:19<08:39,  7.87s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:26<08:03,  7.43s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:32<07:30,  7.04s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:41<08:12,  7.82s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:48<07:51,  7.60s/it]Running generate_until requests:  27%|██▋       | 22/83 [02:54<07:05,  6.98s/it]Running generate_until requests:  28%|██▊       | 23/83 [02:59<06:23,  6.39s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:04<06:01,  6.12s/it]Running generate_until requests:  30%|███       | 25/83 [03:19<08:23,  8.68s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:24<07:01,  7.40s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:30<06:44,  7.22s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:36<06:10,  6.74s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:51<08:12,  9.12s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:56<07:08,  8.09s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:01<06:07,  7.07s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:07<05:48,  6.83s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:22<07:37,  9.15s/it]Running generate_until requests:  41%|████      | 34/83 [04:27<06:28,  7.93s/it]Running generate_until requests:  42%|████▏     | 35/83 [04:32<05:34,  6.96s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:36<04:55,  6.28s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:41<04:29,  5.85s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:56<06:20,  8.44s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:00<05:11,  7.09s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:05<04:41,  6.56s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:10<04:19,  6.18s/it]Running generate_until requests:  51%|█████     | 42/83 [05:25<05:57,  8.72s/it]Running generate_until requests:  52%|█████▏    | 43/83 [05:31<05:12,  7.82s/it]Running generate_until requests:  53%|█████▎    | 44/83 [05:37<04:52,  7.50s/it]Running generate_until requests:  54%|█████▍    | 45/83 [05:42<04:17,  6.78s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:46<03:34,  5.80s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:51<03:25,  5.70s/it]Running generate_until requests:  58%|█████▊    | 48/83 [05:58<03:26,  5.89s/it]Running generate_until requests:  59%|█████▉    | 49/83 [06:01<02:55,  5.18s/it]Running generate_until requests:  60%|██████    | 50/83 [06:06<02:43,  4.96s/it]Running generate_until requests:  61%|██████▏   | 51/83 [06:20<04:10,  7.83s/it]Running generate_until requests:  63%|██████▎   | 52/83 [06:24<03:22,  6.52s/it]Running generate_until requests:  64%|██████▍   | 53/83 [06:28<02:57,  5.91s/it]Running generate_until requests:  65%|██████▌   | 54/83 [06:32<02:31,  5.24s/it]Running generate_until requests:  66%|██████▋   | 55/83 [06:36<02:21,  5.05s/it]Running generate_until requests:  67%|██████▋   | 56/83 [06:42<02:18,  5.12s/it]Running generate_until requests:  69%|██████▊   | 57/83 [06:51<02:46,  6.40s/it]Running generate_until requests:  70%|██████▉   | 58/83 [06:58<02:46,  6.66s/it]Running generate_until requests:  71%|███████   | 59/83 [07:02<02:17,  5.73s/it]Running generate_until requests:  72%|███████▏  | 60/83 [07:06<02:01,  5.29s/it]Running generate_until requests:  73%|███████▎  | 61/83 [07:12<02:02,  5.55s/it]Running generate_until requests:  75%|███████▍  | 62/83 [07:15<01:39,  4.72s/it]Running generate_until requests:  76%|███████▌  | 63/83 [07:18<01:23,  4.16s/it]Running generate_until requests:  77%|███████▋  | 64/83 [07:25<01:34,  4.96s/it]Running generate_until requests:  78%|███████▊  | 65/83 [07:29<01:23,  4.63s/it]Running generate_until requests:  80%|███████▉  | 66/83 [07:31<01:08,  4.05s/it]Running generate_until requests:  81%|████████  | 67/83 [07:35<01:02,  3.92s/it]Running generate_until requests:  82%|████████▏ | 68/83 [07:38<00:55,  3.72s/it]Running generate_until requests:  83%|████████▎ | 69/83 [07:43<00:58,  4.17s/it]Running generate_until requests:  84%|████████▍ | 70/83 [07:48<00:56,  4.35s/it]Running generate_until requests:  86%|████████▌ | 71/83 [07:52<00:49,  4.14s/it]Running generate_until requests:  87%|████████▋ | 72/83 [07:56<00:45,  4.17s/it]Running generate_until requests:  88%|████████▊ | 73/83 [08:00<00:41,  4.17s/it]Running generate_until requests:  89%|████████▉ | 74/83 [08:05<00:39,  4.36s/it]Running generate_until requests:  90%|█████████ | 75/83 [08:12<00:40,  5.01s/it]Running generate_until requests:  92%|█████████▏| 76/83 [08:15<00:31,  4.46s/it]Running generate_until requests:  93%|█████████▎| 77/83 [08:19<00:25,  4.23s/it]Running generate_until requests:  94%|█████████▍| 78/83 [08:22<00:19,  3.92s/it]Running generate_until requests:  95%|█████████▌| 79/83 [08:26<00:15,  3.90s/it]Running generate_until requests:  96%|█████████▋| 80/83 [08:30<00:12,  4.03s/it]Running generate_until requests:  98%|█████████▊| 81/83 [08:35<00:08,  4.26s/it]Running generate_until requests:  99%|█████████▉| 82/83 [08:38<00:03,  3.98s/it]Running generate_until requests: 100%|██████████| 83/83 [08:42<00:00,  3.95s/it]Running generate_until requests: 100%|██████████| 83/83 [08:42<00:00,  6.29s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:02:51:21,525 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:21,534 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:21,536 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:21,543 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:21,550 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:21,584 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:21,779 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:21,822 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:26,151 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:26,152 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:26,156 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:26,156 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': False}
2024-06-04:02:51:26,584 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:26,585 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:26,591 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:26,591 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': False}
2024-06-04:02:51:27,211 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:27,212 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:27,216 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:27,216 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': False}
2024-06-04:02:51:27,404 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:27,406 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:27,410 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:27,410 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:51:28,726 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:28,728 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:28,732 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:28,732 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:51:28,796 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:28,798 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:28,803 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:28,803 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': False}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:51:28,954 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:28,955 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:28,959 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:28,959 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': False}
2024-06-04:02:51:29,154 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:29,155 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:29,159 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:29,159 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.21s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.53s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.14s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.95s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.99s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.60s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.58s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.93s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.61s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.57s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.12s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.06s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.23s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.10s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:16,133 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:16,135 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:16,316 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/83 [00:00<00:00, 199.33it/s] 49%|████▉     | 41/83 [00:00<00:00, 200.14it/s] 75%|███████▍  | 62/83 [00:00<00:00, 200.31it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
100%|██████████| 83/83 [00:00<00:00, 201.16it/s]100%|██████████| 83/83 [00:00<00:00, 200.68it/s]
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:16,791 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:16,792 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:16,989 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 204.31it/s] 51%|█████     | 42/82 [00:00<00:00, 205.64it/s] 77%|███████▋  | 63/82 [00:00<00:00, 206.33it/s]100%|██████████| 82/82 [00:00<00:00, 206.20it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:37,375 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:37,377 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:52:37,549 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:37,577 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:37,579 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 25%|██▌       | 21/83 [00:00<00:00, 202.44it/s]2024-06-04:02:52:37,742 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 51%|█████     | 42/83 [00:00<00:00, 204.87it/s] 26%|██▌       | 21/82 [00:00<00:00, 206.94it/s] 76%|███████▌  | 63/83 [00:00<00:00, 206.37it/s] 51%|█████     | 42/82 [00:00<00:00, 207.66it/s]100%|██████████| 83/83 [00:00<00:00, 206.46it/s]
 77%|███████▋  | 63/82 [00:00<00:00, 208.26it/s]100%|██████████| 82/82 [00:00<00:00, 208.56it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:52:50,073 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:50,149 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:50,151 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:50,431 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 140.25it/s] 36%|███▌      | 30/83 [00:00<00:00, 141.03it/s] 54%|█████▍    | 45/83 [00:00<00:00, 141.19it/s] 72%|███████▏  | 60/83 [00:00<00:00, 141.30it/s] 90%|█████████ | 75/83 [00:00<00:00, 136.21it/s]100%|██████████| 83/83 [00:00<00:00, 138.64it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:54,387 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:54,390 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:54,438 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:54,441 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:54,779 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:02:52:54,829 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 133.76it/s] 13%|█▎        | 11/82 [00:00<00:00, 107.67it/s] 34%|███▎      | 28/83 [00:00<00:00, 131.78it/s] 27%|██▋       | 22/82 [00:00<00:00, 108.70it/s] 51%|█████     | 42/83 [00:00<00:00, 129.24it/s] 40%|████      | 33/82 [00:00<00:00, 109.02it/s] 54%|█████▎    | 44/82 [00:00<00:00, 109.18it/s] 66%|██████▋   | 55/83 [00:00<00:00, 105.24it/s] 67%|██████▋   | 55/82 [00:00<00:00, 108.95it/s] 81%|████████  | 67/83 [00:00<00:00, 92.22it/s]  80%|████████  | 66/82 [00:00<00:00, 109.02it/s] 94%|█████████▍| 77/82 [00:00<00:00, 109.19it/s] 93%|█████████▎| 77/83 [00:00<00:00, 83.06it/s]100%|██████████| 82/82 [00:00<00:00, 109.00it/s]
100%|██████████| 83/83 [00:00<00:00, 98.84it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:55,815 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:55,818 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:56,292 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 106.88it/s] 27%|██▋       | 22/82 [00:00<00:00, 107.83it/s] 40%|████      | 33/82 [00:00<00:00, 108.26it/s] 54%|█████▎    | 44/82 [00:00<00:00, 108.62it/s] 67%|██████▋   | 55/82 [00:00<00:00, 108.22it/s] 80%|████████  | 66/82 [00:00<00:00, 108.47it/s] 94%|█████████▍| 77/82 [00:00<00:00, 108.66it/s]100%|██████████| 82/82 [00:00<00:00, 108.43it/s]
2024-06-04:02:53:07,288 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:53:07,288 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:53:07,288 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:53:07,288 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:53:07,289 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:53:07,289 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:53:07,289 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:53:07,289 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:12<16:25, 12.02s/it]Running generate_until requests:   2%|▏         | 2/83 [00:22<15:16, 11.31s/it]Running generate_until requests:   4%|▎         | 3/83 [00:29<11:57,  8.97s/it]Running generate_until requests:   5%|▍         | 4/83 [00:33<09:35,  7.28s/it]Running generate_until requests:   6%|▌         | 5/83 [00:40<09:22,  7.21s/it]Running generate_until requests:   7%|▋         | 6/83 [00:46<08:43,  6.80s/it]Running generate_until requests:   8%|▊         | 7/83 [00:54<09:07,  7.20s/it]Running generate_until requests:  10%|▉         | 8/83 [01:00<08:31,  6.83s/it]Running generate_until requests:  11%|█         | 9/83 [01:07<08:17,  6.72s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:11<07:20,  6.04s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:16<06:40,  5.56s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:24<07:34,  6.40s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:28<06:34,  5.63s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:34<06:31,  5.67s/it]Running generate_until requests:  18%|█▊        | 15/83 [01:43<07:31,  6.64s/it]Running generate_until requests:  19%|█▉        | 16/83 [01:52<08:27,  7.58s/it]Running generate_until requests:  20%|██        | 17/83 [01:58<07:49,  7.12s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:05<07:27,  6.89s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:12<07:30,  7.04s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:20<07:42,  7.35s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:25<06:53,  6.68s/it]Running generate_until requests:  27%|██▋       | 22/83 [02:31<06:23,  6.28s/it]Running generate_until requests:  28%|██▊       | 23/83 [02:37<06:18,  6.31s/it]Running generate_until requests:  29%|██▉       | 24/83 [02:43<06:10,  6.29s/it]Running generate_until requests:  30%|███       | 25/83 [02:49<05:48,  6.01s/it]Running generate_until requests:  31%|███▏      | 26/83 [02:53<05:15,  5.54s/it]Running generate_until requests:  33%|███▎      | 27/83 [02:59<05:21,  5.73s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:05<05:09,  5.62s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:10<04:52,  5.41s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:16<05:09,  5.83s/it]Running generate_until requests:  37%|███▋      | 31/83 [03:21<04:42,  5.43s/it]Running generate_until requests:  39%|███▊      | 32/83 [03:27<04:44,  5.57s/it]Running generate_until requests:  40%|███▉      | 33/83 [03:31<04:16,  5.12s/it]Running generate_until requests:  41%|████      | 34/83 [03:37<04:31,  5.53s/it]Running generate_until requests:  42%|████▏     | 35/83 [03:43<04:26,  5.56s/it]Running generate_until requests:  43%|████▎     | 36/83 [03:47<04:02,  5.16s/it]Running generate_until requests:  45%|████▍     | 37/83 [03:53<04:03,  5.29s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:01<04:36,  6.15s/it]Running generate_until requests:  47%|████▋     | 39/83 [04:05<04:00,  5.46s/it]Running generate_until requests:  48%|████▊     | 40/83 [04:10<03:52,  5.41s/it]Running generate_until requests:  49%|████▉     | 41/83 [04:15<03:35,  5.14s/it]Running generate_until requests:  51%|█████     | 42/83 [04:21<03:46,  5.52s/it]Running generate_until requests:  52%|█████▏    | 43/83 [04:28<03:57,  5.93s/it]Running generate_until requests:  53%|█████▎    | 44/83 [04:33<03:40,  5.64s/it]Running generate_until requests:  54%|█████▍    | 45/83 [04:38<03:33,  5.63s/it]Running generate_until requests:  55%|█████▌    | 46/83 [04:43<03:17,  5.33s/it]Running generate_until requests:  57%|█████▋    | 47/83 [04:49<03:13,  5.36s/it]Running generate_until requests:  58%|█████▊    | 48/83 [04:55<03:19,  5.70s/it]Running generate_until requests:  59%|█████▉    | 49/83 [04:59<02:54,  5.13s/it]Running generate_until requests:  60%|██████    | 50/83 [05:04<02:50,  5.18s/it]Running generate_until requests:  61%|██████▏   | 51/83 [05:09<02:43,  5.10s/it]Running generate_until requests:  63%|██████▎   | 52/83 [05:11<02:12,  4.29s/it]Running generate_until requests:  64%|██████▍   | 53/83 [05:17<02:18,  4.62s/it]Running generate_until requests:  65%|██████▌   | 54/83 [05:21<02:13,  4.59s/it]Running generate_until requests:  66%|██████▋   | 55/83 [05:26<02:11,  4.69s/it]Running generate_until requests:  67%|██████▋   | 56/83 [05:35<02:35,  5.75s/it]Running generate_until requests:  69%|██████▊   | 57/83 [05:41<02:32,  5.88s/it]Running generate_until requests:  70%|██████▉   | 58/83 [05:47<02:27,  5.89s/it]Running generate_until requests:  71%|███████   | 59/83 [05:51<02:12,  5.51s/it]Running generate_until requests:  72%|███████▏  | 60/83 [05:56<01:58,  5.13s/it]Running generate_until requests:  73%|███████▎  | 61/83 [06:02<02:01,  5.54s/it]Running generate_until requests:  75%|███████▍  | 62/83 [06:07<01:53,  5.41s/it]Running generate_until requests:  76%|███████▌  | 63/83 [06:12<01:45,  5.30s/it]Running generate_until requests:  77%|███████▋  | 64/83 [06:27<02:34,  8.11s/it]Running generate_until requests:  78%|███████▊  | 65/83 [06:32<02:07,  7.11s/it]Running generate_until requests:  80%|███████▉  | 66/83 [06:35<01:40,  5.91s/it]Running generate_until requests:  81%|████████  | 67/83 [06:39<01:25,  5.37s/it]Running generate_until requests:  82%|████████▏ | 68/83 [06:42<01:11,  4.74s/it]Running generate_until requests:  83%|████████▎ | 69/83 [06:50<01:21,  5.83s/it]Running generate_until requests:  84%|████████▍ | 70/83 [06:57<01:20,  6.18s/it]Running generate_until requests:  86%|████████▌ | 71/83 [07:01<01:04,  5.39s/it]Running generate_until requests:  87%|████████▋ | 72/83 [07:06<00:58,  5.30s/it]Running generate_until requests:  88%|████████▊ | 73/83 [07:12<00:53,  5.34s/it]Running generate_until requests:  89%|████████▉ | 74/83 [07:16<00:45,  5.09s/it]Running generate_until requests:  90%|█████████ | 75/83 [07:23<00:44,  5.51s/it]Running generate_until requests:  92%|█████████▏| 76/83 [07:28<00:39,  5.65s/it]Running generate_until requests:  93%|█████████▎| 77/83 [07:32<00:30,  5.04s/it]Running generate_until requests:  94%|█████████▍| 78/83 [07:37<00:24,  4.87s/it]Running generate_until requests:  95%|█████████▌| 79/83 [07:40<00:18,  4.53s/it]Running generate_until requests:  96%|█████████▋| 80/83 [07:45<00:13,  4.51s/it]Running generate_until requests:  98%|█████████▊| 81/83 [07:51<00:10,  5.04s/it]Running generate_until requests:  99%|█████████▉| 82/83 [07:55<00:04,  4.72s/it]Running generate_until requests: 100%|██████████| 83/83 [07:58<00:00,  4.18s/it]Running generate_until requests: 100%|██████████| 83/83 [07:58<00:00,  5.76s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:03:02:56,211 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:02:56,211 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:02:56,215 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:02:56,263 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:02:56,449 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:02:56,500 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:02:56,512 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:02:56,926 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:03:01,221 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:03:01,222 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:03:01,228 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:03:01,228 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': False}
2024-06-04:03:03:01,383 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:03:01,384 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:03:01,388 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:03:01,388 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': False}
2024-06-04:03:03:01,992 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:03:01,993 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:03:01,997 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:03:01,997 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': False}
2024-06-04:03:03:02,009 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:03:02,011 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:03:02,015 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:03:02,015 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:03:03,471 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:03:03,473 INFO     [main.py:378] Selected Tasks: ['gsm8k']
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:03:03,478 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:03:03,478 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': False}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:03:03,659 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:03:03,661 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:03:03,666 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:03:03,666 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': False}
2024-06-04:03:03:03,768 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:03:03,769 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:03:03,773 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:03:03,773 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': False}
2024-06-04:03:03:04,145 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:03:04,146 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:03:04,150 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:03:04,150 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.6, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.16s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.93s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.62s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:03,  1.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.64s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.51s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.89s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.59s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.53s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.15s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.07s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.10s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.07s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.04s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.04s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:03:50,567 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:03:50,639 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:03:50,641 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:03:50,820 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/83 [00:00<00:00, 196.62it/s] 48%|████▊     | 40/83 [00:00<00:00, 197.32it/s] 72%|███████▏  | 60/83 [00:00<00:00, 197.95it/s] 96%|█████████▋| 80/83 [00:00<00:00, 198.33it/s]100%|██████████| 83/83 [00:00<00:00, 198.02it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:03:54,688 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:03:54,690 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:03:54,875 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 23%|██▎       | 19/83 [00:00<00:00, 183.53it/s] 46%|████▌     | 38/83 [00:00<00:00, 167.80it/s] 70%|██████▉   | 58/83 [00:00<00:00, 180.81it/s] 94%|█████████▍| 78/83 [00:00<00:00, 187.57it/s]100%|██████████| 83/83 [00:00<00:00, 184.27it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:04:25,139 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:04:25,141 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:04:25,501 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 131.77it/s] 34%|███▎      | 28/83 [00:00<00:00, 132.67it/s] 51%|█████     | 42/83 [00:00<00:00, 127.93it/s] 67%|██████▋   | 56/83 [00:00<00:00, 129.46it/s] 84%|████████▍ | 70/83 [00:00<00:00, 130.80it/s]100%|██████████| 83/83 [00:00<00:00, 130.89it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:04:28,632 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:04:28,634 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:04:28,853 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:04:28,855 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:04:28,891 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:04:28,904 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:04:28,906 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
  0%|          | 0/82 [00:00<?, ?it/s] 16%|█▌        | 13/82 [00:00<00:00, 123.62it/s] 33%|███▎      | 27/82 [00:00<00:00, 128.21it/s]2024-06-04:03:04:29,174 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:03:04:29,209 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 50%|█████     | 41/82 [00:00<00:00, 129.78it/s] 17%|█▋        | 14/83 [00:00<00:00, 136.68it/s] 17%|█▋        | 14/82 [00:00<00:00, 135.84it/s] 67%|██████▋   | 55/82 [00:00<00:00, 132.12it/s] 35%|███▍      | 29/83 [00:00<00:00, 142.62it/s] 34%|███▍      | 28/82 [00:00<00:00, 135.95it/s] 84%|████████▍ | 69/82 [00:00<00:00, 133.68it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 53%|█████▎    | 44/83 [00:00<00:00, 144.23it/s]100%|██████████| 82/82 [00:00<00:00, 132.38it/s]
 51%|█████     | 42/82 [00:00<00:00, 136.06it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 72%|███████▏  | 60/83 [00:00<00:00, 149.20it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:04:29,615 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:04:29,617 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:04:29,637 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:04:29,640 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 68%|██████▊   | 56/82 [00:00<00:00, 133.59it/s] 90%|█████████ | 75/83 [00:00<00:00, 142.16it/s] 85%|████████▌ | 70/82 [00:00<00:00, 126.64it/s]100%|██████████| 83/83 [00:00<00:00, 141.31it/s]
100%|██████████| 82/82 [00:00<00:00, 128.98it/s]
2024-06-04:03:04:30,063 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:03:04:30,204 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
 13%|█▎        | 11/82 [00:00<00:00, 102.49it/s]  0%|          | 0/82 [00:00<?, ?it/s] 27%|██▋       | 22/82 [00:00<00:00, 104.41it/s]  6%|▌         | 5/82 [00:00<00:01, 47.35it/s] 40%|████      | 33/82 [00:00<00:00, 102.02it/s] 15%|█▍        | 12/82 [00:00<00:01, 59.51it/s] 54%|█████▎    | 44/82 [00:00<00:00, 102.91it/s] 23%|██▎       | 19/82 [00:00<00:01, 58.29it/s] 67%|██████▋   | 55/82 [00:00<00:00, 102.94it/s] 32%|███▏      | 26/82 [00:00<00:00, 62.27it/s] 80%|████████  | 66/82 [00:00<00:00, 103.49it/s] 40%|████      | 33/82 [00:00<00:00, 63.83it/s] 94%|█████████▍| 77/82 [00:00<00:00, 103.80it/s] 49%|████▉     | 40/82 [00:00<00:00, 62.09it/s]100%|██████████| 82/82 [00:00<00:00, 103.21it/s]
 57%|█████▋    | 47/82 [00:00<00:00, 64.10it/s] 66%|██████▌   | 54/82 [00:00<00:00, 64.46it/s] 74%|███████▍  | 61/82 [00:00<00:00, 62.57it/s] 84%|████████▍ | 69/82 [00:01<00:00, 63.28it/s] 93%|█████████▎| 76/82 [00:01<00:00, 64.20it/s]100%|██████████| 82/82 [00:01<00:00, 62.75it/s]
2024-06-04:03:04:42,621 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:04:42,621 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:04:42,621 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:04:42,621 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:04:42,621 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:04:42,621 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:04:42,622 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:04:42,622 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:10<14:22, 10.52s/it]Running generate_until requests:   2%|▏         | 2/83 [00:21<14:17, 10.58s/it]Running generate_until requests:   4%|▎         | 3/83 [00:27<11:34,  8.68s/it]Running generate_until requests:   5%|▍         | 4/83 [00:33<09:46,  7.43s/it]Running generate_until requests:   6%|▌         | 5/83 [00:40<09:26,  7.26s/it]Running generate_until requests:   7%|▋         | 6/83 [00:50<10:37,  8.28s/it]Running generate_until requests:   8%|▊         | 7/83 [00:59<10:43,  8.46s/it]Running generate_until requests:  10%|▉         | 8/83 [01:05<09:37,  7.71s/it]Running generate_until requests:  11%|█         | 9/83 [01:12<09:31,  7.73s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:17<08:21,  6.87s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:23<07:52,  6.57s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:31<08:15,  6.98s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:35<06:51,  5.88s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:40<06:44,  5.87s/it]Running generate_until requests:  18%|█▊        | 15/83 [01:48<07:08,  6.30s/it]Running generate_until requests:  19%|█▉        | 16/83 [01:58<08:22,  7.50s/it]Running generate_until requests:  20%|██        | 17/83 [02:04<07:45,  7.05s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:10<07:22,  6.81s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:18<07:27,  6.99s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:27<08:04,  7.70s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:31<06:54,  6.69s/it]Running generate_until requests:  27%|██▋       | 22/83 [02:37<06:32,  6.43s/it]Running generate_until requests:  28%|██▊       | 23/83 [02:44<06:29,  6.48s/it]Running generate_until requests:  29%|██▉       | 24/83 [02:50<06:17,  6.39s/it]Running generate_until requests:  30%|███       | 25/83 [02:54<05:38,  5.83s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:00<05:31,  5.82s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:06<05:17,  5.68s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:11<05:03,  5.52s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:16<04:50,  5.38s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:23<05:10,  5.86s/it]Running generate_until requests:  37%|███▋      | 31/83 [03:28<04:54,  5.65s/it]Running generate_until requests:  39%|███▊      | 32/83 [03:35<05:01,  5.92s/it]Running generate_until requests:  40%|███▉      | 33/83 [03:39<04:37,  5.55s/it]Running generate_until requests:  41%|████      | 34/83 [03:48<05:17,  6.49s/it]Running generate_until requests:  42%|████▏     | 35/83 [03:53<04:57,  6.20s/it]Running generate_until requests:  43%|████▎     | 36/83 [03:58<04:31,  5.78s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:04<04:21,  5.69s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:14<05:16,  7.03s/it]Running generate_until requests:  47%|████▋     | 39/83 [04:18<04:36,  6.28s/it]Running generate_until requests:  48%|████▊     | 40/83 [04:26<04:53,  6.82s/it]Running generate_until requests:  49%|████▉     | 41/83 [04:31<04:21,  6.22s/it]Running generate_until requests:  51%|█████     | 42/83 [04:39<04:34,  6.68s/it]Running generate_until requests:  52%|█████▏    | 43/83 [04:46<04:29,  6.75s/it]Running generate_until requests:  53%|█████▎    | 44/83 [04:53<04:31,  6.97s/it]Running generate_until requests:  54%|█████▍    | 45/83 [04:59<04:09,  6.57s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:06<04:08,  6.72s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:11<03:45,  6.26s/it]Running generate_until requests:  58%|█████▊    | 48/83 [05:18<03:41,  6.32s/it]Running generate_until requests:  59%|█████▉    | 49/83 [05:22<03:08,  5.55s/it]Running generate_until requests:  60%|██████    | 50/83 [05:27<03:06,  5.66s/it]Running generate_until requests:  61%|██████▏   | 51/83 [05:33<03:02,  5.69s/it]Running generate_until requests:  63%|██████▎   | 52/83 [05:37<02:36,  5.05s/it]Running generate_until requests:  64%|██████▍   | 53/83 [05:42<02:34,  5.14s/it]Running generate_until requests:  65%|██████▌   | 54/83 [05:47<02:23,  4.96s/it]Running generate_until requests:  66%|██████▋   | 55/83 [05:51<02:17,  4.90s/it]Running generate_until requests:  67%|██████▋   | 56/83 [06:00<02:44,  6.10s/it]Running generate_until requests:  69%|██████▊   | 57/83 [06:07<02:42,  6.23s/it]Running generate_until requests:  70%|██████▉   | 58/83 [06:13<02:33,  6.14s/it]Running generate_until requests:  71%|███████   | 59/83 [06:17<02:15,  5.65s/it]Running generate_until requests:  72%|███████▏  | 60/83 [06:22<02:01,  5.29s/it]Running generate_until requests:  73%|███████▎  | 61/83 [06:28<02:01,  5.52s/it]Running generate_until requests:  75%|███████▍  | 62/83 [06:32<01:48,  5.17s/it]Running generate_until requests:  76%|███████▌  | 63/83 [06:38<01:46,  5.30s/it]Running generate_until requests:  77%|███████▋  | 64/83 [06:45<01:50,  5.81s/it]Running generate_until requests:  78%|███████▊  | 65/83 [06:50<01:39,  5.51s/it]Running generate_until requests:  80%|███████▉  | 66/83 [06:53<01:21,  4.81s/it]Running generate_until requests:  81%|████████  | 67/83 [06:57<01:13,  4.61s/it]Running generate_until requests:  82%|████████▏ | 68/83 [07:00<01:02,  4.17s/it]Running generate_until requests:  83%|████████▎ | 69/83 [07:08<01:13,  5.28s/it]Running generate_until requests:  84%|████████▍ | 70/83 [07:15<01:14,  5.73s/it]Running generate_until requests:  86%|████████▌ | 71/83 [07:18<01:01,  5.11s/it]Running generate_until requests:  87%|████████▋ | 72/83 [07:24<00:56,  5.17s/it]Running generate_until requests:  88%|████████▊ | 73/83 [07:29<00:52,  5.27s/it]Running generate_until requests:  89%|████████▉ | 74/83 [07:34<00:47,  5.23s/it]Running generate_until requests:  90%|█████████ | 75/83 [07:41<00:45,  5.74s/it]Running generate_until requests:  92%|█████████▏| 76/83 [07:47<00:39,  5.67s/it]Running generate_until requests:  93%|█████████▎| 77/83 [07:52<00:32,  5.41s/it]Running generate_until requests:  94%|█████████▍| 78/83 [07:56<00:25,  5.17s/it]Running generate_until requests:  95%|█████████▌| 79/83 [08:03<00:22,  5.53s/it]Running generate_until requests:  96%|█████████▋| 80/83 [08:07<00:15,  5.08s/it]Running generate_until requests:  98%|█████████▊| 81/83 [08:13<00:10,  5.43s/it]Running generate_until requests:  99%|█████████▉| 82/83 [08:17<00:05,  5.04s/it]Running generate_until requests: 100%|██████████| 83/83 [08:21<00:00,  4.77s/it]Running generate_until requests: 100%|██████████| 83/83 [08:21<00:00,  6.04s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:03:15:00,278 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:15:00,765 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:15:00,872 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:15:01,256 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:15:01,335 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:15:01,336 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:15:01,409 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:15:01,443 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:15:05,433 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:15:05,434 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:15:05,438 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:15:05,438 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': False}
2024-06-04:03:15:05,620 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:15:05,621 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:15:05,626 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:15:05,626 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:15:07,636 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:15:07,637 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:15:07,643 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:15:07,643 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': False}
2024-06-04:03:15:08,114 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:15:08,115 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:15:08,119 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:15:08,119 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': False}
2024-06-04:03:15:08,194 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:15:08,195 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:15:08,198 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:15:08,199 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': False}
2024-06-04:03:15:08,209 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:15:08,210 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:15:08,215 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:15:08,215 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': False}
2024-06-04:03:15:08,307 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:15:08,309 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:15:08,310 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:15:08,312 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:15:08,313 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:15:08,313 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': False}
2024-06-04:03:15:08,316 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:15:08,317 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.7, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.13s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.18s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.96s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.74s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.69s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.94s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.61s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.87s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.57s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.57s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.24s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.05s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.25s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.15s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.18s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:15:53,684 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:15:53,686 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:15:53,863 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 204.51it/s] 51%|█████     | 42/82 [00:00<00:00, 204.55it/s] 77%|███████▋  | 63/82 [00:00<00:00, 204.72it/s]100%|██████████| 82/82 [00:00<00:00, 204.67it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:16:00,589 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:16:00,591 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:16:00,872 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 136.92it/s] 34%|███▎      | 28/83 [00:00<00:00, 137.71it/s] 51%|█████     | 42/83 [00:00<00:00, 138.00it/s] 67%|██████▋   | 56/83 [00:00<00:00, 137.71it/s] 84%|████████▍ | 70/83 [00:00<00:00, 135.11it/s]100%|██████████| 83/83 [00:00<00:00, 135.98it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:16:10,572 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:16:10,574 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:16:10,733 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 203.52it/s] 51%|█████     | 42/82 [00:00<00:00, 204.16it/s] 77%|███████▋  | 63/82 [00:00<00:00, 204.34it/s]100%|██████████| 82/82 [00:00<00:00, 204.65it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:16:15,359 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:16:15,361 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:16:15,538 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 204.96it/s] 51%|█████     | 42/82 [00:00<00:00, 207.65it/s] 77%|███████▋  | 63/82 [00:00<00:00, 207.74it/s]100%|██████████| 82/82 [00:00<00:00, 208.18it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:16:19,670 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:16:19,671 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:16:19,876 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/83 [00:00<00:00, 194.95it/s] 48%|████▊     | 40/83 [00:00<00:00, 196.04it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 72%|███████▏  | 60/83 [00:00<00:00, 197.00it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:16:20,248 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:16:20,250 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 96%|█████████▋| 80/83 [00:00<00:00, 198.11it/s]100%|██████████| 83/83 [00:00<00:00, 197.38it/s]
2024-06-04:03:16:20,416 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 206.35it/s] 51%|█████     | 42/82 [00:00<00:00, 207.52it/s] 77%|███████▋  | 63/82 [00:00<00:00, 208.00it/s]100%|██████████| 82/82 [00:00<00:00, 208.01it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:16:28,849 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:16:28,851 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:16:29,012 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
2024-06-04:03:16:29,014 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
  0%|          | 0/83 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:16:29,089 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:16:29,091 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 24%|██▍       | 20/83 [00:00<00:00, 199.13it/s] 49%|████▉     | 41/83 [00:00<00:00, 202.50it/s]2024-06-04:03:16:29,291 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 75%|███████▍  | 62/83 [00:00<00:00, 204.89it/s] 25%|██▌       | 21/83 [00:00<00:00, 204.45it/s]100%|██████████| 83/83 [00:00<00:00, 206.13it/s]100%|██████████| 83/83 [00:00<00:00, 204.87it/s]
 51%|█████     | 42/83 [00:00<00:00, 206.04it/s] 76%|███████▌  | 63/83 [00:00<00:00, 207.04it/s]100%|██████████| 83/83 [00:00<00:00, 207.24it/s]
2024-06-04:03:16:39,614 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:16:39,614 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:16:39,614 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:16:39,614 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:16:39,614 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:16:39,614 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:16:39,614 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:16:39,615 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:14<20:11, 14.77s/it]Running generate_until requests:   2%|▏         | 2/83 [00:28<19:21, 14.33s/it]Running generate_until requests:   4%|▎         | 3/83 [00:36<15:09, 11.37s/it]Running generate_until requests:   5%|▍         | 4/83 [00:42<12:19,  9.37s/it]Running generate_until requests:   6%|▌         | 5/83 [00:51<11:36,  8.93s/it]Running generate_until requests:   7%|▋         | 6/83 [00:57<10:14,  7.98s/it]Running generate_until requests:   8%|▊         | 7/83 [01:05<10:24,  8.21s/it]Running generate_until requests:  10%|▉         | 8/83 [01:13<09:50,  7.88s/it]Running generate_until requests:  11%|█         | 9/83 [01:22<10:21,  8.40s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:28<09:10,  7.54s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:36<09:26,  7.86s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:46<10:02,  8.48s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:50<08:13,  7.04s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:57<07:57,  6.92s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:07<09:03,  7.99s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:18<10:00,  8.96s/it]Running generate_until requests:  20%|██        | 17/83 [02:25<09:15,  8.42s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:33<08:46,  8.10s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:42<08:53,  8.34s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:52<09:31,  9.07s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:58<08:08,  7.88s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:04<07:39,  7.53s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:12<07:33,  7.56s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:18<07:08,  7.26s/it]Running generate_until requests:  30%|███       | 25/83 [03:26<06:59,  7.23s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:32<06:41,  7.05s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:41<06:59,  7.48s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:47<06:27,  7.04s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:53<06:11,  6.88s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:02<06:32,  7.41s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:09<06:15,  7.23s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:17<06:18,  7.43s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:21<05:31,  6.63s/it]Running generate_until requests:  41%|████      | 34/83 [04:32<06:20,  7.76s/it]Running generate_until requests:  42%|████▏     | 35/83 [04:38<05:52,  7.33s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:45<05:42,  7.30s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:52<05:25,  7.07s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:01<05:50,  7.79s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:06<05:06,  6.97s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:17<05:40,  7.92s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:21<04:53,  6.99s/it]Running generate_until requests:  51%|█████     | 42/83 [05:28<04:48,  7.03s/it]Running generate_until requests:  52%|█████▏    | 43/83 [05:37<04:55,  7.38s/it]Running generate_until requests:  53%|█████▎    | 44/83 [05:42<04:22,  6.72s/it]Running generate_until requests:  54%|█████▍    | 45/83 [05:48<04:12,  6.64s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:53<03:48,  6.17s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:59<03:36,  6.01s/it]Running generate_until requests:  58%|█████▊    | 48/83 [06:07<03:47,  6.50s/it]Running generate_until requests:  59%|█████▉    | 49/83 [06:11<03:21,  5.92s/it]Running generate_until requests:  60%|██████    | 50/83 [06:20<03:46,  6.86s/it]Running generate_until requests:  61%|██████▏   | 51/83 [06:38<05:26, 10.20s/it]Running generate_until requests:  63%|██████▎   | 52/83 [06:45<04:42,  9.11s/it]Running generate_until requests:  64%|██████▍   | 53/83 [06:51<04:09,  8.31s/it]Running generate_until requests:  65%|██████▌   | 54/83 [06:57<03:36,  7.47s/it]Running generate_until requests:  66%|██████▋   | 55/83 [07:03<03:21,  7.20s/it]Running generate_until requests:  67%|██████▋   | 56/83 [07:15<03:54,  8.68s/it]Running generate_until requests:  69%|██████▊   | 57/83 [07:22<03:28,  8.01s/it]Running generate_until requests:  70%|██████▉   | 58/83 [07:29<03:12,  7.70s/it]Running generate_until requests:  71%|███████   | 59/83 [07:34<02:48,  7.02s/it]Running generate_until requests:  72%|███████▏  | 60/83 [07:40<02:29,  6.49s/it]Running generate_until requests:  73%|███████▎  | 61/83 [07:47<02:31,  6.90s/it]Running generate_until requests:  75%|███████▍  | 62/83 [07:52<02:10,  6.23s/it]Running generate_until requests:  76%|███████▌  | 63/83 [07:58<02:00,  6.03s/it]Running generate_until requests:  77%|███████▋  | 64/83 [08:05<02:03,  6.49s/it]Running generate_until requests:  78%|███████▊  | 65/83 [08:11<01:51,  6.20s/it]Running generate_until requests:  80%|███████▉  | 66/83 [08:16<01:39,  5.85s/it]Running generate_until requests:  81%|████████  | 67/83 [08:21<01:30,  5.67s/it]Running generate_until requests:  82%|████████▏ | 68/83 [08:25<01:16,  5.08s/it]Running generate_until requests:  83%|████████▎ | 69/83 [08:35<01:30,  6.50s/it]Running generate_until requests:  84%|████████▍ | 70/83 [08:42<01:29,  6.89s/it]Running generate_until requests:  86%|████████▌ | 71/83 [08:47<01:15,  6.29s/it]Running generate_until requests:  87%|████████▋ | 72/83 [08:52<01:04,  5.82s/it]Running generate_until requests:  88%|████████▊ | 73/83 [08:58<00:58,  5.82s/it]Running generate_until requests:  89%|████████▉ | 74/83 [09:05<00:55,  6.13s/it]Running generate_until requests:  90%|█████████ | 75/83 [09:13<00:54,  6.84s/it]Running generate_until requests:  92%|█████████▏| 76/83 [09:18<00:42,  6.12s/it]Running generate_until requests:  93%|█████████▎| 77/83 [09:23<00:34,  5.81s/it]Running generate_until requests:  94%|█████████▍| 78/83 [09:28<00:28,  5.70s/it]Running generate_until requests:  95%|█████████▌| 79/83 [09:37<00:26,  6.59s/it]Running generate_until requests:  96%|█████████▋| 80/83 [09:43<00:19,  6.43s/it]Running generate_until requests:  98%|█████████▊| 81/83 [09:50<00:13,  6.74s/it]Running generate_until requests:  99%|█████████▉| 82/83 [09:55<00:06,  6.16s/it]Running generate_until requests: 100%|██████████| 83/83 [09:59<00:00,  5.60s/it]Running generate_until requests: 100%|██████████| 83/83 [09:59<00:00,  7.23s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:03:28:20,718 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:28:20,750 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:28:20,823 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:28:20,824 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:28:20,920 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:28:21,407 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:28:21,484 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:28:21,576 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:28:25,399 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:28:25,400 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:28:25,405 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:28:25,405 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': False}
2024-06-04:03:28:25,497 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:28:25,498 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:28:25,502 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:28:25,502 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': False}
2024-06-04:03:28:26,648 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:28:26,649 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:28:26,653 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:28:26,653 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': False}
2024-06-04:03:28:26,738 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:28:26,739 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:28:26,742 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:28:26,743 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:28:27,913 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:28:27,914 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:28:27,920 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:28:27,920 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:28:28,589 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:28:28,591 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:28:28,596 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:28:28,596 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': False}
2024-06-04:03:28:28,635 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:28:28,636 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:28:28,641 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:28:28,641 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': False}
2024-06-04:03:28:28,717 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:28:28,719 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:28:28,724 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:28:28,724 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.8, 'check': False}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.88s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.93s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.93s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.76s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.41s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.88s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.63s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.50s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.55s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.72s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.20s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.07s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.40s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.56s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.99s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:29:14,691 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:29:14,692 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:29:14,871 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/83 [00:00<00:00, 196.35it/s] 48%|████▊     | 40/83 [00:00<00:00, 197.22it/s] 72%|███████▏  | 60/83 [00:00<00:00, 197.80it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 98%|█████████▊| 81/83 [00:00<00:00, 199.71it/s]100%|██████████| 83/83 [00:00<00:00, 199.04it/s]
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:29:15,303 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:29:15,306 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:29:15,473 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/83 [00:00<00:00, 198.09it/s] 48%|████▊     | 40/83 [00:00<00:00, 198.84it/s] 72%|███████▏  | 60/83 [00:00<00:00, 198.85it/s] 96%|█████████▋| 80/83 [00:00<00:00, 197.96it/s]100%|██████████| 83/83 [00:00<00:00, 198.22it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:29:35,523 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:29:35,598 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:29:35,600 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:29:35,796 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 25%|██▌       | 21/83 [00:00<00:00, 204.41it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:29:35,978 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:29:35,980 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 51%|█████     | 42/83 [00:00<00:00, 206.05it/s] 76%|███████▌  | 63/83 [00:00<00:00, 206.04it/s]2024-06-04:03:29:36,136 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s]100%|██████████| 83/83 [00:00<00:00, 206.45it/s]
 23%|██▎       | 19/82 [00:00<00:00, 181.88it/s] 46%|████▋     | 38/82 [00:00<00:00, 150.21it/s] 66%|██████▌   | 54/82 [00:00<00:00, 142.99it/s] 84%|████████▍ | 69/82 [00:00<00:00, 131.93it/s]100%|██████████| 82/82 [00:00<00:00, 132.19it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:29:52,974 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:29:52,976 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:29:53,141 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:29:53,143 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:29:53,213 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 131.60it/s] 34%|███▍      | 28/82 [00:00<00:00, 132.19it/s]2024-06-04:03:29:53,449 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 51%|█████     | 42/82 [00:00<00:00, 134.84it/s] 17%|█▋        | 14/82 [00:00<00:00, 138.09it/s] 68%|██████▊   | 56/82 [00:00<00:00, 135.50it/s] 34%|███▍      | 28/82 [00:00<00:00, 136.23it/s] 85%|████████▌ | 70/82 [00:00<00:00, 135.22it/s] 51%|█████     | 42/82 [00:00<00:00, 135.50it/s]100%|██████████| 82/82 [00:00<00:00, 134.82it/s]
 68%|██████▊   | 56/82 [00:00<00:00, 132.25it/s] 85%|████████▌ | 70/82 [00:00<00:00, 126.40it/s]100%|██████████| 82/82 [00:00<00:00, 127.59it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:29:54,292 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:29:54,294 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:29:54,349 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:29:54,352 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:29:54,761 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
2024-06-04:03:29:54,801 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/82 [00:00<?, ?it/s]  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 103.22it/s] 11%|█         | 9/83 [00:00<00:00, 81.26it/s] 27%|██▋       | 22/82 [00:00<00:00, 106.49it/s] 22%|██▏       | 18/83 [00:00<00:00, 82.46it/s] 40%|████      | 33/82 [00:00<00:00, 108.01it/s] 33%|███▎      | 27/83 [00:00<00:00, 84.51it/s] 55%|█████▍    | 45/82 [00:00<00:00, 108.95it/s] 43%|████▎     | 36/83 [00:00<00:00, 83.20it/s] 68%|██████▊   | 56/82 [00:00<00:00, 109.27it/s] 54%|█████▍    | 45/83 [00:00<00:00, 84.16it/s] 83%|████████▎ | 68/82 [00:00<00:00, 109.67it/s] 65%|██████▌   | 54/83 [00:00<00:00, 82.26it/s] 98%|█████████▊| 80/82 [00:00<00:00, 109.65it/s]100%|██████████| 82/82 [00:00<00:00, 108.85it/s]
 76%|███████▌  | 63/83 [00:00<00:00, 84.25it/s] 89%|████████▉ | 74/83 [00:00<00:00, 91.61it/s]100%|██████████| 83/83 [00:00<00:00, 88.24it/s]
2024-06-04:03:30:06,402 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:30:06,402 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:30:06,402 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:30:06,402 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:30:06,402 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:30:06,402 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:30:06,402 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:03:30:06,403 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:11<16:04, 11.77s/it]Running generate_until requests:   2%|▏         | 2/83 [00:22<14:54, 11.04s/it]Running generate_until requests:   4%|▎         | 3/83 [00:28<11:59,  8.99s/it]Running generate_until requests:   5%|▍         | 4/83 [00:33<09:46,  7.42s/it]Running generate_until requests:   6%|▌         | 5/83 [00:40<09:22,  7.22s/it]Running generate_until requests:   7%|▋         | 6/83 [00:46<08:40,  6.75s/it]Running generate_until requests:   8%|▊         | 7/83 [00:54<08:50,  6.98s/it]Running generate_until requests:  10%|▉         | 8/83 [01:00<08:24,  6.73s/it]Running generate_until requests:  11%|█         | 9/83 [01:08<08:45,  7.10s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:13<07:48,  6.42s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:20<08:02,  6.70s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:28<08:27,  7.15s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:33<07:41,  6.59s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:39<07:12,  6.27s/it]Running generate_until requests:  18%|█▊        | 15/83 [01:51<09:03,  7.99s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:01<09:34,  8.57s/it]Running generate_until requests:  20%|██        | 17/83 [02:07<08:35,  7.80s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:13<07:56,  7.34s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:21<07:54,  7.41s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:30<08:23,  8.00s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:35<07:15,  7.02s/it]Running generate_until requests:  27%|██▋       | 22/83 [02:40<06:42,  6.60s/it]Running generate_until requests:  28%|██▊       | 23/83 [02:47<06:35,  6.60s/it]Running generate_until requests:  29%|██▉       | 24/83 [02:53<06:12,  6.31s/it]Running generate_until requests:  30%|███       | 25/83 [03:00<06:26,  6.66s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:06<06:02,  6.36s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:11<05:35,  5.98s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:16<05:14,  5.71s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:21<05:01,  5.59s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:29<05:24,  6.12s/it]Running generate_until requests:  37%|███▋      | 31/83 [03:34<05:05,  5.88s/it]Running generate_until requests:  39%|███▊      | 32/83 [03:39<04:52,  5.74s/it]Running generate_until requests:  40%|███▉      | 33/83 [03:43<04:22,  5.24s/it]Running generate_until requests:  41%|████      | 34/83 [03:53<05:14,  6.42s/it]Running generate_until requests:  42%|████▏     | 35/83 [03:59<05:10,  6.47s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:05<04:56,  6.32s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:11<04:39,  6.08s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:19<04:58,  6.63s/it]Running generate_until requests:  47%|████▋     | 39/83 [04:23<04:22,  5.97s/it]Running generate_until requests:  48%|████▊     | 40/83 [04:31<04:44,  6.62s/it]Running generate_until requests:  49%|████▉     | 41/83 [04:36<04:11,  5.98s/it]Running generate_until requests:  51%|█████     | 42/83 [04:43<04:22,  6.39s/it]Running generate_until requests:  52%|█████▏    | 43/83 [04:50<04:21,  6.54s/it]Running generate_until requests:  53%|█████▎    | 44/83 [04:54<03:51,  5.94s/it]Running generate_until requests:  54%|█████▍    | 45/83 [05:00<03:42,  5.86s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:06<03:33,  5.76s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:10<03:11,  5.33s/it]Running generate_until requests:  58%|█████▊    | 48/83 [05:17<03:20,  5.72s/it]Running generate_until requests:  59%|█████▉    | 49/83 [05:21<03:00,  5.31s/it]Running generate_until requests:  60%|██████    | 50/83 [05:28<03:14,  5.90s/it]Running generate_until requests:  61%|██████▏   | 51/83 [05:35<03:15,  6.11s/it]Running generate_until requests:  63%|██████▎   | 52/83 [05:38<02:46,  5.38s/it]Running generate_until requests:  64%|██████▍   | 53/83 [05:44<02:44,  5.48s/it]Running generate_until requests:  65%|██████▌   | 54/83 [05:48<02:28,  5.12s/it]Running generate_until requests:  66%|██████▋   | 55/83 [05:54<02:27,  5.27s/it]Running generate_until requests:  67%|██████▋   | 56/83 [06:06<03:16,  7.26s/it]Running generate_until requests:  69%|██████▊   | 57/83 [06:14<03:13,  7.44s/it]Running generate_until requests:  70%|██████▉   | 58/83 [06:20<02:53,  6.94s/it]Running generate_until requests:  71%|███████   | 59/83 [06:24<02:30,  6.27s/it]Running generate_until requests:  72%|███████▏  | 60/83 [06:29<02:10,  5.69s/it]Running generate_until requests:  73%|███████▎  | 61/83 [06:35<02:08,  5.85s/it]Running generate_until requests:  75%|███████▍  | 62/83 [06:40<01:57,  5.59s/it]Running generate_until requests:  76%|███████▌  | 63/83 [06:44<01:45,  5.25s/it]Running generate_until requests:  77%|███████▋  | 64/83 [06:51<01:45,  5.53s/it]Running generate_until requests:  78%|███████▊  | 65/83 [06:55<01:35,  5.30s/it]Running generate_until requests:  80%|███████▉  | 66/83 [06:59<01:23,  4.91s/it]Running generate_until requests:  81%|████████  | 67/83 [07:03<01:11,  4.45s/it]Running generate_until requests:  82%|████████▏ | 68/83 [07:06<01:01,  4.11s/it]Running generate_until requests:  83%|████████▎ | 69/83 [07:13<01:09,  5.00s/it]Running generate_until requests:  84%|████████▍ | 70/83 [07:19<01:09,  5.35s/it]Running generate_until requests:  86%|████████▌ | 71/83 [07:23<00:59,  4.99s/it]Running generate_until requests:  87%|████████▋ | 72/83 [07:27<00:50,  4.64s/it]Running generate_until requests:  88%|████████▊ | 73/83 [07:32<00:47,  4.72s/it]Running generate_until requests:  89%|████████▉ | 74/83 [07:37<00:42,  4.69s/it]Running generate_until requests:  90%|█████████ | 75/83 [07:45<00:45,  5.68s/it]Running generate_until requests:  92%|█████████▏| 76/83 [07:49<00:36,  5.29s/it]Running generate_until requests:  93%|█████████▎| 77/83 [07:54<00:30,  5.11s/it]Running generate_until requests:  94%|█████████▍| 78/83 [07:58<00:24,  4.96s/it]Running generate_until requests:  95%|█████████▌| 79/83 [08:05<00:21,  5.36s/it]Running generate_until requests:  96%|█████████▋| 80/83 [08:10<00:15,  5.25s/it]Running generate_until requests:  98%|█████████▊| 81/83 [08:16<00:11,  5.57s/it]Running generate_until requests:  99%|█████████▉| 82/83 [08:20<00:05,  5.09s/it]Running generate_until requests: 100%|██████████| 83/83 [08:24<00:00,  4.91s/it]Running generate_until requests: 100%|██████████| 83/83 [08:24<00:00,  6.08s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:03:42:10,416 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:42:10,418 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:42:10,587 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:42:10,755 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:42:10,848 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:42:10,871 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:42:10,932 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:42:10,945 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:42:15,519 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:42:15,520 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:42:15,525 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:42:15,525 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': False}
2024-06-04:03:42:15,604 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:42:15,606 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:42:15,611 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:42:15,611 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:42:17,372 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:42:17,373 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:42:17,379 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:42:17,379 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': False}
2024-06-04:03:42:17,631 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:42:17,632 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:42:17,637 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:42:17,637 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': False}
2024-06-04:03:42:17,687 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:42:17,689 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:42:17,694 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:42:17,694 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': False}
2024-06-04:03:42:17,704 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:42:17,705 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:42:17,710 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:42:17,710 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': False}
2024-06-04:03:42:17,865 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:42:17,867 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:42:17,872 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:42:17,872 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': False}
2024-06-04:03:42:17,909 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:42:17,910 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:42:17,915 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:42:17,915 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.9, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.11s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.25s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.03s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.64s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.71s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.67s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.62s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.63s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:01,  1.98s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:01,  1.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.65s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.63s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.56s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.19s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.09s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.16s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.07s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.11s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:43:03,895 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:43:03,897 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:43:04,072 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 203.81it/s] 51%|█████     | 42/82 [00:00<00:00, 204.22it/s] 77%|███████▋  | 63/82 [00:00<00:00, 202.26it/s]100%|██████████| 82/82 [00:00<00:00, 203.35it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:43:05,518 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:43:05,603 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:43:05,604 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:43:05,775 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 205.18it/s] 51%|█████     | 42/83 [00:00<00:00, 205.80it/s] 76%|███████▌  | 63/83 [00:00<00:00, 206.18it/s]100%|██████████| 83/83 [00:00<00:00, 206.37it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:43:23,482 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:43:23,484 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:43:23,821 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 136.78it/s] 34%|███▎      | 28/83 [00:00<00:00, 137.46it/s] 51%|█████     | 42/83 [00:00<00:00, 137.69it/s] 67%|██████▋   | 56/83 [00:00<00:00, 137.83it/s] 84%|████████▍ | 70/83 [00:00<00:00, 137.85it/s]100%|██████████| 83/83 [00:00<00:00, 137.76it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:43:45,294 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:43:45,296 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:43:45,475 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:43:45,478 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:43:45,619 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:43:45,708 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:43:45,711 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 21%|██        | 17/82 [00:00<00:00, 163.99it/s] 41%|████▏     | 34/82 [00:00<00:00, 141.77it/s]2024-06-04:03:43:45,914 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/82 [00:00<?, ?it/s] 60%|█████▉    | 49/82 [00:00<00:00, 136.83it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:43:46,006 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:43:46,008 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 13%|█▎        | 11/82 [00:00<00:00, 104.11it/s]2024-06-04:03:43:46,092 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
 77%|███████▋  | 63/82 [00:00<00:00, 134.52it/s]  0%|          | 0/82 [00:00<?, ?it/s] 27%|██▋       | 22/82 [00:00<00:00, 104.64it/s] 94%|█████████▍| 77/82 [00:00<00:00, 133.18it/s] 16%|█▌        | 13/82 [00:00<00:00, 129.79it/s]100%|██████████| 82/82 [00:00<00:00, 136.08it/s]
 40%|████      | 33/82 [00:00<00:00, 104.84it/s] 33%|███▎      | 27/82 [00:00<00:00, 130.68it/s]2024-06-04:03:43:46,342 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 54%|█████▎    | 44/82 [00:00<00:00, 104.95it/s] 50%|█████     | 41/82 [00:00<00:00, 131.32it/s] 16%|█▌        | 13/83 [00:00<00:00, 128.50it/s] 67%|██████▋   | 55/82 [00:00<00:00, 104.84it/s] 67%|██████▋   | 55/82 [00:00<00:00, 130.98it/s] 33%|███▎      | 27/83 [00:00<00:00, 129.69it/s] 80%|████████  | 66/82 [00:00<00:00, 104.79it/s] 84%|████████▍ | 69/82 [00:00<00:00, 130.65it/s] 48%|████▊     | 40/83 [00:00<00:00, 129.79it/s] 94%|█████████▍| 77/82 [00:00<00:00, 104.76it/s]100%|██████████| 82/82 [00:00<00:00, 104.78it/s]
100%|██████████| 82/82 [00:00<00:00, 130.80it/s]
 64%|██████▍   | 53/83 [00:00<00:00, 128.60it/s] 80%|███████▉  | 66/83 [00:00<00:00, 123.40it/s] 95%|█████████▌| 79/83 [00:00<00:00, 118.83it/s]100%|██████████| 83/83 [00:00<00:00, 122.72it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:43:50,942 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:43:50,945 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:43:51,400 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 106.43it/s] 27%|██▋       | 22/83 [00:00<00:00, 107.65it/s] 40%|███▉      | 33/83 [00:00<00:00, 108.14it/s] 53%|█████▎    | 44/83 [00:00<00:00, 108.26it/s] 66%|██████▋   | 55/83 [00:00<00:00, 108.36it/s] 80%|███████▉  | 66/83 [00:00<00:00, 108.56it/s] 93%|█████████▎| 77/83 [00:00<00:00, 108.69it/s]100%|██████████| 83/83 [00:00<00:00, 108.37it/s]
2024-06-04:03:44:03,519 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:44:03,519 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:44:03,519 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:44:03,519 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:44:03,519 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:44:03,519 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:44:03,520 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:44:03,519 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 465, in <module>
    cli_evaluate()
  File "/private/home/beidic/yang/GRIFFIN2/main.py", line 384, in cli_evaluate
    results = xevaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/yang/GRIFFIN2/xevaluator.py", line 262, in simple_evaluate
    results = evaluate(
              ^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/lm_eval/utils.py", line 288, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/yang/GRIFFIN2/xevaluator.py", line 407, in evaluate
    resps = getattr(lm, reqtype)(cloned_reqs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/yang/GRIFFIN2/xhuggingface.py", line 1187, in generate_until
    cont = self._model_generate(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/yang/GRIFFIN2/xhuggingface.py", line 747, in _model_generate
    outputs = self.model.generate(
              ^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/transformers/generation/utils.py", line 1544, in generate
    return self.greedy_search(
           ^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/yang/GRIFFIN2/llama10.py", line 1270, in greedy_search
    outputs = self(
              ^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/yang/GRIFFIN2/llama10.py", line 952, in forward
    logits = logits.float()
             ^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 602.00 MiB. GPU 1 has a total capacity of 31.74 GiB of which 541.38 MiB is free. Including non-PyTorch memory, this process has 31.21 GiB memory in use. Of the allocated memory 24.89 GiB is allocated by PyTorch, and 5.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running generate_until requests:   1%|          | 1/83 [00:07<10:45,  7.88s/it][2024-06-04 03:44:14,086] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2953658 closing signal SIGTERM
[2024-06-04 03:44:14,086] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2953660 closing signal SIGTERM
[2024-06-04 03:44:14,086] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2953661 closing signal SIGTERM
[2024-06-04 03:44:14,086] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2953662 closing signal SIGTERM
[2024-06-04 03:44:14,087] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2953663 closing signal SIGTERM
[2024-06-04 03:44:14,087] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2953664 closing signal SIGTERM
[2024-06-04 03:44:14,087] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2953665 closing signal SIGTERM
[2024-06-04 03:44:15,495] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 2953659) of binary: /private/home/beidic/.conda/envs/griffin/bin/python
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_03:44:14
  host      : learnfair5135.h2.fair
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2953659)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
