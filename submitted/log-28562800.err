Already on 'yangexp2'
Your configuration specifies to merge with the ref 'refs/heads/yangexp2'
from the remote, but no such ref was fetched.
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/huggingface-cli", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py", line 51, in main
    service.run()
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/user.py", line 98, in run
    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 111, in login
    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 307, in _login
    raise ValueError("Invalid token passed!")
ValueError: Invalid token passed!
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:01:31:05,183 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:05,184 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:05,184 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:05,184 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:05,212 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:05,325 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:06,610 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:07,697 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:12,028 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:12,029 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:12,030 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:12,030 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:12,031 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:12,031 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:12,032 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:12,033 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:12,034 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:12,035 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:12,040 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:12,041 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:01:31:12,041 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:12,041 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:12,041 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:12,041 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:12,041 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:01:31:12,041 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:01:31:12,041 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:01:31:12,041 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:01:31:16,193 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:16,195 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:16,202 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:16,202 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:01:31:17,991 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:17,995 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:18,008 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:18,008 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:01:31:20,389 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:20,391 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:20,397 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:20,397 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:22<01:07, 22.53s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:29<01:27, 29.33s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:29<01:27, 29.09s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:29<01:27, 29.11s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:29<01:27, 29.32s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:16, 25.41s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:29<01:27, 29.14s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:03, 21.18s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:49<00:51, 25.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:58<00:58, 29.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:58<00:58, 29.24s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:52<00:54, 27.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:58<00:58, 29.29s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:58<00:58, 29.29s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:58<00:58, 29.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:55<00:55, 27.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:28<00:29, 29.49s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:29<00:29, 29.85s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:25<00:28, 28.96s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:29<00:29, 29.85s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:29<00:29, 29.94s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:29<00:29, 29.94s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:21<00:28, 28.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:23<00:28, 28.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:34<00:00, 20.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:34<00:00, 23.69s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 19.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 22.28s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:34<00:00, 20.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:34<00:00, 23.71s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 19.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 22.79s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:35<00:00, 20.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:35<00:00, 23.78s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:34<00:00, 20.38s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:34<00:00, 23.73s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:35<00:00, 20.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:35<00:00, 23.85s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:27<00:00, 19.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:27<00:00, 21.82s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:32,333 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:32,335 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:32,585 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 197.13it/s] 49%|████▉     | 40/82 [00:00<00:00, 198.16it/s] 73%|███████▎  | 60/82 [00:00<00:00, 198.74it/s] 98%|█████████▊| 80/82 [00:00<00:00, 198.67it/s]100%|██████████| 82/82 [00:00<00:00, 198.47it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:53,432 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:53,435 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:53,571 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:53,573 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:53,755 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 131.08it/s]2024-06-04:01:33:53,946 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 34%|███▍      | 28/82 [00:00<00:00, 131.51it/s] 16%|█▌        | 13/83 [00:00<00:00, 129.25it/s] 51%|█████     | 42/82 [00:00<00:00, 131.17it/s] 33%|███▎      | 27/83 [00:00<00:00, 130.22it/s] 68%|██████▊   | 56/82 [00:00<00:00, 131.09it/s] 49%|████▉     | 41/83 [00:00<00:00, 130.40it/s] 85%|████████▌ | 70/82 [00:00<00:00, 131.16it/s] 66%|██████▋   | 55/83 [00:00<00:00, 129.94it/s]100%|██████████| 82/82 [00:00<00:00, 130.99it/s]
 89%|████████▉ | 74/83 [00:00<00:00, 148.79it/s]100%|██████████| 83/83 [00:00<00:00, 144.35it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:34:00,560 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:00,562 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:00,731 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 209.61it/s] 52%|█████▏    | 43/82 [00:00<00:00, 211.04it/s] 79%|███████▉  | 65/82 [00:00<00:00, 211.13it/s]100%|██████████| 82/82 [00:00<00:00, 211.16it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:01:34:01,495 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:34:01,570 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:01,572 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:01,741 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 27%|██▋       | 22/83 [00:00<00:00, 210.67it/s] 53%|█████▎    | 44/83 [00:00<00:00, 211.80it/s] 80%|███████▉  | 66/83 [00:00<00:00, 211.96it/s]100%|██████████| 83/83 [00:00<00:00, 211.89it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:34:09,635 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:09,637 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:10,024 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 15%|█▍        | 12/82 [00:00<00:00, 117.72it/s] 30%|███       | 25/82 [00:00<00:00, 119.12it/s] 45%|████▌     | 37/82 [00:00<00:00, 115.99it/s] 60%|█████▉    | 49/82 [00:00<00:00, 116.09it/s] 76%|███████▌  | 62/82 [00:00<00:00, 117.84it/s] 91%|█████████▏| 75/82 [00:00<00:00, 118.73it/s]100%|██████████| 82/82 [00:00<00:00, 118.08it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:35:10,711 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:35:10,716 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:35:11,618 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s]  8%|▊         | 7/83 [00:00<00:01, 63.32it/s] 17%|█▋        | 14/83 [00:00<00:01, 58.98it/s] 24%|██▍       | 20/83 [00:00<00:01, 53.71it/s] 33%|███▎      | 27/83 [00:00<00:00, 57.78it/s] 40%|███▉      | 33/83 [00:00<00:00, 58.43it/s] 47%|████▋     | 39/83 [00:00<00:00, 55.31it/s] 54%|█████▍    | 45/83 [00:00<00:00, 53.55it/s] 66%|██████▋   | 55/83 [00:00<00:00, 66.87it/s] 75%|███████▍  | 62/83 [00:01<00:00, 67.15it/s] 83%|████████▎ | 69/83 [00:01<00:00, 58.34it/s] 92%|█████████▏| 76/83 [00:01<00:00, 57.35it/s] 99%|█████████▉| 82/83 [00:01<00:00, 53.78it/s]100%|██████████| 83/83 [00:01<00:00, 57.14it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:35:53,403 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:35:53,407 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:35:54,210 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s]  7%|▋         | 6/83 [00:00<00:01, 58.56it/s] 20%|██        | 17/83 [00:00<00:00, 84.46it/s] 33%|███▎      | 27/83 [00:00<00:00, 91.26it/s] 46%|████▌     | 38/83 [00:00<00:00, 92.99it/s] 58%|█████▊    | 48/83 [00:00<00:00, 85.49it/s] 69%|██████▊   | 57/83 [00:00<00:00, 86.15it/s] 80%|███████▉  | 66/83 [00:00<00:00, 78.35it/s] 93%|█████████▎| 77/83 [00:00<00:00, 85.67it/s]100%|██████████| 83/83 [00:00<00:00, 85.95it/s]
2024-06-04:01:36:06,375 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:36:06,375 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:36:06,375 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:36:06,375 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:36:06,376 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:01:36:06,376 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:36:06,377 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:36:06,380 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [01:46<2:26:00, 106.83s/it]Running generate_until requests:   2%|▏         | 2/83 [03:31<2:22:36, 105.63s/it]Running generate_until requests:   4%|▎         | 3/83 [04:50<2:04:31, 93.39s/it] Running generate_until requests:   5%|▍         | 4/83 [05:46<1:43:41, 78.75s/it]Running generate_until requests:   6%|▌         | 5/83 [07:30<1:53:52, 87.59s/it]Running generate_until requests:   7%|▋         | 6/83 [08:34<1:42:17, 79.71s/it]Running generate_until requests:   8%|▊         | 7/83 [09:56<1:42:08, 80.64s/it]Running generate_until requests:  10%|▉         | 8/83 [11:11<1:38:10, 78.54s/it]Running generate_until requests:  11%|█         | 9/83 [13:14<1:54:14, 92.63s/it]Running generate_until requests:  12%|█▏        | 10/83 [14:03<1:36:26, 79.27s/it]Running generate_until requests:  13%|█▎        | 11/83 [15:19<1:33:43, 78.11s/it]Running generate_until requests:  14%|█▍        | 12/83 [16:56<1:39:05, 83.75s/it]Running generate_until requests:  16%|█▌        | 13/83 [17:50<1:27:26, 74.95s/it]Running generate_until requests:  17%|█▋        | 14/83 [19:09<1:27:19, 75.94s/it]Running generate_until requests:  18%|█▊        | 15/83 [21:33<1:49:31, 96.64s/it]Running generate_until requests:  19%|█▉        | 16/83 [23:36<1:56:40, 104.48s/it]Running generate_until requests:  20%|██        | 17/83 [25:08<1:50:50, 100.76s/it]Running generate_until requests:  22%|██▏       | 18/83 [26:15<1:38:12, 90.66s/it] Running generate_until requests:  23%|██▎       | 19/83 [27:36<1:33:38, 87.79s/it]Running generate_until requests:  24%|██▍       | 20/83 [29:21<1:37:31, 92.89s/it]Running generate_until requests:  25%|██▌       | 21/83 [30:25<1:27:03, 84.24s/it]Running generate_until requests:  27%|██▋       | 22/83 [31:36<1:21:37, 80.29s/it]Running generate_until requests:  28%|██▊       | 23/83 [33:02<1:22:03, 82.06s/it]Running generate_until requests:  29%|██▉       | 24/83 [34:20<1:19:32, 80.89s/it]Running generate_until requests:  30%|███       | 25/83 [36:34<1:33:29, 96.71s/it]Running generate_until requests:  31%|███▏      | 26/83 [38:09<1:31:15, 96.05s/it]Running generate_until requests:  33%|███▎      | 27/83 [39:47<1:30:23, 96.85s/it]Running generate_until requests:  34%|███▎      | 28/83 [40:39<1:16:23, 83.34s/it]Running generate_until requests:  35%|███▍      | 29/83 [42:23<1:20:40, 89.63s/it]Running generate_until requests:  36%|███▌      | 30/83 [44:01<1:21:15, 92.00s/it]Running generate_until requests:  37%|███▋      | 31/83 [44:51<1:08:56, 79.55s/it]Running generate_until requests:  39%|███▊      | 32/83 [46:34<1:13:21, 86.30s/it]Running generate_until requests:  40%|███▉      | 33/83 [47:57<1:11:19, 85.58s/it]Running generate_until requests:  41%|████      | 34/83 [50:05<1:20:15, 98.28s/it]Running generate_until requests:  42%|████▏     | 35/83 [51:29<1:15:04, 93.84s/it]Running generate_until requests:  43%|████▎     | 36/83 [52:18<1:02:58, 80.39s/it]Running generate_until requests:  45%|████▍     | 37/83 [53:25<58:41, 76.55s/it]  Running generate_until requests:  46%|████▌     | 38/83 [55:04<1:02:24, 83.21s/it]Running generate_until requests:  47%|████▋     | 39/83 [56:17<58:40, 80.00s/it]  Running generate_until requests:  48%|████▊     | 40/83 [58:11<1:04:36, 90.16s/it]Running generate_until requests:  49%|████▉     | 41/83 [59:05<55:38, 79.48s/it]  Running generate_until requests:  51%|█████     | 42/83 [1:00:23<54:00, 79.05s/it]Running generate_until requests:  52%|█████▏    | 43/83 [1:01:45<53:14, 79.85s/it]Running generate_until requests:  53%|█████▎    | 44/83 [1:02:39<46:55, 72.19s/it]Running generate_until requests:  54%|█████▍    | 45/83 [1:03:46<44:42, 70.58s/it]Running generate_until requests:  55%|█████▌    | 46/83 [1:04:53<42:50, 69.46s/it]Running generate_until requests:  57%|█████▋    | 47/83 [1:05:43<38:13, 63.71s/it]Running generate_until requests:  58%|█████▊    | 48/83 [1:06:32<34:34, 59.26s/it]Running generate_until requests:  59%|█████▉    | 49/83 [1:07:11<30:09, 53.22s/it]Running generate_until requests:  60%|██████    | 50/83 [1:08:30<33:33, 61.02s/it]Running generate_until requests:  61%|██████▏   | 51/83 [1:10:01<37:14, 69.81s/it]Running generate_until requests:  63%|██████▎   | 52/83 [1:10:44<31:58, 61.89s/it]Running generate_until requests:  64%|██████▍   | 53/83 [1:11:39<29:49, 59.64s/it]Running generate_until requests:  65%|██████▌   | 54/83 [1:12:37<28:39, 59.28s/it]Running generate_until requests:  66%|██████▋   | 55/83 [1:13:58<30:39, 65.70s/it]Running generate_until requests:  67%|██████▋   | 56/83 [1:16:58<45:00, 100.00s/it]Running generate_until requests:  69%|██████▊   | 57/83 [1:18:51<45:07, 104.12s/it]Running generate_until requests:  70%|██████▉   | 58/83 [1:20:15<40:48, 97.93s/it] Running generate_until requests:  71%|███████   | 59/83 [1:21:09<33:57, 84.90s/it]Running generate_until requests:  72%|███████▏  | 60/83 [1:22:00<28:37, 74.65s/it]Running generate_until requests:  73%|███████▎  | 61/83 [1:23:43<30:29, 83.14s/it]Running generate_until requests:  75%|███████▍  | 62/83 [1:24:57<28:07, 80.37s/it]Running generate_until requests:  76%|███████▌  | 63/83 [1:26:11<26:06, 78.34s/it]Running generate_until requests:  77%|███████▋  | 64/83 [1:27:17<23:42, 74.85s/it]Running generate_until requests:  78%|███████▊  | 65/83 [1:28:10<20:28, 68.28s/it]Running generate_until requests:  80%|███████▉  | 66/83 [1:28:59<17:41, 62.44s/it]Running generate_until requests:  81%|████████  | 67/83 [1:29:40<14:53, 55.86s/it]Running generate_until requests:  82%|████████▏ | 68/83 [1:30:56<15:30, 62.01s/it]Running generate_until requests:  83%|████████▎ | 69/83 [1:33:30<20:53, 89.53s/it]Running generate_until requests:  84%|████████▍ | 70/83 [1:35:02<19:34, 90.32s/it]Running generate_until requests:  86%|████████▌ | 71/83 [1:35:46<15:19, 76.61s/it]Running generate_until requests:  87%|████████▋ | 72/83 [1:36:45<13:02, 71.17s/it]Running generate_until requests:  88%|████████▊ | 73/83 [1:37:49<11:29, 69.00s/it]Running generate_until requests:  89%|████████▉ | 74/83 [1:39:06<10:43, 71.54s/it]Running generate_until requests:  90%|█████████ | 75/83 [1:41:02<11:17, 84.63s/it]Running generate_until requests:  92%|█████████▏| 76/83 [1:41:35<08:04, 69.15s/it]Running generate_until requests:  93%|█████████▎| 77/83 [1:42:29<06:27, 64.63s/it]Running generate_until requests:  94%|█████████▍| 78/83 [1:43:39<05:32, 66.42s/it]Running generate_until requests:  95%|█████████▌| 79/83 [1:44:36<04:14, 63.64s/it]Running generate_until requests:  96%|█████████▋| 80/83 [1:46:00<03:28, 69.50s/it]Running generate_until requests:  98%|█████████▊| 81/83 [1:47:17<02:23, 71.92s/it]Running generate_until requests:  99%|█████████▉| 82/83 [1:48:31<01:12, 72.38s/it]Running generate_until requests: 100%|██████████| 83/83 [1:49:40<00:00, 71.43s/it]Running generate_until requests: 100%|██████████| 83/83 [1:49:40<00:00, 79.28s/it]
[2024-06-04 05:20:17,968] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -7) local_rank: 0 (pid: 885544) of binary: /private/home/beidic/.conda/envs/griffin/bin/python
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
main.py FAILED
------------------------------------------------------
Failures:
[1]:
  time      : 2024-06-04_05:20:17
  host      : learnfair5292.h2.fair
  rank      : 1 (local_rank: 1)
  exitcode  : -7 (pid: 885545)
  error_file: <N/A>
  traceback : Signal 7 (SIGBUS) received by PID 885545
[2]:
  time      : 2024-06-04_05:20:17
  host      : learnfair5292.h2.fair
  rank      : 4 (local_rank: 4)
  exitcode  : -7 (pid: 885548)
  error_file: <N/A>
  traceback : Signal 7 (SIGBUS) received by PID 885548
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_05:20:17
  host      : learnfair5292.h2.fair
  rank      : 0 (local_rank: 0)
  exitcode  : -7 (pid: 885544)
  error_file: <N/A>
  traceback : Signal 7 (SIGBUS) received by PID 885544
======================================================
/var/spool/slurm//job28562800/slurm_script: line 65: 885533 Bus error               (core dumped) accelerate launch --main_process_port 29510 --num_processes 8 --num_machines 1 main.py --model xhf --model_args pretrained=meta-llama/Meta-Llama-3-8B-Instruct,cats=True,spr=$sparsitylevel,check=True,kernel_size=16,thr=0.1 --tasks gsm8k --batch_size 1 --limit 0.5
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:05:20:32,008 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:20:32,009 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:20:32,009 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:20:32,011 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:20:32,068 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:20:32,139 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:20:32,281 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:20:33,632 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:05:20:38,995 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:20:38,996 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:20:38,997 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:20:38,997 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:20:39,001 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:20:39,001 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:05:20:39,002 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:20:39,002 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:05:20:39,015 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:20:39,016 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:20:39,020 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:20:39,020 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:05:20:39,049 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:20:39,050 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:20:39,056 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:20:39,057 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:05:20:39,091 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:20:39,092 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:20:39,097 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:20:39,097 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:05:20:39,161 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:20:39,162 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:20:39,167 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:20:39,167 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:05:20:43,383 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:20:43,385 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:20:43,389 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:20:43,389 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.20s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.89s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.89s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.91s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.87s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.97s/it]2024-06-04:05:20:44,127 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:05:20:44,129 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:05:20:44,133 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:05:20:44,134 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.11s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.99s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.26s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.91s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.39s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.36s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.52s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.37s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.37s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.40s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:16,  5.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.79s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.14s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.27s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:05:22:05,001 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:22:05,077 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:22:05,079 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:22:05,468 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 130.48it/s] 34%|███▎      | 28/83 [00:00<00:00, 127.87it/s] 51%|█████     | 42/83 [00:00<00:00, 129.56it/s] 67%|██████▋   | 56/83 [00:00<00:00, 130.46it/s] 84%|████████▍ | 70/83 [00:00<00:00, 130.85it/s]100%|██████████| 83/83 [00:00<00:00, 130.52it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:22:09,971 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:22:09,973 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:22:10,038 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:22:10,040 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:22:10,153 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:22:10,156 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:22:10,228 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:05:22:10,303 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/82 [00:00<00:00, 147.54it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:22:10,363 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:22:10,366 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 16%|█▌        | 13/83 [00:00<00:00, 126.85it/s] 37%|███▋      | 30/82 [00:00<00:00, 137.87it/s]2024-06-04:05:22:10,471 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 31%|███▏      | 26/83 [00:00<00:00, 128.55it/s] 54%|█████▎    | 44/82 [00:00<00:00, 134.06it/s] 17%|█▋        | 14/82 [00:00<00:00, 131.95it/s] 48%|████▊     | 40/83 [00:00<00:00, 130.72it/s] 71%|███████   | 58/82 [00:00<00:00, 132.27it/s]2024-06-04:05:22:10,678 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 34%|███▍      | 28/82 [00:00<00:00, 132.18it/s] 65%|██████▌   | 54/83 [00:00<00:00, 131.23it/s] 88%|████████▊ | 72/82 [00:00<00:00, 132.34it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 17%|█▋        | 14/82 [00:00<00:00, 132.37it/s] 51%|█████     | 42/82 [00:00<00:00, 132.19it/s] 82%|████████▏ | 68/83 [00:00<00:00, 131.69it/s]100%|██████████| 82/82 [00:00<00:00, 134.09it/s]
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:22:10,885 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:22:10,887 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 34%|███▍      | 28/82 [00:00<00:00, 131.43it/s] 68%|██████▊   | 56/82 [00:00<00:00, 132.37it/s] 99%|█████████▉| 82/83 [00:00<00:00, 131.95it/s]100%|██████████| 83/83 [00:00<00:00, 131.14it/s]
 51%|█████     | 42/82 [00:00<00:00, 130.52it/s] 85%|████████▌ | 70/82 [00:00<00:00, 128.83it/s] 68%|██████▊   | 56/82 [00:00<00:00, 130.73it/s]100%|██████████| 82/82 [00:00<00:00, 128.48it/s]
 85%|████████▌ | 70/82 [00:00<00:00, 130.00it/s]2024-06-04:05:22:11,288 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s]100%|██████████| 82/82 [00:00<00:00, 131.02it/s]
 16%|█▌        | 13/83 [00:00<00:00, 127.38it/s] 31%|███▏      | 26/83 [00:00<00:00, 127.26it/s] 47%|████▋     | 39/83 [00:00<00:00, 127.24it/s] 63%|██████▎   | 52/83 [00:00<00:00, 128.04it/s] 80%|███████▉  | 66/83 [00:00<00:00, 129.40it/s] 96%|█████████▋| 80/83 [00:00<00:00, 130.10it/s]100%|██████████| 83/83 [00:00<00:00, 129.13it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:22:36,480 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:22:36,483 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:22:37,033 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 15%|█▍        | 12/82 [00:00<00:00, 119.55it/s] 30%|███       | 25/82 [00:00<00:00, 120.08it/s] 46%|████▋     | 38/82 [00:00<00:00, 121.25it/s] 62%|██████▏   | 51/82 [00:00<00:00, 121.24it/s] 78%|███████▊  | 64/82 [00:00<00:00, 121.22it/s] 94%|█████████▍| 77/82 [00:00<00:00, 121.22it/s]100%|██████████| 82/82 [00:00<00:00, 121.04it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:05:22:57,034 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:22:57,037 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:05:22:57,744 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 10%|▉         | 8/83 [00:00<00:00, 79.84it/s] 23%|██▎       | 19/83 [00:00<00:00, 94.49it/s] 35%|███▍      | 29/83 [00:00<00:00, 77.71it/s] 46%|████▌     | 38/83 [00:00<00:00, 76.58it/s] 55%|█████▌    | 46/83 [00:00<00:00, 71.83it/s] 65%|██████▌   | 54/83 [00:00<00:00, 73.42it/s] 75%|███████▍  | 62/83 [00:00<00:00, 69.73it/s] 88%|████████▊ | 73/83 [00:00<00:00, 79.61it/s] 99%|█████████▉| 82/83 [00:01<00:00, 71.60it/s]100%|██████████| 83/83 [00:01<00:00, 74.06it/s]
2024-06-04:05:23:08,808 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:05:23:08,808 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:05:23:08,808 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:05:23:08,808 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:05:23:08,808 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:05:23:08,808 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:05:23:08,809 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:05:23:08,809 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [01:50<2:30:22, 110.03s/it]Running generate_until requests:   2%|▏         | 2/83 [03:16<2:09:53, 96.22s/it] Running generate_until requests:   4%|▎         | 3/83 [04:05<1:39:39, 74.74s/it]Running generate_until requests:   5%|▍         | 4/83 [04:37<1:16:09, 57.84s/it]Running generate_until requests:   6%|▌         | 5/83 [05:08<1:02:29, 48.07s/it]Running generate_until requests:   7%|▋         | 6/83 [05:44<56:21, 43.92s/it]  Running generate_until requests:   8%|▊         | 7/83 [07:29<1:20:52, 63.85s/it]Running generate_until requests:  10%|▉         | 8/83 [08:18<1:14:07, 59.30s/it]Running generate_until requests:  11%|█         | 9/83 [09:29<1:17:30, 62.84s/it]Running generate_until requests:  12%|█▏        | 10/83 [09:55<1:02:33, 51.42s/it]Running generate_until requests:  13%|█▎        | 11/83 [10:37<58:16, 48.57s/it]  Running generate_until requests:  14%|█▍        | 12/83 [11:44<1:04:12, 54.26s/it]Running generate_until requests:  16%|█▌        | 13/83 [12:02<50:21, 43.17s/it]  Running generate_until requests:  17%|█▋        | 14/83 [12:55<53:16, 46.33s/it]Running generate_until requests:  18%|█▊        | 15/83 [14:43<1:13:24, 64.78s/it]Running generate_until requests:  19%|█▉        | 16/83 [16:38<1:29:24, 80.06s/it]Running generate_until requests:  20%|██        | 17/83 [17:30<1:18:43, 71.56s/it]Running generate_until requests:  22%|██▏       | 18/83 [18:24<1:11:47, 66.27s/it]Running generate_until requests:  23%|██▎       | 19/83 [19:23<1:08:22, 64.11s/it]Running generate_until requests:  24%|██▍       | 20/83 [20:50<1:14:18, 70.78s/it]Running generate_until requests:  25%|██▌       | 21/83 [21:34<1:04:52, 62.77s/it]Running generate_until requests:  27%|██▋       | 22/83 [22:18<58:12, 57.26s/it]  Running generate_until requests:  28%|██▊       | 23/83 [23:55<1:09:07, 69.13s/it]Running generate_until requests:  29%|██▉       | 24/83 [24:39<1:00:27, 61.48s/it]Running generate_until requests:  30%|███       | 25/83 [25:22<54:16, 56.14s/it]  Running generate_until requests:  31%|███▏      | 26/83 [26:19<53:25, 56.24s/it]Running generate_until requests:  33%|███▎      | 27/83 [28:12<1:08:35, 73.49s/it]Running generate_until requests:  34%|███▎      | 28/83 [28:52<58:00, 63.27s/it]  Running generate_until requests:  35%|███▍      | 29/83 [30:24<1:04:39, 71.84s/it]Running generate_until requests:  36%|███▌      | 30/83 [31:49<1:07:00, 75.86s/it]Running generate_until requests:  37%|███▋      | 31/83 [32:30<56:42, 65.43s/it]  Running generate_until requests:  39%|███▊      | 32/83 [33:38<56:23, 66.34s/it]Running generate_until requests:  40%|███▉      | 33/83 [33:59<43:45, 52.51s/it]Running generate_until requests:  41%|████      | 34/83 [35:47<56:39, 69.37s/it]Running generate_until requests:  42%|████▏     | 35/83 [36:27<48:16, 60.34s/it]Running generate_until requests:  43%|████▎     | 36/83 [37:31<48:08, 61.45s/it]Running generate_until requests:  45%|████▍     | 37/83 [37:59<39:28, 51.49s/it]Running generate_until requests:  46%|████▌     | 38/83 [39:27<46:48, 62.42s/it]Running generate_until requests:  47%|████▋     | 39/83 [39:45<35:57, 49.03s/it]Running generate_until requests:  48%|████▊     | 40/83 [41:04<41:41, 58.19s/it]Running generate_until requests:  49%|████▉     | 41/83 [41:50<38:00, 54.30s/it]Running generate_until requests:  51%|█████     | 42/83 [43:05<41:30, 60.74s/it]Running generate_until requests:  52%|█████▏    | 43/83 [43:46<36:32, 54.82s/it]Running generate_until requests:  53%|█████▎    | 44/83 [44:25<32:25, 49.89s/it]Running generate_until requests:  54%|█████▍    | 45/83 [45:01<29:04, 45.91s/it]Running generate_until requests:  55%|█████▌    | 46/83 [45:58<30:14, 49.05s/it]Running generate_until requests:  57%|█████▋    | 47/83 [46:51<30:12, 50.34s/it]Running generate_until requests:  58%|█████▊    | 48/83 [47:17<25:04, 42.99s/it]Running generate_until requests:  59%|█████▉    | 49/83 [47:47<22:11, 39.15s/it]Running generate_until requests:  60%|██████    | 50/83 [48:30<22:06, 40.20s/it]Running generate_until requests:  61%|██████▏   | 51/83 [49:07<20:54, 39.20s/it]Running generate_until requests:  63%|██████▎   | 52/83 [49:31<17:56, 34.74s/it]Running generate_until requests:  64%|██████▍   | 53/83 [50:20<19:34, 39.14s/it]Running generate_until requests:  65%|██████▌   | 54/83 [50:56<18:27, 38.18s/it]Running generate_until requests:  66%|██████▋   | 55/83 [51:36<18:06, 38.79s/it]Running generate_until requests:  67%|██████▋   | 56/83 [52:54<22:41, 50.44s/it]Running generate_until requests:  69%|██████▊   | 57/83 [54:07<24:47, 57.23s/it]Running generate_until requests:  70%|██████▉   | 58/83 [54:45<21:24, 51.37s/it]Running generate_until requests:  71%|███████   | 59/83 [55:14<17:49, 44.58s/it]Running generate_until requests:  72%|███████▏  | 60/83 [55:48<15:52, 41.42s/it]Running generate_until requests:  73%|███████▎  | 61/83 [56:49<17:19, 47.27s/it]Running generate_until requests:  75%|███████▍  | 62/83 [57:14<14:13, 40.65s/it]Running generate_until requests:  76%|███████▌  | 63/83 [57:45<12:39, 37.95s/it]Running generate_until requests:  77%|███████▋  | 64/83 [59:06<16:02, 50.65s/it]Running generate_until requests:  78%|███████▊  | 65/83 [59:30<12:51, 42.86s/it]Running generate_until requests:  80%|███████▉  | 66/83 [59:55<10:34, 37.34s/it]Running generate_until requests:  81%|████████  | 67/83 [1:00:18<08:51, 33.20s/it]Running generate_until requests:  82%|████████▏ | 68/83 [1:00:54<08:27, 33.85s/it]Running generate_until requests:  83%|████████▎ | 69/83 [1:02:06<10:36, 45.43s/it]Running generate_until requests:  84%|████████▍ | 70/83 [1:02:59<10:20, 47.71s/it]Running generate_until requests:  86%|████████▌ | 71/83 [1:03:13<07:31, 37.64s/it]Running generate_until requests:  87%|████████▋ | 72/83 [1:03:39<06:14, 34.03s/it]Running generate_until requests:  88%|████████▊ | 73/83 [1:04:04<05:12, 31.23s/it]Running generate_until requests:  89%|████████▉ | 74/83 [1:04:41<04:57, 33.08s/it]Running generate_until requests:  90%|█████████ | 75/83 [1:05:36<05:17, 39.73s/it]Running generate_until requests:  92%|█████████▏| 76/83 [1:06:15<04:36, 39.43s/it]Running generate_until requests:  93%|█████████▎| 77/83 [1:06:52<03:51, 38.56s/it]Running generate_until requests:  94%|█████████▍| 78/83 [1:07:23<03:02, 36.43s/it]Running generate_until requests:  95%|█████████▌| 79/83 [1:08:17<02:46, 41.68s/it]Running generate_until requests:  96%|█████████▋| 80/83 [1:09:09<02:14, 44.89s/it]Running generate_until requests:  98%|█████████▊| 81/83 [1:09:41<01:22, 41.02s/it]Running generate_until requests:  99%|█████████▉| 82/83 [1:10:16<00:39, 39.16s/it]Running generate_until requests: 100%|██████████| 83/83 [1:10:52<00:00, 38.25s/it]Running generate_until requests: 100%|██████████| 83/83 [1:10:52<00:00, 51.24s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:06:34:46,392 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:34:46,393 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:34:46,393 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:34:46,409 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:34:46,424 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:34:46,825 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:34:48,377 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:34:48,388 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:34:53,238 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:34:53,239 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:34:53,244 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:34:53,244 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:34:53,303 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:34:53,304 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:34:53,309 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:34:53,309 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:34:53,331 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:34:53,332 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:34:53,338 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:34:53,338 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:34:53,357 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:34:53,358 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:34:53,363 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:34:53,363 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:34:53,403 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:34:53,405 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:34:53,409 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:34:53,409 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:34:53,739 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:34:53,740 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:34:53,745 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:34:53,745 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  3.00s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.89s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.92s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.90s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.93s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.01s/it]2024-06-04:06:34:58,628 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:34:58,631 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:34:58,639 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:34:58,639 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:34:58,650 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:34:58,652 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:34:58,657 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:34:58,657 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  3.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.99s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.09s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.99s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.39s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.95s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:03,  3.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:03,  3.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.15s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.42s/it]

Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.49s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.45s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.44s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.46s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:17,  5.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.84s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  3.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.26s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:36:23,995 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:36:23,997 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:36:24,003 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:36:24,006 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:36:24,328 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
2024-06-04:06:36:24,328 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]  0%|          | 0/82 [00:00<?, ?it/s] 16%|█▌        | 13/82 [00:00<00:00, 128.69it/s] 16%|█▌        | 13/82 [00:00<00:00, 128.72it/s] 33%|███▎      | 27/82 [00:00<00:00, 129.47it/s] 33%|███▎      | 27/82 [00:00<00:00, 129.46it/s] 49%|████▉     | 40/82 [00:00<00:00, 129.46it/s] 49%|████▉     | 40/82 [00:00<00:00, 129.45it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 65%|██████▍   | 53/82 [00:00<00:00, 129.56it/s] 65%|██████▍   | 53/82 [00:00<00:00, 129.56it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:36:24,819 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:36:24,821 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 80%|████████  | 66/82 [00:00<00:00, 129.61it/s] 80%|████████  | 66/82 [00:00<00:00, 129.62it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:36:24,882 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:36:24,884 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:36:24,948 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:36:24,950 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 98%|█████████▊| 80/82 [00:00<00:00, 129.92it/s] 98%|█████████▊| 80/82 [00:00<00:00, 129.92it/s]100%|██████████| 82/82 [00:00<00:00, 129.65it/s]100%|██████████| 82/82 [00:00<00:00, 129.65it/s]

Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:36:24,993 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:36:24,995 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:36:25,104 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:06:36:25,192 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 134.02it/s]2024-06-04:06:36:25,289 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
 17%|█▋        | 14/82 [00:00<00:00, 139.04it/s]  0%|          | 0/83 [00:00<?, ?it/s] 34%|███▎      | 28/83 [00:00<00:00, 137.28it/s]2024-06-04:06:36:25,346 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 34%|███▍      | 28/82 [00:00<00:00, 137.45it/s] 17%|█▋        | 14/83 [00:00<00:00, 134.06it/s] 51%|█████     | 42/83 [00:00<00:00, 138.36it/s] 17%|█▋        | 14/82 [00:00<00:00, 132.29it/s] 52%|█████▏    | 43/82 [00:00<00:00, 139.33it/s] 34%|███▎      | 28/83 [00:00<00:00, 133.74it/s] 69%|██████▊   | 57/83 [00:00<00:00, 139.73it/s] 34%|███▍      | 28/82 [00:00<00:00, 133.07it/s] 71%|███████   | 58/82 [00:00<00:00, 140.21it/s] 51%|█████     | 42/83 [00:00<00:00, 133.60it/s] 87%|████████▋ | 72/83 [00:00<00:00, 140.30it/s] 51%|█████     | 42/82 [00:00<00:00, 133.22it/s]100%|██████████| 83/83 [00:00<00:00, 139.55it/s]
 67%|██████▋   | 56/83 [00:00<00:00, 134.82it/s] 89%|████████▉ | 73/82 [00:00<00:00, 140.94it/s]100%|██████████| 82/82 [00:00<00:00, 144.85it/s]
 73%|███████▎  | 60/82 [00:00<00:00, 149.42it/s] 93%|█████████▎| 77/83 [00:00<00:00, 159.67it/s]100%|██████████| 83/83 [00:00<00:00, 151.46it/s]
 99%|█████████▉| 81/82 [00:00<00:00, 168.99it/s]100%|██████████| 82/82 [00:00<00:00, 155.80it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:37:02,491 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:37:02,494 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:37:02,858 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 16%|█▌        | 13/83 [00:00<00:00, 127.34it/s] 31%|███▏      | 26/83 [00:00<00:00, 126.83it/s] 47%|████▋     | 39/83 [00:00<00:00, 125.88it/s] 63%|██████▎   | 52/83 [00:00<00:00, 126.79it/s] 78%|███████▊  | 65/83 [00:00<00:00, 127.81it/s] 94%|█████████▍| 78/83 [00:00<00:00, 128.28it/s]100%|██████████| 83/83 [00:00<00:00, 127.65it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:06:37:04,597 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:37:04,678 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:37:04,682 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:37:05,163 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]  8%|▊         | 7/83 [00:00<00:01, 65.67it/s] 17%|█▋        | 14/83 [00:00<00:01, 66.39it/s] 25%|██▌       | 21/83 [00:00<00:00, 66.69it/s] 34%|███▎      | 28/83 [00:00<00:00, 66.23it/s] 42%|████▏     | 35/83 [00:00<00:00, 66.36it/s] 51%|█████     | 42/83 [00:00<00:00, 66.39it/s] 59%|█████▉    | 49/83 [00:00<00:00, 66.39it/s] 67%|██████▋   | 56/83 [00:00<00:00, 66.56it/s] 76%|███████▌  | 63/83 [00:00<00:00, 66.62it/s] 84%|████████▍ | 70/83 [00:01<00:00, 66.64it/s] 93%|█████████▎| 77/83 [00:01<00:00, 66.61it/s]100%|██████████| 83/83 [00:01<00:00, 66.50it/s]
2024-06-04:06:37:16,868 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:37:16,868 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:37:16,868 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:37:16,868 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:37:16,868 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:37:16,868 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:37:16,868 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:37:16,870 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:25<35:15, 25.80s/it]Running generate_until requests:   2%|▏         | 2/83 [00:56<38:27, 28.49s/it]Running generate_until requests:   4%|▎         | 3/83 [01:09<28:34, 21.43s/it]Running generate_until requests:   5%|▍         | 4/83 [01:24<24:47, 18.83s/it]Running generate_until requests:   6%|▌         | 5/83 [01:39<22:48, 17.54s/it]Running generate_until requests:   7%|▋         | 6/83 [01:51<20:19, 15.84s/it]Running generate_until requests:   8%|▊         | 7/83 [02:26<28:03, 22.15s/it]Running generate_until requests:  10%|▉         | 8/83 [02:44<25:51, 20.69s/it]Running generate_until requests:  11%|█         | 9/83 [02:59<23:19, 18.91s/it]Running generate_until requests:  12%|█▏        | 10/83 [03:08<19:26, 15.98s/it]Running generate_until requests:  13%|█▎        | 11/83 [03:27<20:09, 16.80s/it]Running generate_until requests:  14%|█▍        | 12/83 [03:55<24:01, 20.30s/it]Running generate_until requests:  16%|█▌        | 13/83 [04:15<23:22, 20.04s/it]Running generate_until requests:  17%|█▋        | 14/83 [04:31<21:45, 18.92s/it]Running generate_until requests:  18%|█▊        | 15/83 [05:18<30:52, 27.25s/it]Running generate_until requests:  19%|█▉        | 16/83 [05:55<33:46, 30.24s/it]Running generate_until requests:  20%|██        | 17/83 [06:12<28:57, 26.32s/it]Running generate_until requests:  22%|██▏       | 18/83 [06:23<23:31, 21.72s/it]Running generate_until requests:  23%|██▎       | 19/83 [06:39<21:24, 20.07s/it]Running generate_until requests:  24%|██▍       | 20/83 [07:07<23:35, 22.47s/it]Running generate_until requests:  25%|██▌       | 21/83 [07:26<22:02, 21.33s/it]Running generate_until requests:  27%|██▋       | 22/83 [07:39<19:06, 18.80s/it]Running generate_until requests:  28%|██▊       | 23/83 [08:11<22:53, 22.89s/it]Running generate_until requests:  29%|██▉       | 24/83 [08:28<20:36, 20.96s/it]Running generate_until requests:  30%|███       | 25/83 [08:52<21:10, 21.90s/it]Running generate_until requests:  31%|███▏      | 26/83 [09:06<18:40, 19.66s/it]Running generate_until requests:  33%|███▎      | 27/83 [09:33<20:12, 21.65s/it]Running generate_until requests:  34%|███▎      | 28/83 [09:46<17:35, 19.18s/it]Running generate_until requests:  35%|███▍      | 29/83 [10:12<19:04, 21.19s/it]Running generate_until requests:  36%|███▌      | 30/83 [10:30<17:59, 20.37s/it]Running generate_until requests:  37%|███▋      | 31/83 [10:51<17:35, 20.30s/it]Running generate_until requests:  39%|███▊      | 32/83 [11:11<17:16, 20.32s/it]Running generate_until requests:  40%|███▉      | 33/83 [11:18<13:35, 16.32s/it]Running generate_until requests:  41%|████      | 34/83 [11:42<15:08, 18.55s/it]Running generate_until requests:  42%|████▏     | 35/83 [11:58<14:23, 18.00s/it]Running generate_until requests:  43%|████▎     | 36/83 [12:17<14:15, 18.20s/it]Running generate_until requests:  45%|████▍     | 37/83 [12:32<13:13, 17.24s/it]Running generate_until requests:  46%|████▌     | 38/83 [13:09<17:17, 23.05s/it]Running generate_until requests:  47%|████▋     | 39/83 [13:16<13:32, 18.46s/it]Running generate_until requests:  48%|████▊     | 40/83 [13:31<12:18, 17.16s/it]Running generate_until requests:  49%|████▉     | 41/83 [13:44<11:07, 15.89s/it]Running generate_until requests:  51%|█████     | 42/83 [14:41<19:17, 28.22s/it]Running generate_until requests:  52%|█████▏    | 43/83 [14:53<15:35, 23.40s/it]Running generate_until requests:  53%|█████▎    | 44/83 [15:06<13:14, 20.37s/it]Running generate_until requests:  54%|█████▍    | 45/83 [15:16<10:59, 17.37s/it]Running generate_until requests:  55%|█████▌    | 46/83 [15:28<09:40, 15.70s/it]Running generate_until requests:  57%|█████▋    | 47/83 [15:44<09:31, 15.86s/it]Running generate_until requests:  58%|█████▊    | 48/83 [16:01<09:27, 16.21s/it]Running generate_until requests:  59%|█████▉    | 49/83 [16:11<08:01, 14.16s/it]Running generate_until requests:  60%|██████    | 50/83 [16:31<08:45, 15.92s/it]Running generate_until requests:  61%|██████▏   | 51/83 [16:44<08:03, 15.11s/it]Running generate_until requests:  63%|██████▎   | 52/83 [16:55<07:13, 13.99s/it]Running generate_until requests:  64%|██████▍   | 53/83 [17:07<06:38, 13.30s/it]Running generate_until requests:  65%|██████▌   | 54/83 [17:24<07:00, 14.52s/it]Running generate_until requests:  66%|██████▋   | 55/83 [17:36<06:23, 13.68s/it]Running generate_until requests:  67%|██████▋   | 56/83 [18:15<09:33, 21.24s/it]Running generate_until requests:  69%|██████▊   | 57/83 [18:51<11:09, 25.74s/it]Running generate_until requests:  70%|██████▉   | 58/83 [19:02<08:48, 21.15s/it]Running generate_until requests:  71%|███████   | 59/83 [19:12<07:06, 17.77s/it]Running generate_until requests:  72%|███████▏  | 60/83 [19:29<06:45, 17.62s/it]Running generate_until requests:  73%|███████▎  | 61/83 [19:48<06:34, 17.94s/it]Running generate_until requests:  75%|███████▍  | 62/83 [20:03<06:01, 17.22s/it]Running generate_until requests:  76%|███████▌  | 63/83 [20:16<05:20, 16.02s/it]Running generate_until requests:  77%|███████▋  | 64/83 [20:34<05:11, 16.42s/it]Running generate_until requests:  78%|███████▊  | 65/83 [20:44<04:22, 14.60s/it]Running generate_until requests:  80%|███████▉  | 66/83 [20:55<03:47, 13.40s/it]Running generate_until requests:  81%|████████  | 67/83 [21:03<03:11, 11.98s/it]Running generate_until requests:  82%|████████▏ | 68/83 [21:13<02:50, 11.38s/it]Running generate_until requests:  83%|████████▎ | 69/83 [21:40<03:42, 15.89s/it]Running generate_until requests:  84%|████████▍ | 70/83 [21:57<03:31, 16.26s/it]Running generate_until requests:  86%|████████▌ | 71/83 [22:04<02:41, 13.48s/it]Running generate_until requests:  87%|████████▋ | 72/83 [22:09<02:00, 10.91s/it]Running generate_until requests:  88%|████████▊ | 73/83 [22:22<01:56, 11.62s/it]Running generate_until requests:  89%|████████▉ | 74/83 [22:36<01:52, 12.48s/it]Running generate_until requests:  90%|█████████ | 75/83 [23:03<02:13, 16.67s/it]Running generate_until requests:  92%|█████████▏| 76/83 [23:15<01:46, 15.22s/it]Running generate_until requests:  93%|█████████▎| 77/83 [23:28<01:27, 14.58s/it]Running generate_until requests:  94%|█████████▍| 78/83 [23:38<01:06, 13.28s/it]Running generate_until requests:  95%|█████████▌| 79/83 [24:02<01:06, 16.53s/it]Running generate_until requests:  96%|█████████▋| 80/83 [24:14<00:45, 15.15s/it]Running generate_until requests:  98%|█████████▊| 81/83 [24:26<00:28, 14.07s/it]Running generate_until requests:  99%|█████████▉| 82/83 [24:37<00:13, 13.36s/it]Running generate_until requests: 100%|██████████| 83/83 [24:45<00:00, 11.63s/it]Running generate_until requests: 100%|██████████| 83/83 [24:45<00:00, 17.90s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:07:22:02,903 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:22:02,903 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:22:02,911 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:22:02,989 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:22:03,116 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:22:03,221 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:22:03,269 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:22:04,527 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:22:09,851 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:22:09,851 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:22:09,852 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:22:09,852 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:22:09,852 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:22:09,853 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:22:09,857 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:22:09,857 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:22:09,857 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:22:09,857 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:22:09,858 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:22:09,858 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:22:09,937 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:22:09,938 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:22:09,942 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:22:09,942 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:22:10,143 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:22:10,144 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:22:10,148 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:22:10,148 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:22:10,279 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:22:10,280 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:22:10,284 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:22:10,284 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:07:22:12,718 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:22:12,719 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:22:12,725 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:22:12,725 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.13s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.84s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.91s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.82s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.85s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.01s/it]2024-06-04:07:22:15,122 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:22:15,124 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:22:15,133 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:22:15,134 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.38s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.11s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.93s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.99s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.26s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.92s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.44s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.95s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.40s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.51s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.39s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.37s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.39s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.44s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.86s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.10s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:07:23:40,760 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:23:40,795 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:23:40,797 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:23:40,841 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:23:40,843 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:07:23:41,054 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:23:41,055 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:23:41,057 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:23:41,097 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:23:41,099 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:07:23:41,159 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 16%|█▌        | 13/82 [00:00<00:00, 127.17it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:23:41,201 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:23:41,203 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:23:41,223 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:23:41,225 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 18%|█▊        | 15/83 [00:00<00:00, 141.05it/s] 32%|███▏      | 26/82 [00:00<00:00, 128.19it/s]2024-06-04:07:23:41,352 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
2024-06-04:07:23:41,366 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 48%|████▊     | 39/82 [00:00<00:00, 128.52it/s]  0%|          | 0/83 [00:00<?, ?it/s] 36%|███▌      | 30/83 [00:00<00:00, 135.01it/s] 16%|█▌        | 13/83 [00:00<00:00, 127.51it/s] 65%|██████▍   | 53/82 [00:00<00:00, 131.22it/s] 17%|█▋        | 14/83 [00:00<00:00, 134.63it/s] 53%|█████▎    | 44/83 [00:00<00:00, 133.30it/s]2024-06-04:07:23:41,516 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 33%|███▎      | 27/83 [00:00<00:00, 129.64it/s] 82%|████████▏ | 67/82 [00:00<00:00, 132.68it/s] 34%|███▎      | 28/83 [00:00<00:00, 135.07it/s]2024-06-04:07:23:41,603 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
 70%|██████▉   | 58/83 [00:00<00:00, 132.09it/s]  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 132.95it/s] 49%|████▉     | 41/83 [00:00<00:00, 131.34it/s] 99%|█████████▉| 81/82 [00:00<00:00, 133.24it/s]100%|██████████| 82/82 [00:00<00:00, 131.69it/s]
 51%|█████     | 42/83 [00:00<00:00, 134.74it/s] 87%|████████▋ | 72/83 [00:00<00:00, 131.35it/s] 16%|█▌        | 13/82 [00:00<00:00, 128.94it/s] 34%|███▍      | 28/82 [00:00<00:00, 133.26it/s] 66%|██████▋   | 55/83 [00:00<00:00, 132.49it/s]100%|██████████| 83/83 [00:00<00:00, 132.44it/s]
 75%|███████▍  | 62/83 [00:00<00:00, 159.27it/s] 32%|███▏      | 26/82 [00:00<00:00, 129.09it/s] 51%|█████     | 42/82 [00:00<00:00, 133.78it/s] 83%|████████▎ | 69/83 [00:00<00:00, 133.12it/s] 94%|█████████▍| 78/83 [00:00<00:00, 151.04it/s] 49%|████▉     | 40/82 [00:00<00:00, 132.31it/s]100%|██████████| 83/83 [00:00<00:00, 147.09it/s]
 68%|██████▊   | 56/82 [00:00<00:00, 134.31it/s]100%|██████████| 83/83 [00:00<00:00, 133.75it/s]100%|██████████| 83/83 [00:00<00:00, 132.54it/s]
 66%|██████▌   | 54/82 [00:00<00:00, 129.88it/s] 85%|████████▌ | 70/82 [00:00<00:00, 133.50it/s]100%|██████████| 82/82 [00:00<00:00, 133.07it/s]
 82%|████████▏ | 67/82 [00:00<00:00, 124.59it/s] 98%|█████████▊| 80/82 [00:00<00:00, 120.25it/s]100%|██████████| 82/82 [00:00<00:00, 124.22it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:24:13,730 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:24:13,733 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:24:14,215 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s]  7%|▋         | 6/83 [00:00<00:01, 54.43it/s] 16%|█▌        | 13/83 [00:00<00:01, 61.04it/s] 27%|██▋       | 22/83 [00:00<00:00, 70.16it/s] 36%|███▌      | 30/83 [00:00<00:00, 73.80it/s] 46%|████▌     | 38/83 [00:00<00:00, 71.05it/s] 58%|█████▊    | 48/83 [00:00<00:00, 79.81it/s] 71%|███████   | 59/83 [00:00<00:00, 87.85it/s] 82%|████████▏ | 68/83 [00:00<00:00, 78.92it/s] 94%|█████████▍| 78/83 [00:01<00:00, 82.15it/s]100%|██████████| 83/83 [00:01<00:00, 75.37it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:24:18,122 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:24:18,124 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:24:18,610 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 105.71it/s] 27%|██▋       | 22/82 [00:00<00:00, 106.35it/s] 40%|████      | 33/82 [00:00<00:00, 106.80it/s] 54%|█████▎    | 44/82 [00:00<00:00, 106.69it/s] 67%|██████▋   | 55/82 [00:00<00:00, 106.46it/s] 80%|████████  | 66/82 [00:00<00:00, 106.51it/s] 94%|█████████▍| 77/82 [00:00<00:00, 106.40it/s]100%|██████████| 82/82 [00:00<00:00, 106.45it/s]
2024-06-04:07:24:30,488 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:24:30,488 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:24:30,488 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:24:30,488 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:24:30,488 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:24:30,488 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:24:30,488 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:07:24:30,490 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:19<26:55, 19.70s/it]Running generate_until requests:   2%|▏         | 2/83 [00:36<24:24, 18.09s/it]Running generate_until requests:   4%|▎         | 3/83 [00:46<18:52, 14.16s/it]Running generate_until requests:   5%|▍         | 4/83 [00:52<14:27, 10.99s/it]Running generate_until requests:   6%|▌         | 5/83 [01:01<13:34, 10.45s/it]Running generate_until requests:   7%|▋         | 6/83 [01:12<13:26, 10.47s/it]Running generate_until requests:   8%|▊         | 7/83 [01:27<15:11, 11.99s/it]Running generate_until requests:  10%|▉         | 8/83 [01:36<14:02, 11.23s/it]Running generate_until requests:  11%|█         | 9/83 [01:47<13:31, 10.96s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:55<12:20, 10.15s/it]Running generate_until requests:  13%|█▎        | 11/83 [02:08<13:05, 10.91s/it]Running generate_until requests:  14%|█▍        | 12/83 [02:22<14:11, 11.99s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:28<11:52, 10.19s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:37<11:08,  9.69s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:58<14:59, 13.24s/it]Running generate_until requests:  19%|█▉        | 16/83 [03:18<16:57, 15.19s/it]Running generate_until requests:  20%|██        | 17/83 [03:29<15:17, 13.90s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:37<13:07, 12.11s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:47<12:21, 11.59s/it]Running generate_until requests:  24%|██▍       | 20/83 [04:01<12:58, 12.35s/it]Running generate_until requests:  25%|██▌       | 21/83 [04:13<12:35, 12.18s/it]Running generate_until requests:  27%|██▋       | 22/83 [04:21<11:10, 10.99s/it]Running generate_until requests:  28%|██▊       | 23/83 [04:29<09:57,  9.95s/it]Running generate_until requests:  29%|██▉       | 24/83 [04:36<09:02,  9.19s/it]Running generate_until requests:  30%|███       | 25/83 [04:48<09:34,  9.90s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:56<08:49,  9.29s/it]Running generate_until requests:  33%|███▎      | 27/83 [05:10<09:55, 10.64s/it]Running generate_until requests:  34%|███▎      | 28/83 [05:17<08:53,  9.71s/it]Running generate_until requests:  35%|███▍      | 29/83 [05:30<09:37, 10.69s/it]Running generate_until requests:  36%|███▌      | 30/83 [05:40<09:07, 10.33s/it]Running generate_until requests:  37%|███▋      | 31/83 [05:50<08:58, 10.36s/it]Running generate_until requests:  39%|███▊      | 32/83 [06:00<08:49, 10.38s/it]Running generate_until requests:  40%|███▉      | 33/83 [06:04<07:04,  8.49s/it]Running generate_until requests:  41%|████      | 34/83 [06:19<08:17, 10.16s/it]Running generate_until requests:  42%|████▏     | 35/83 [06:27<07:48,  9.76s/it]Running generate_until requests:  43%|████▎     | 36/83 [06:36<07:27,  9.53s/it]Running generate_until requests:  45%|████▍     | 37/83 [06:48<07:42, 10.05s/it]Running generate_until requests:  46%|████▌     | 38/83 [07:01<08:21, 11.13s/it]Running generate_until requests:  47%|████▋     | 39/83 [07:07<06:58,  9.51s/it]Running generate_until requests:  48%|████▊     | 40/83 [07:16<06:41,  9.34s/it]Running generate_until requests:  49%|████▉     | 41/83 [07:22<05:56,  8.48s/it]Running generate_until requests:  51%|█████     | 42/83 [07:38<07:08, 10.46s/it]Running generate_until requests:  52%|█████▏    | 43/83 [07:46<06:39, 10.00s/it]Running generate_until requests:  53%|█████▎    | 44/83 [07:53<05:46,  8.89s/it]Running generate_until requests:  54%|█████▍    | 45/83 [08:00<05:21,  8.47s/it]Running generate_until requests:  55%|█████▌    | 46/83 [08:07<04:52,  7.91s/it]Running generate_until requests:  57%|█████▋    | 47/83 [08:17<05:10,  8.62s/it]Running generate_until requests:  58%|█████▊    | 48/83 [08:27<05:19,  9.14s/it]Running generate_until requests:  59%|█████▉    | 49/83 [08:32<04:24,  7.77s/it]Running generate_until requests:  60%|██████    | 50/83 [08:43<04:47,  8.73s/it]Running generate_until requests:  61%|██████▏   | 51/83 [08:52<04:44,  8.91s/it]Running generate_until requests:  63%|██████▎   | 52/83 [08:59<04:15,  8.23s/it]Running generate_until requests:  64%|██████▍   | 53/83 [09:09<04:25,  8.84s/it]Running generate_until requests:  65%|██████▌   | 54/83 [09:15<03:53,  8.05s/it]Running generate_until requests:  66%|██████▋   | 55/83 [09:22<03:30,  7.51s/it]Running generate_until requests:  67%|██████▋   | 56/83 [09:39<04:43, 10.51s/it]Running generate_until requests:  69%|██████▊   | 57/83 [09:48<04:21, 10.06s/it]Running generate_until requests:  70%|██████▉   | 58/83 [09:56<03:56,  9.44s/it]Running generate_until requests:  71%|███████   | 59/83 [10:03<03:26,  8.61s/it]Running generate_until requests:  72%|███████▏  | 60/83 [10:09<03:02,  7.94s/it]Running generate_until requests:  73%|███████▎  | 61/83 [10:21<03:17,  8.97s/it]Running generate_until requests:  75%|███████▍  | 62/83 [10:26<02:48,  8.02s/it]Running generate_until requests:  76%|███████▌  | 63/83 [10:35<02:43,  8.17s/it]Running generate_until requests:  77%|███████▋  | 64/83 [10:50<03:14, 10.26s/it]Running generate_until requests:  78%|███████▊  | 65/83 [10:56<02:42,  9.03s/it]Running generate_until requests:  80%|███████▉  | 66/83 [11:02<02:16,  8.05s/it]Running generate_until requests:  81%|████████  | 67/83 [11:06<01:48,  6.78s/it]Running generate_until requests:  82%|████████▏ | 68/83 [11:12<01:40,  6.73s/it]Running generate_until requests:  83%|████████▎ | 69/83 [11:26<02:02,  8.76s/it]Running generate_until requests:  84%|████████▍ | 70/83 [11:36<01:58,  9.08s/it]Running generate_until requests:  86%|████████▌ | 71/83 [11:41<01:35,  7.94s/it]Running generate_until requests:  87%|████████▋ | 72/83 [11:48<01:22,  7.52s/it]Running generate_until requests:  88%|████████▊ | 73/83 [11:55<01:14,  7.44s/it]Running generate_until requests:  89%|████████▉ | 74/83 [12:01<01:02,  7.00s/it]Running generate_until requests:  90%|█████████ | 75/83 [12:17<01:17,  9.66s/it]Running generate_until requests:  92%|█████████▏| 76/83 [12:22<00:58,  8.30s/it]Running generate_until requests:  93%|█████████▎| 77/83 [12:29<00:46,  7.82s/it]Running generate_until requests:  94%|█████████▍| 78/83 [12:36<00:38,  7.72s/it]Running generate_until requests:  95%|█████████▌| 79/83 [12:44<00:30,  7.70s/it]Running generate_until requests:  96%|█████████▋| 80/83 [12:49<00:20,  6.94s/it]Running generate_until requests:  98%|█████████▊| 81/83 [12:57<00:14,  7.18s/it]Running generate_until requests:  99%|█████████▉| 82/83 [13:03<00:06,  6.98s/it]Running generate_until requests: 100%|██████████| 83/83 [13:09<00:00,  6.55s/it]Running generate_until requests: 100%|██████████| 83/83 [13:09<00:00,  9.51s/it]
double free or corruption (!prev)
[2024-06-04 07:43:30,294] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -6) local_rank: 1 (pid: 1007864) of binary: /private/home/beidic/.conda/envs/griffin/bin/python
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
main.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_07:43:30
  host      : learnfair5292.h2.fair
  rank      : 1 (local_rank: 1)
  exitcode  : -6 (pid: 1007864)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 1007864
========================================================
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:07:43:42,413 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:43:42,416 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:43:42,461 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:43:42,472 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:43:42,702 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:43:43,031 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:43:43,045 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:43:43,682 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:43:48,782 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:43:48,784 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:43:48,788 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:43:48,788 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:43:49,282 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:43:49,283 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:43:49,288 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:43:49,288 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:43:49,288 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:43:49,289 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:43:49,294 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:43:49,294 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:43:49,353 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:43:49,354 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:43:49,358 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:43:49,358 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:43:49,707 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:43:49,708 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:43:49,712 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:43:49,712 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:07:43:50,342 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:43:50,343 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:43:50,348 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:43:50,348 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:07:43:53,059 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:43:53,061 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:43:53,067 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:43:53,067 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.93s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.64s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.84s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.93s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.21s/it]2024-06-04:07:43:55,050 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:43:55,055 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:43:55,071 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:43:55,072 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.96s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.90s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.89s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.96s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.12s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.28s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.86s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.40s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.34s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.35s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.35s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.37s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.15s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.42s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:08,  4.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.76s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.52s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:45:19,513 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:45:19,515 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:45:19,559 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:45:19,561 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:45:19,661 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:45:19,663 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:07:45:19,807 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:45:19,823 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
  0%|          | 0/83 [00:00<?, ?it/s]Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:45:19,825 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:45:19,907 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
2024-06-04:07:45:19,920 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
 18%|█▊        | 15/83 [00:00<00:00, 143.95it/s]  0%|          | 0/82 [00:00<?, ?it/s]  0%|          | 0/83 [00:00<?, ?it/s] 16%|█▌        | 13/82 [00:00<00:00, 126.24it/s] 36%|███▌      | 30/83 [00:00<00:00, 143.22it/s] 17%|█▋        | 14/83 [00:00<00:00, 130.90it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 32%|███▏      | 26/82 [00:00<00:00, 127.17it/s] 57%|█████▋    | 47/83 [00:00<00:00, 152.57it/s]2024-06-04:07:45:20,150 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:45:20,155 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:45:20,157 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
  0%|          | 0/82 [00:00<?, ?it/s] 34%|███▎      | 28/83 [00:00<00:00, 122.91it/s] 49%|████▉     | 40/82 [00:00<00:00, 128.57it/s] 76%|███████▌  | 63/83 [00:00<00:00, 146.04it/s] 54%|█████▍    | 45/83 [00:00<00:00, 141.41it/s] 17%|█▋        | 14/82 [00:00<00:00, 130.67it/s] 66%|██████▌   | 54/82 [00:00<00:00, 129.81it/s] 94%|█████████▍| 78/83 [00:00<00:00, 139.18it/s] 34%|███▍      | 28/82 [00:00<00:00, 132.14it/s] 72%|███████▏  | 60/83 [00:00<00:00, 133.02it/s]100%|██████████| 83/83 [00:00<00:00, 141.52it/s]
2024-06-04:07:45:20,441 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 83%|████████▎ | 68/82 [00:00<00:00, 131.11it/s] 51%|█████     | 42/82 [00:00<00:00, 132.73it/s] 89%|████████▉ | 74/83 [00:00<00:00, 130.45it/s] 24%|██▍       | 20/83 [00:00<00:00, 194.96it/s]100%|██████████| 82/82 [00:00<00:00, 131.30it/s]100%|██████████| 82/82 [00:00<00:00, 130.15it/s]
100%|██████████| 83/83 [00:00<00:00, 132.38it/s]
 70%|██████▉   | 57/82 [00:00<00:00, 139.33it/s] 48%|████▊     | 40/83 [00:00<00:00, 196.39it/s] 94%|█████████▍| 77/82 [00:00<00:00, 160.15it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:07:45:20,713 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
100%|██████████| 82/82 [00:00<00:00, 150.85it/s]
 73%|███████▎  | 61/83 [00:00<00:00, 199.10it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:45:20,786 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:45:20,788 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 99%|█████████▉| 82/83 [00:00<00:00, 202.46it/s]100%|██████████| 83/83 [00:00<00:00, 200.62it/s]
2024-06-04:07:45:20,981 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 208.32it/s] 51%|█████     | 42/83 [00:00<00:00, 209.02it/s] 76%|███████▌  | 63/83 [00:00<00:00, 209.36it/s]100%|██████████| 83/83 [00:00<00:00, 209.41it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:45:49,655 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:45:49,657 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:45:50,115 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 108.25it/s] 29%|██▉       | 24/82 [00:00<00:00, 115.94it/s] 45%|████▌     | 37/82 [00:00<00:00, 118.39it/s] 61%|██████    | 50/82 [00:00<00:00, 119.43it/s] 77%|███████▋  | 63/82 [00:00<00:00, 120.01it/s] 93%|█████████▎| 76/82 [00:00<00:00, 120.54it/s]100%|██████████| 82/82 [00:00<00:00, 119.22it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:46:31,293 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:46:31,298 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:46:31,948 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s]  7%|▋         | 6/82 [00:00<00:01, 51.31it/s] 15%|█▍        | 12/82 [00:00<00:01, 54.96it/s] 22%|██▏       | 18/82 [00:00<00:01, 56.55it/s] 29%|██▉       | 24/82 [00:00<00:01, 54.49it/s] 37%|███▋      | 30/82 [00:00<00:00, 53.38it/s] 45%|████▌     | 37/82 [00:00<00:00, 54.60it/s] 52%|█████▏    | 43/82 [00:00<00:00, 52.97it/s] 60%|█████▉    | 49/82 [00:00<00:00, 54.84it/s] 71%|███████   | 58/82 [00:01<00:00, 64.80it/s] 87%|████████▋ | 71/82 [00:01<00:00, 82.21it/s]100%|██████████| 82/82 [00:01<00:00, 89.54it/s]100%|██████████| 82/82 [00:01<00:00, 67.70it/s]
2024-06-04:07:46:43,461 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:46:43,461 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:46:43,461 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:46:43,461 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:46:43,461 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:46:43,461 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:46:43,462 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:46:43,462 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:21<29:04, 21.28s/it]Running generate_until requests:   2%|▏         | 2/83 [00:44<29:58, 22.20s/it]Running generate_until requests:   4%|▎         | 3/83 [00:53<21:55, 16.45s/it]Running generate_until requests:   5%|▍         | 4/83 [00:59<16:12, 12.31s/it]Running generate_until requests:   6%|▌         | 5/83 [01:09<15:01, 11.56s/it]Running generate_until requests:   7%|▋         | 6/83 [01:16<12:39,  9.87s/it]Running generate_until requests:   8%|▊         | 7/83 [01:28<13:15, 10.47s/it]Running generate_until requests:  10%|▉         | 8/83 [01:36<12:24,  9.93s/it]Running generate_until requests:  11%|█         | 9/83 [01:48<12:51, 10.43s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:56<11:45,  9.67s/it]Running generate_until requests:  13%|█▎        | 11/83 [02:04<11:02,  9.20s/it]Running generate_until requests:  14%|█▍        | 12/83 [02:18<12:32, 10.60s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:25<10:58,  9.40s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:35<11:16,  9.80s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:49<12:35, 11.11s/it]Running generate_until requests:  19%|█▉        | 16/83 [03:11<15:59, 14.32s/it]Running generate_until requests:  20%|██        | 17/83 [03:22<14:29, 13.17s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:30<12:48, 11.82s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:38<11:23, 10.68s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:52<12:08, 11.57s/it]Running generate_until requests:  25%|██▌       | 21/83 [04:02<11:18, 10.95s/it]Running generate_until requests:  27%|██▋       | 22/83 [04:09<10:11, 10.02s/it]Running generate_until requests:  28%|██▊       | 23/83 [04:23<11:03, 11.06s/it]Running generate_until requests:  29%|██▉       | 24/83 [04:31<10:08, 10.32s/it]Running generate_until requests:  30%|███       | 25/83 [04:43<10:19, 10.68s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:51<09:16,  9.76s/it]Running generate_until requests:  33%|███▎      | 27/83 [05:04<09:59, 10.70s/it]Running generate_until requests:  34%|███▎      | 28/83 [05:11<09:00,  9.82s/it]Running generate_until requests:  35%|███▍      | 29/83 [05:20<08:30,  9.46s/it]Running generate_until requests:  36%|███▌      | 30/83 [05:32<09:08, 10.35s/it]Running generate_until requests:  37%|███▋      | 31/83 [05:41<08:26,  9.74s/it]Running generate_until requests:  39%|███▊      | 32/83 [05:49<07:56,  9.35s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:55<06:48,  8.18s/it]Running generate_until requests:  41%|████      | 34/83 [06:08<07:57,  9.74s/it]Running generate_until requests:  42%|████▏     | 35/83 [06:16<07:17,  9.11s/it]Running generate_until requests:  43%|████▎     | 36/83 [06:23<06:45,  8.63s/it]Running generate_until requests:  45%|████▍     | 37/83 [06:32<06:38,  8.66s/it]Running generate_until requests:  46%|████▌     | 38/83 [06:48<08:17, 11.05s/it]Running generate_until requests:  47%|████▋     | 39/83 [06:55<07:02,  9.61s/it]Running generate_until requests:  48%|████▊     | 40/83 [07:03<06:40,  9.30s/it]Running generate_until requests:  49%|████▉     | 41/83 [07:10<06:00,  8.59s/it]Running generate_until requests:  51%|█████     | 42/83 [07:17<05:35,  8.20s/it]Running generate_until requests:  52%|█████▏    | 43/83 [07:27<05:45,  8.63s/it]Running generate_until requests:  53%|█████▎    | 44/83 [07:33<05:06,  7.87s/it]Running generate_until requests:  54%|█████▍    | 45/83 [07:41<05:02,  7.95s/it]Running generate_until requests:  55%|█████▌    | 46/83 [07:49<04:48,  7.81s/it]Running generate_until requests:  57%|█████▋    | 47/83 [07:57<04:39,  7.78s/it]Running generate_until requests:  58%|█████▊    | 48/83 [08:08<05:10,  8.86s/it]Running generate_until requests:  59%|█████▉    | 49/83 [08:13<04:21,  7.70s/it]Running generate_until requests:  60%|██████    | 50/83 [08:25<04:56,  8.99s/it]Running generate_until requests:  61%|██████▏   | 51/83 [08:36<05:04,  9.50s/it]Running generate_until requests:  63%|██████▎   | 52/83 [08:41<04:19,  8.36s/it]Running generate_until requests:  64%|██████▍   | 53/83 [08:50<04:15,  8.52s/it]Running generate_until requests:  65%|██████▌   | 54/83 [08:56<03:46,  7.81s/it]Running generate_until requests:  66%|██████▋   | 55/83 [09:03<03:31,  7.56s/it]Running generate_until requests:  67%|██████▋   | 56/83 [09:21<04:49, 10.73s/it]Running generate_until requests:  69%|██████▊   | 57/83 [09:38<05:26, 12.56s/it]Running generate_until requests:  70%|██████▉   | 58/83 [09:46<04:40, 11.23s/it]Running generate_until requests:  71%|███████   | 59/83 [09:53<03:53,  9.75s/it]Running generate_until requests:  72%|███████▏  | 60/83 [09:59<03:18,  8.65s/it]Running generate_until requests:  73%|███████▎  | 61/83 [10:09<03:18,  9.03s/it]Running generate_until requests:  75%|███████▍  | 62/83 [10:16<02:59,  8.52s/it]Running generate_until requests:  76%|███████▌  | 63/83 [10:24<02:45,  8.26s/it]Running generate_until requests:  77%|███████▋  | 64/83 [10:34<02:48,  8.85s/it]Running generate_until requests:  78%|███████▊  | 65/83 [10:41<02:27,  8.22s/it]Running generate_until requests:  80%|███████▉  | 66/83 [10:47<02:09,  7.63s/it]Running generate_until requests:  81%|████████  | 67/83 [10:51<01:44,  6.55s/it]Running generate_until requests:  82%|████████▏ | 68/83 [10:59<01:42,  6.85s/it]Running generate_until requests:  83%|████████▎ | 69/83 [11:11<01:57,  8.42s/it]Running generate_until requests:  84%|████████▍ | 70/83 [11:22<01:59,  9.18s/it]Running generate_until requests:  86%|████████▌ | 71/83 [11:27<01:37,  8.15s/it]Running generate_until requests:  87%|████████▋ | 72/83 [11:34<01:25,  7.81s/it]Running generate_until requests:  88%|████████▊ | 73/83 [11:41<01:16,  7.60s/it]Running generate_until requests:  89%|████████▉ | 74/83 [11:48<01:05,  7.30s/it]Running generate_until requests:  90%|█████████ | 75/83 [12:02<01:14,  9.36s/it]Running generate_until requests:  92%|█████████▏| 76/83 [12:08<00:57,  8.28s/it]Running generate_until requests:  93%|█████████▎| 77/83 [12:16<00:49,  8.32s/it]Running generate_until requests:  94%|█████████▍| 78/83 [12:26<00:43,  8.64s/it]Running generate_until requests:  95%|█████████▌| 79/83 [12:37<00:37,  9.30s/it]Running generate_until requests:  96%|█████████▋| 80/83 [12:42<00:24,  8.19s/it]Running generate_until requests:  98%|█████████▊| 81/83 [12:50<00:15,  7.93s/it]Running generate_until requests:  99%|█████████▉| 82/83 [12:56<00:07,  7.39s/it]Running generate_until requests: 100%|██████████| 83/83 [13:00<00:00,  6.61s/it]Running generate_until requests: 100%|██████████| 83/83 [13:00<00:00,  9.41s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:08:09:22,442 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:09:22,463 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:09:22,469 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:09:22,544 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:09:22,736 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:09:22,851 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:09:22,991 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:09:23,072 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:09:29,274 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:09:29,276 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:09:29,280 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:09:29,280 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:09:29,310 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:09:29,311 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:09:29,316 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:09:29,316 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:09:29,431 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:09:29,432 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:09:29,435 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:09:29,436 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:09:29,745 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:09:29,746 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:09:29,751 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:09:29,751 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:09:29,776 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:09:29,777 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:09:29,781 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:09:29,781 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:09:29,921 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:09:29,922 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:09:29,926 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:09:29,926 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:08:09:32,484 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:09:32,487 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:09:32,495 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:09:32,495 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.79s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.83s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.99s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.05s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.93s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.04s/it]2024-06-04:08:09:35,378 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:09:35,381 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:09:35,390 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:09:35,390 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:06,  3.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.10s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.11s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.98s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.95s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.43s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.46s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.43s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.44s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.49s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.48s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.92s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:16,  5.39s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.15s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:11:00,676 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:11:00,679 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:11:00,758 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:11:00,760 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:11:00,787 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:11:00,789 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:08:11:00,837 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:11:00,912 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:11:00,915 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:11:00,922 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:11:00,986 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:11:00,989 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:11:01,033 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
 16%|█▌        | 13/82 [00:00<00:00, 127.59it/s]2024-06-04:08:11:01,051 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:11:01,054 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
  0%|          | 0/83 [00:00<?, ?it/s]Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:11:01,058 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
  0%|          | 0/83 [00:00<?, ?it/s] 32%|███▏      | 26/82 [00:00<00:00, 127.97it/s] 16%|█▌        | 13/83 [00:00<00:00, 125.19it/s] 16%|█▌        | 13/83 [00:00<00:00, 127.95it/s]2024-06-04:08:11:01,229 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
 48%|████▊     | 39/82 [00:00<00:00, 128.62it/s]  0%|          | 0/83 [00:00<?, ?it/s] 31%|███▏      | 26/83 [00:00<00:00, 126.93it/s] 33%|███▎      | 27/83 [00:00<00:00, 130.59it/s]2024-06-04:08:11:01,302 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 65%|██████▍   | 53/82 [00:00<00:00, 131.27it/s] 17%|█▋        | 14/83 [00:00<00:00, 134.49it/s] 48%|████▊     | 40/83 [00:00<00:00, 128.71it/s] 49%|████▉     | 41/83 [00:00<00:00, 131.46it/s] 17%|█▋        | 14/82 [00:00<00:00, 133.59it/s]2024-06-04:08:11:01,450 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
 82%|████████▏ | 67/82 [00:00<00:00, 132.71it/s] 34%|███▎      | 28/83 [00:00<00:00, 134.55it/s] 65%|██████▌   | 54/83 [00:00<00:00, 131.01it/s]  0%|          | 0/82 [00:00<?, ?it/s] 66%|██████▋   | 55/83 [00:00<00:00, 131.78it/s] 34%|███▍      | 28/82 [00:00<00:00, 134.09it/s] 99%|█████████▉| 81/82 [00:00<00:00, 133.33it/s] 51%|█████     | 42/83 [00:00<00:00, 135.05it/s]100%|██████████| 82/82 [00:00<00:00, 131.74it/s]
 82%|████████▏ | 68/83 [00:00<00:00, 132.31it/s] 17%|█▋        | 14/82 [00:00<00:00, 130.81it/s] 83%|████████▎ | 69/83 [00:00<00:00, 131.64it/s] 51%|█████     | 42/82 [00:00<00:00, 134.15it/s] 75%|███████▍  | 62/83 [00:00<00:00, 159.48it/s] 99%|█████████▉| 82/83 [00:00<00:00, 133.01it/s]100%|██████████| 83/83 [00:00<00:00, 131.31it/s]
 34%|███▍      | 28/82 [00:00<00:00, 129.53it/s]100%|██████████| 83/83 [00:00<00:00, 130.40it/s]100%|██████████| 83/83 [00:00<00:00, 130.72it/s]
 72%|███████▏  | 59/82 [00:00<00:00, 146.19it/s] 99%|█████████▉| 82/83 [00:00<00:00, 172.53it/s]100%|██████████| 83/83 [00:00<00:00, 159.58it/s]
 55%|█████▍    | 45/82 [00:00<00:00, 143.83it/s] 90%|█████████ | 74/82 [00:00<00:00, 144.61it/s]100%|██████████| 82/82 [00:00<00:00, 140.04it/s]
 73%|███████▎  | 60/82 [00:00<00:00, 134.58it/s] 90%|█████████ | 74/82 [00:00<00:00, 130.21it/s]100%|██████████| 82/82 [00:00<00:00, 130.75it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:11:33,623 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:11:33,627 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:11:34,287 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 92.14it/s] 26%|██▌       | 21/82 [00:00<00:00, 69.02it/s] 35%|███▌      | 29/82 [00:00<00:00, 64.83it/s] 44%|████▍     | 36/82 [00:00<00:00, 62.50it/s] 52%|█████▏    | 43/82 [00:00<00:00, 61.48it/s] 66%|██████▌   | 54/82 [00:00<00:00, 73.37it/s] 79%|███████▉  | 65/82 [00:00<00:00, 81.89it/s] 90%|█████████ | 74/82 [00:01<00:00, 72.78it/s]100%|██████████| 82/82 [00:01<00:00, 72.97it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:11:58,407 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:11:58,410 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:11:58,904 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 105.38it/s] 27%|██▋       | 22/83 [00:00<00:00, 105.33it/s] 40%|███▉      | 33/83 [00:00<00:00, 105.31it/s] 53%|█████▎    | 44/83 [00:00<00:00, 105.75it/s] 66%|██████▋   | 55/83 [00:00<00:00, 106.09it/s] 80%|███████▉  | 66/83 [00:00<00:00, 106.31it/s] 93%|█████████▎| 77/83 [00:00<00:00, 105.42it/s]100%|██████████| 83/83 [00:00<00:00, 105.72it/s]
2024-06-04:08:12:11,277 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:12:11,277 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:12:11,277 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:12:11,277 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:12:11,277 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:12:11,278 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:12:11,278 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:12:11,278 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:18<24:56, 18.25s/it]Running generate_until requests:   2%|▏         | 2/83 [00:34<22:43, 16.83s/it]Running generate_until requests:   4%|▎         | 3/83 [00:43<18:02, 13.53s/it]Running generate_until requests:   5%|▍         | 4/83 [00:50<14:26, 10.97s/it]Running generate_until requests:   6%|▌         | 5/83 [01:00<13:53, 10.69s/it]Running generate_until requests:   7%|▋         | 6/83 [01:08<12:11,  9.50s/it]Running generate_until requests:   8%|▊         | 7/83 [01:19<12:59, 10.26s/it]Running generate_until requests:  10%|▉         | 8/83 [01:28<12:13,  9.78s/it]Running generate_until requests:  11%|█         | 9/83 [01:40<12:46, 10.36s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:47<11:30,  9.47s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:56<11:02,  9.20s/it]Running generate_until requests:  14%|█▍        | 12/83 [02:09<12:15, 10.36s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:14<10:16,  8.81s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:23<10:05,  8.77s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:40<12:54, 11.40s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:56<13:59, 12.54s/it]Running generate_until requests:  20%|██        | 17/83 [03:06<12:58, 11.80s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:15<11:53, 10.98s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:23<11:00, 10.32s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:37<11:51, 11.29s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:45<10:45, 10.41s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:54<09:53,  9.73s/it]Running generate_until requests:  28%|██▊       | 23/83 [04:04<10:02, 10.04s/it]Running generate_until requests:  29%|██▉       | 24/83 [04:12<09:16,  9.44s/it]Running generate_until requests:  30%|███       | 25/83 [04:20<08:41,  8.98s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:28<08:15,  8.69s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:41<09:13,  9.89s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:49<08:33,  9.34s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:59<08:30,  9.44s/it]Running generate_until requests:  36%|███▌      | 30/83 [05:12<09:16, 10.51s/it]Running generate_until requests:  37%|███▋      | 31/83 [05:18<08:00,  9.23s/it]Running generate_until requests:  39%|███▊      | 32/83 [05:26<07:39,  9.01s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:32<06:41,  8.03s/it]Running generate_until requests:  41%|████      | 34/83 [05:46<07:53,  9.67s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:53<07:13,  9.04s/it]Running generate_until requests:  43%|████▎     | 36/83 [06:04<07:31,  9.60s/it]Running generate_until requests:  45%|████▍     | 37/83 [06:14<07:23,  9.65s/it]Running generate_until requests:  46%|████▌     | 38/83 [06:30<08:39, 11.53s/it]Running generate_until requests:  47%|████▋     | 39/83 [06:36<07:20, 10.02s/it]Running generate_until requests:  48%|████▊     | 40/83 [06:44<06:34,  9.18s/it]Running generate_until requests:  49%|████▉     | 41/83 [06:50<05:47,  8.27s/it]Running generate_until requests:  51%|█████     | 42/83 [06:57<05:30,  8.05s/it]Running generate_until requests:  52%|█████▏    | 43/83 [07:07<05:45,  8.63s/it]Running generate_until requests:  53%|█████▎    | 44/83 [07:14<05:09,  7.94s/it]Running generate_until requests:  54%|█████▍    | 45/83 [07:22<05:03,  7.98s/it]Running generate_until requests:  55%|█████▌    | 46/83 [07:30<04:56,  8.01s/it]Running generate_until requests:  57%|█████▋    | 47/83 [07:37<04:38,  7.75s/it]Running generate_until requests:  58%|█████▊    | 48/83 [07:46<04:46,  8.18s/it]Running generate_until requests:  59%|█████▉    | 49/83 [07:52<04:14,  7.47s/it]Running generate_until requests:  60%|██████    | 50/83 [08:02<04:34,  8.33s/it]Running generate_until requests:  61%|██████▏   | 51/83 [08:10<04:25,  8.31s/it]Running generate_until requests:  63%|██████▎   | 52/83 [08:18<04:08,  8.01s/it]Running generate_until requests:  64%|██████▍   | 53/83 [08:27<04:10,  8.36s/it]Running generate_until requests:  65%|██████▌   | 54/83 [08:33<03:43,  7.71s/it]Running generate_until requests:  66%|██████▋   | 55/83 [08:41<03:33,  7.64s/it]Running generate_until requests:  67%|██████▋   | 56/83 [08:59<04:52, 10.85s/it]Running generate_until requests:  69%|██████▊   | 57/83 [09:11<04:54, 11.32s/it]Running generate_until requests:  70%|██████▉   | 58/83 [09:20<04:21, 10.47s/it]Running generate_until requests:  71%|███████   | 59/83 [09:26<03:43,  9.30s/it]Running generate_until requests:  72%|███████▏  | 60/83 [09:33<03:12,  8.35s/it]Running generate_until requests:  73%|███████▎  | 61/83 [09:42<03:13,  8.81s/it]Running generate_until requests:  75%|███████▍  | 62/83 [09:51<03:04,  8.78s/it]Running generate_until requests:  76%|███████▌  | 63/83 [09:58<02:41,  8.06s/it]Running generate_until requests:  77%|███████▋  | 64/83 [10:10<02:58,  9.38s/it]Running generate_until requests:  78%|███████▊  | 65/83 [10:17<02:35,  8.65s/it]Running generate_until requests:  80%|███████▉  | 66/83 [10:25<02:22,  8.39s/it]Running generate_until requests:  81%|████████  | 67/83 [10:29<01:56,  7.27s/it]Running generate_until requests:  82%|████████▏ | 68/83 [10:37<01:49,  7.32s/it]Running generate_until requests:  83%|████████▎ | 69/83 [10:50<02:05,  8.95s/it]Running generate_until requests:  84%|████████▍ | 70/83 [11:00<02:01,  9.33s/it]Running generate_until requests:  86%|████████▌ | 71/83 [11:06<01:39,  8.33s/it]Running generate_until requests:  87%|████████▋ | 72/83 [11:17<01:40,  9.11s/it]Running generate_until requests:  88%|████████▊ | 73/83 [11:24<01:25,  8.56s/it]Running generate_until requests:  89%|████████▉ | 74/83 [11:31<01:13,  8.18s/it]Running generate_until requests:  90%|█████████ | 75/83 [11:43<01:14,  9.33s/it]Running generate_until requests:  92%|█████████▏| 76/83 [11:54<01:09,  9.86s/it]Running generate_until requests:  93%|█████████▎| 77/83 [12:01<00:54,  9.04s/it]Running generate_until requests:  94%|█████████▍| 78/83 [12:09<00:43,  8.67s/it]Running generate_until requests:  95%|█████████▌| 79/83 [12:21<00:37,  9.44s/it]Running generate_until requests:  96%|█████████▋| 80/83 [12:30<00:28,  9.34s/it]Running generate_until requests:  98%|█████████▊| 81/83 [12:37<00:17,  8.76s/it]Running generate_until requests:  99%|█████████▉| 82/83 [12:43<00:07,  7.88s/it]Running generate_until requests: 100%|██████████| 83/83 [12:50<00:00,  7.64s/it]Running generate_until requests: 100%|██████████| 83/83 [12:50<00:00,  9.28s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:08:35:09,990 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:35:10,078 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:35:10,158 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:35:11,191 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:35:11,477 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:35:12,604 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:35:13,278 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:35:14,402 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:35:14,403 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:35:14,404 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:35:14,405 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:35:14,407 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:35:14,407 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:35:14,408 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:35:14,408 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:35:14,784 INFO     [main.py:288] Verbosity set to INFO
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:08:35:16,243 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:35:16,244 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:35:16,249 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:35:16,249 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:35:17,065 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:35:17,066 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:35:17,070 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:35:17,070 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.37s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.49s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.16s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.24s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.78s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.16s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]
2024-06-04:08:35:23,546 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:35:23,549 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:35:23,556 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:35:23,557 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:35:23,837 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:35:23,839 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:35:23,847 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:35:23,847 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.13s/it]2024-06-04:08:35:24,423 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:35:24,425 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:35:24,431 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:35:24,431 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.83s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.31s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:08:35:25,945 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:35:25,946 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:35:25,952 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:35:25,952 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.19s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.43s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.41s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.23s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.44s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:06,  3.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.04s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.79s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.26s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:36:09,788 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:36:09,790 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:36:09,976 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 197.57it/s] 49%|████▉     | 40/82 [00:00<00:00, 170.07it/s] 71%|███████   | 58/82 [00:00<00:00, 151.74it/s] 90%|█████████ | 74/82 [00:00<00:00, 149.90it/s]100%|██████████| 82/82 [00:00<00:00, 159.07it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:36:13,527 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:36:13,529 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:36:13,696 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 197.30it/s] 49%|████▉     | 40/82 [00:00<00:00, 198.30it/s] 73%|███████▎  | 60/82 [00:00<00:00, 198.62it/s] 98%|█████████▊| 80/82 [00:00<00:00, 198.81it/s]100%|██████████| 82/82 [00:00<00:00, 198.58it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:36:29,673 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:36:29,675 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:36:29,840 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 205.25it/s] 51%|█████     | 42/82 [00:00<00:00, 202.39it/s] 77%|███████▋  | 63/82 [00:00<00:00, 202.24it/s]100%|██████████| 82/82 [00:00<00:00, 201.79it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:36:30,538 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:36:30,540 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:36:30,739 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 205.34it/s] 51%|█████     | 42/82 [00:00<00:00, 206.27it/s] 77%|███████▋  | 63/82 [00:00<00:00, 206.79it/s]100%|██████████| 82/82 [00:00<00:00, 206.81it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:36:50,041 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:36:50,044 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:36:50,225 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 206.47it/s] 52%|█████▏    | 43/83 [00:00<00:00, 208.95it/s] 78%|███████▊  | 65/83 [00:00<00:00, 209.85it/s]100%|██████████| 83/83 [00:00<00:00, 207.57it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:37:03,251 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:37:03,254 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:37:03,679 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 10%|▉         | 8/83 [00:00<00:00, 75.32it/s] 19%|█▉        | 16/83 [00:00<00:00, 75.94it/s] 29%|██▉       | 24/83 [00:00<00:00, 76.29it/s] 39%|███▊      | 32/83 [00:00<00:00, 75.34it/s] 48%|████▊     | 40/83 [00:00<00:00, 74.46it/s] 58%|█████▊    | 48/83 [00:00<00:00, 74.50it/s] 67%|██████▋   | 56/83 [00:00<00:00, 74.49it/s] 77%|███████▋  | 64/83 [00:00<00:00, 74.41it/s] 87%|████████▋ | 72/83 [00:00<00:00, 74.42it/s] 96%|█████████▋| 80/83 [00:01<00:00, 74.52it/s]100%|██████████| 83/83 [00:01<00:00, 74.74it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:37:17,364 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:37:17,367 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:37:17,556 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 206.21it/s] 51%|█████     | 42/83 [00:00<00:00, 208.20it/s] 76%|███████▌  | 63/83 [00:00<00:00, 208.96it/s]100%|██████████| 83/83 [00:00<00:00, 209.00it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:08:37:40,881 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:37:40,966 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:37:40,969 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:37:41,464 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]  8%|▊         | 7/83 [00:00<00:01, 64.64it/s] 17%|█▋        | 14/83 [00:00<00:01, 65.44it/s] 25%|██▌       | 21/83 [00:00<00:00, 65.81it/s] 34%|███▎      | 28/83 [00:00<00:00, 65.83it/s] 42%|████▏     | 35/83 [00:00<00:00, 65.90it/s] 51%|█████     | 42/83 [00:00<00:00, 65.98it/s] 59%|█████▉    | 49/83 [00:00<00:00, 66.05it/s] 67%|██████▋   | 56/83 [00:00<00:00, 66.12it/s] 76%|███████▌  | 63/83 [00:00<00:00, 66.11it/s] 84%|████████▍ | 70/83 [00:01<00:00, 66.06it/s] 93%|█████████▎| 77/83 [00:01<00:00, 66.17it/s]100%|██████████| 83/83 [00:01<00:00, 66.01it/s]
2024-06-04:08:37:55,607 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:37:55,607 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:37:55,608 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:37:55,608 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:37:55,608 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:37:55,608 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:37:55,608 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:37:55,608 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:14<20:27, 14.97s/it]Running generate_until requests:   2%|▏         | 2/83 [00:35<24:42, 18.30s/it]Running generate_until requests:   4%|▎         | 3/83 [00:45<19:01, 14.27s/it]Running generate_until requests:   5%|▍         | 4/83 [00:51<14:28, 11.00s/it]Running generate_until requests:   6%|▌         | 5/83 [01:01<13:52, 10.67s/it]Running generate_until requests:   7%|▋         | 6/83 [01:08<12:12,  9.51s/it]Running generate_until requests:   8%|▊         | 7/83 [01:19<12:38,  9.97s/it]Running generate_until requests:  10%|▉         | 8/83 [01:28<11:59,  9.59s/it]Running generate_until requests:  11%|█         | 9/83 [01:39<12:36, 10.23s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:46<11:13,  9.23s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:55<10:50,  9.04s/it]Running generate_until requests:  14%|█▍        | 12/83 [02:08<12:02, 10.17s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:13<10:06,  8.66s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:21<09:56,  8.65s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:36<11:43, 10.34s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:51<13:05, 11.72s/it]Running generate_until requests:  20%|██        | 17/83 [03:01<12:23, 11.26s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:10<11:27, 10.58s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:20<11:08, 10.44s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:33<11:56, 11.37s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:40<10:23, 10.06s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:49<09:38,  9.48s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:58<09:23,  9.39s/it]Running generate_until requests:  29%|██▉       | 24/83 [04:06<08:45,  8.91s/it]Running generate_until requests:  30%|███       | 25/83 [04:13<08:08,  8.43s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:21<07:57,  8.37s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:33<08:49,  9.46s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:41<08:16,  9.03s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:49<07:43,  8.59s/it]Running generate_until requests:  36%|███▌      | 30/83 [05:02<08:43,  9.89s/it]Running generate_until requests:  37%|███▋      | 31/83 [05:11<08:20,  9.63s/it]Running generate_until requests:  39%|███▊      | 32/83 [05:20<08:01,  9.44s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:25<06:55,  8.31s/it]Running generate_until requests:  41%|████      | 34/83 [05:39<08:02,  9.85s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:46<07:18,  9.13s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:54<06:48,  8.68s/it]Running generate_until requests:  45%|████▍     | 37/83 [06:03<06:43,  8.78s/it]Running generate_until requests:  46%|████▌     | 38/83 [06:18<08:01, 10.71s/it]Running generate_until requests:  47%|████▋     | 39/83 [06:24<06:54,  9.43s/it]Running generate_until requests:  48%|████▊     | 40/83 [06:32<06:17,  8.77s/it]Running generate_until requests:  49%|████▉     | 41/83 [06:38<05:33,  7.95s/it]Running generate_until requests:  51%|█████     | 42/83 [06:45<05:21,  7.85s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:56<05:41,  8.54s/it]Running generate_until requests:  53%|█████▎    | 44/83 [07:02<05:05,  7.83s/it]Running generate_until requests:  54%|█████▍    | 45/83 [07:10<04:58,  7.85s/it]Running generate_until requests:  55%|█████▌    | 46/83 [07:17<04:49,  7.82s/it]Running generate_until requests:  57%|█████▋    | 47/83 [07:23<04:21,  7.27s/it]Running generate_until requests:  58%|█████▊    | 48/83 [07:33<04:34,  7.86s/it]Running generate_until requests:  59%|█████▉    | 49/83 [07:38<03:58,  7.01s/it]Running generate_until requests:  60%|██████    | 50/83 [07:48<04:24,  8.00s/it]Running generate_until requests:  61%|██████▏   | 51/83 [07:56<04:20,  8.13s/it]Running generate_until requests:  63%|██████▎   | 52/83 [08:04<04:04,  7.87s/it]Running generate_until requests:  64%|██████▍   | 53/83 [08:14<04:17,  8.59s/it]Running generate_until requests:  65%|██████▌   | 54/83 [08:20<03:44,  7.76s/it]Running generate_until requests:  66%|██████▋   | 55/83 [08:26<03:24,  7.32s/it]Running generate_until requests:  67%|██████▋   | 56/83 [08:44<04:44, 10.54s/it]Running generate_until requests:  69%|██████▊   | 57/83 [08:55<04:37, 10.69s/it]Running generate_until requests:  70%|██████▉   | 58/83 [09:04<04:14, 10.18s/it]Running generate_until requests:  71%|███████   | 59/83 [09:10<03:37,  9.06s/it]Running generate_until requests:  72%|███████▏  | 60/83 [09:17<03:08,  8.21s/it]Running generate_until requests:  73%|███████▎  | 61/83 [09:26<03:10,  8.68s/it]Running generate_until requests:  75%|███████▍  | 62/83 [09:34<02:51,  8.19s/it]Running generate_until requests:  76%|███████▌  | 63/83 [09:40<02:35,  7.78s/it]Running generate_until requests:  77%|███████▋  | 64/83 [09:49<02:32,  8.02s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:56<02:18,  7.69s/it]Running generate_until requests:  80%|███████▉  | 66/83 [10:04<02:11,  7.75s/it]Running generate_until requests:  81%|████████  | 67/83 [10:09<01:52,  7.05s/it]Running generate_until requests:  82%|████████▏ | 68/83 [10:16<01:43,  6.87s/it]Running generate_until requests:  83%|████████▎ | 69/83 [10:26<01:51,  7.93s/it]Running generate_until requests:  84%|████████▍ | 70/83 [10:36<01:51,  8.59s/it]Running generate_until requests:  86%|████████▌ | 71/83 [10:42<01:33,  7.77s/it]Running generate_until requests:  87%|████████▋ | 72/83 [10:50<01:25,  7.81s/it]Running generate_until requests:  88%|████████▊ | 73/83 [10:57<01:15,  7.55s/it]Running generate_until requests:  89%|████████▉ | 74/83 [11:04<01:07,  7.46s/it]Running generate_until requests:  90%|█████████ | 75/83 [11:16<01:09,  8.69s/it]Running generate_until requests:  92%|█████████▏| 76/83 [11:25<01:01,  8.81s/it]Running generate_until requests:  93%|█████████▎| 77/83 [11:31<00:47,  7.92s/it]Running generate_until requests:  94%|█████████▍| 78/83 [11:37<00:37,  7.56s/it]Running generate_until requests:  95%|█████████▌| 79/83 [11:48<00:34,  8.56s/it]Running generate_until requests:  96%|█████████▋| 80/83 [11:56<00:24,  8.24s/it]Running generate_until requests:  98%|█████████▊| 81/83 [12:05<00:17,  8.58s/it]Running generate_until requests:  99%|█████████▉| 82/83 [12:11<00:07,  7.75s/it]Running generate_until requests: 100%|██████████| 83/83 [12:17<00:00,  7.28s/it]Running generate_until requests: 100%|██████████| 83/83 [12:17<00:00,  8.89s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:09:02:46,552 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:02:46,552 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:02:46,552 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:02:46,552 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:02:46,552 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:02:46,595 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:02:47,284 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:02:48,579 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:02:53,429 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:02:53,430 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:02:53,434 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:02:53,434 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:02:53,475 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:02:53,477 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:02:53,478 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:02:53,479 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:02:53,481 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:02:53,481 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:02:53,481 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:02:53,483 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:02:53,484 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:02:53,484 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:02:53,488 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:02:53,488 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:02:53,553 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:02:53,554 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:02:53,558 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:02:53,558 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:02:53,636 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:02:53,637 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:02:53,640 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:02:53,641 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:09:02:57,486 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:02:57,488 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:02:57,492 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:02:57,492 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.10s/it]2024-06-04:09:02:58,144 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:02:58,146 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:02:58,155 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:02:58,155 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.92s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.90s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.85s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.94s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.03s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.94s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.26s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.88s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.94s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.47s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.34s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.39s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.39s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.40s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.45s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.19s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:16,  5.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.82s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.25s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.23s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:09:04:24,179 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:04:24,260 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:04:24,262 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:04:24,429 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:04:24,431 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:04:24,432 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:04:24,434 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:04:24,554 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:04:24,556 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:04:24,594 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:09:04:24,675 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:09:04:24,706 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
 20%|██        | 17/83 [00:00<00:00, 161.94it/s]  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:04:24,732 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:04:24,734 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 16%|█▌        | 13/83 [00:00<00:00, 128.85it/s] 18%|█▊        | 15/82 [00:00<00:00, 139.69it/s] 41%|████      | 34/83 [00:00<00:00, 144.41it/s]2024-06-04:09:04:24,896 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
 31%|███▏      | 26/83 [00:00<00:00, 129.17it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/83 [00:00<?, ?it/s] 35%|███▌      | 29/82 [00:00<00:00, 138.04it/s] 59%|█████▉    | 49/83 [00:00<00:00, 138.66it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:04:24,998 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:04:25,000 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 48%|████▊     | 40/83 [00:00<00:00, 131.09it/s] 17%|█▋        | 14/83 [00:00<00:00, 133.22it/s] 57%|█████▋    | 47/82 [00:00<00:00, 152.08it/s] 76%|███████▌  | 63/83 [00:00<00:00, 136.24it/s] 65%|██████▌   | 54/83 [00:00<00:00, 133.00it/s]2024-06-04:09:04:25,122 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
 34%|███▎      | 28/83 [00:00<00:00, 134.23it/s]  0%|          | 0/82 [00:00<?, ?it/s] 77%|███████▋  | 63/82 [00:00<00:00, 142.28it/s] 93%|█████████▎| 77/83 [00:00<00:00, 134.46it/s] 82%|████████▏ | 68/83 [00:00<00:00, 133.93it/s]100%|██████████| 83/83 [00:00<00:00, 137.19it/s]
 51%|█████     | 42/83 [00:00<00:00, 134.72it/s] 16%|█▌        | 13/82 [00:00<00:00, 126.10it/s] 95%|█████████▌| 78/82 [00:00<00:00, 136.94it/s]100%|██████████| 82/82 [00:00<00:00, 139.20it/s]
2024-06-04:09:04:25,314 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
 99%|█████████▉| 82/83 [00:00<00:00, 132.21it/s]100%|██████████| 83/83 [00:00<00:00, 132.03it/s]
  0%|          | 0/82 [00:00<?, ?it/s] 32%|███▏      | 26/82 [00:00<00:00, 127.46it/s] 67%|██████▋   | 56/83 [00:00<00:00, 118.48it/s] 16%|█▌        | 13/82 [00:00<00:00, 128.95it/s] 48%|████▊     | 39/82 [00:00<00:00, 121.46it/s] 83%|████████▎ | 69/83 [00:00<00:00, 119.33it/s] 32%|███▏      | 26/82 [00:00<00:00, 125.63it/s] 63%|██████▎   | 52/82 [00:00<00:00, 117.87it/s] 99%|█████████▉| 82/83 [00:00<00:00, 118.52it/s]100%|██████████| 83/83 [00:00<00:00, 122.14it/s]
 48%|████▊     | 39/82 [00:00<00:00, 116.20it/s] 79%|███████▉  | 65/82 [00:00<00:00, 119.51it/s] 62%|██████▏   | 51/82 [00:00<00:00, 111.19it/s] 94%|█████████▍| 77/82 [00:00<00:00, 112.85it/s]100%|██████████| 82/82 [00:00<00:00, 115.96it/s]
 77%|███████▋  | 63/82 [00:00<00:00, 108.64it/s] 90%|█████████ | 74/82 [00:00<00:00, 107.06it/s]100%|██████████| 82/82 [00:00<00:00, 110.67it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:05:06,821 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:05:06,824 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:05:07,573 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 103.13it/s] 27%|██▋       | 22/83 [00:00<00:00, 103.43it/s] 40%|███▉      | 33/83 [00:00<00:00, 104.69it/s] 53%|█████▎    | 44/83 [00:00<00:00, 104.61it/s] 66%|██████▋   | 55/83 [00:00<00:00, 105.12it/s] 80%|███████▉  | 66/83 [00:00<00:00, 105.31it/s] 93%|█████████▎| 77/83 [00:00<00:00, 105.62it/s]100%|██████████| 83/83 [00:00<00:00, 100.60it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:05:28,467 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:05:28,469 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:05:28,877 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 15%|█▍        | 12/82 [00:00<00:00, 117.90it/s] 29%|██▉       | 24/82 [00:00<00:00, 117.74it/s] 44%|████▍     | 36/82 [00:00<00:00, 117.70it/s] 59%|█████▊    | 48/82 [00:00<00:00, 117.88it/s] 73%|███████▎  | 60/82 [00:00<00:00, 117.78it/s] 88%|████████▊ | 72/82 [00:00<00:00, 117.26it/s]100%|██████████| 82/82 [00:00<00:00, 117.51it/s]
2024-06-04:09:05:40,931 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:05:40,932 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:05:40,931 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:05:40,932 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:05:40,932 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:05:40,932 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:05:40,932 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:05:40,932 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:12<16:32, 12.10s/it]Running generate_until requests:   2%|▏         | 2/83 [00:26<18:24, 13.64s/it]Running generate_until requests:   4%|▎         | 3/83 [00:36<15:37, 11.71s/it]Running generate_until requests:   5%|▍         | 4/83 [00:41<11:54,  9.05s/it]Running generate_until requests:   6%|▌         | 5/83 [00:48<10:42,  8.24s/it]Running generate_until requests:   7%|▋         | 6/83 [00:53<09:33,  7.44s/it]Running generate_until requests:   8%|▊         | 7/83 [01:02<09:51,  7.78s/it]Running generate_until requests:  10%|▉         | 8/83 [01:09<09:25,  7.54s/it]Running generate_until requests:  11%|█         | 9/83 [01:18<09:55,  8.05s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:24<08:48,  7.24s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:31<08:36,  7.18s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:41<09:31,  8.05s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:44<07:45,  6.64s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:51<07:38,  6.64s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:03<09:23,  8.29s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:14<10:18,  9.24s/it]Running generate_until requests:  20%|██        | 17/83 [02:22<09:47,  8.90s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:29<09:02,  8.34s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:38<09:06,  8.53s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:49<09:36,  9.15s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:56<08:52,  8.59s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:03<08:04,  7.94s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:09<07:24,  7.41s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:15<06:57,  7.08s/it]Running generate_until requests:  30%|███       | 25/83 [03:23<07:03,  7.30s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:30<06:45,  7.11s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:37<06:35,  7.07s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:43<06:18,  6.88s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:49<05:57,  6.63s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:59<06:46,  7.67s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:06<06:25,  7.42s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:14<06:29,  7.63s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:19<05:36,  6.73s/it]Running generate_until requests:  41%|████      | 34/83 [04:29<06:19,  7.75s/it]Running generate_until requests:  42%|████▏     | 35/83 [04:35<05:46,  7.23s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:41<05:22,  6.86s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:48<05:18,  6.92s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:00<06:18,  8.41s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:05<05:32,  7.55s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:15<05:53,  8.21s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:20<05:02,  7.21s/it]Running generate_until requests:  51%|█████     | 42/83 [05:26<04:36,  6.76s/it]Running generate_until requests:  52%|█████▏    | 43/83 [05:33<04:43,  7.08s/it]Running generate_until requests:  53%|█████▎    | 44/83 [05:38<04:12,  6.46s/it]Running generate_until requests:  54%|█████▍    | 45/83 [05:45<04:04,  6.43s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:51<03:56,  6.40s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:56<03:31,  5.86s/it]Running generate_until requests:  58%|█████▊    | 48/83 [06:03<03:39,  6.28s/it]Running generate_until requests:  59%|█████▉    | 49/83 [06:07<03:11,  5.64s/it]Running generate_until requests:  60%|██████    | 50/83 [06:15<03:27,  6.30s/it]Running generate_until requests:  61%|██████▏   | 51/83 [06:22<03:25,  6.41s/it]Running generate_until requests:  63%|██████▎   | 52/83 [06:29<03:31,  6.82s/it]Running generate_until requests:  64%|██████▍   | 53/83 [06:37<03:28,  6.96s/it]Running generate_until requests:  65%|██████▌   | 54/83 [06:42<03:03,  6.33s/it]Running generate_until requests:  66%|██████▋   | 55/83 [06:48<02:53,  6.21s/it]Running generate_until requests:  67%|██████▋   | 56/83 [07:02<03:55,  8.70s/it]Running generate_until requests:  69%|██████▊   | 57/83 [07:10<03:43,  8.60s/it]Running generate_until requests:  70%|██████▉   | 58/83 [07:18<03:24,  8.17s/it]Running generate_until requests:  71%|███████   | 59/83 [07:23<02:54,  7.28s/it]Running generate_until requests:  72%|███████▏  | 60/83 [07:28<02:31,  6.60s/it]Running generate_until requests:  73%|███████▎  | 61/83 [07:36<02:32,  6.95s/it]Running generate_until requests:  75%|███████▍  | 62/83 [07:41<02:17,  6.56s/it]Running generate_until requests:  76%|███████▌  | 63/83 [07:46<02:03,  6.17s/it]Running generate_until requests:  77%|███████▋  | 64/83 [07:53<02:00,  6.35s/it]Running generate_until requests:  78%|███████▊  | 65/83 [07:59<01:50,  6.12s/it]Running generate_until requests:  80%|███████▉  | 66/83 [08:03<01:35,  5.63s/it]Running generate_until requests:  81%|████████  | 67/83 [08:08<01:24,  5.30s/it]Running generate_until requests:  82%|████████▏ | 68/83 [08:13<01:18,  5.26s/it]Running generate_until requests:  83%|████████▎ | 69/83 [08:24<01:36,  6.92s/it]Running generate_until requests:  84%|████████▍ | 70/83 [08:32<01:33,  7.20s/it]Running generate_until requests:  86%|████████▌ | 71/83 [08:36<01:14,  6.23s/it]Running generate_until requests:  87%|████████▋ | 72/83 [08:42<01:09,  6.30s/it]Running generate_until requests:  88%|████████▊ | 73/83 [08:48<01:00,  6.09s/it]Running generate_until requests:  89%|████████▉ | 74/83 [08:53<00:53,  5.99s/it]Running generate_until requests:  90%|█████████ | 75/83 [09:03<00:55,  6.97s/it]Running generate_until requests:  92%|█████████▏| 76/83 [09:11<00:51,  7.40s/it]Running generate_until requests:  93%|█████████▎| 77/83 [09:16<00:39,  6.57s/it]Running generate_until requests:  94%|█████████▍| 78/83 [09:20<00:29,  5.94s/it]Running generate_until requests:  95%|█████████▌| 79/83 [09:29<00:26,  6.73s/it]Running generate_until requests:  96%|█████████▋| 80/83 [09:35<00:19,  6.48s/it]Running generate_until requests:  98%|█████████▊| 81/83 [09:42<00:13,  6.68s/it]Running generate_until requests:  99%|█████████▉| 82/83 [09:47<00:06,  6.11s/it]Running generate_until requests: 100%|██████████| 83/83 [09:52<00:00,  5.76s/it]Running generate_until requests: 100%|██████████| 83/83 [09:52<00:00,  7.13s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:09:28:11,341 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:28:11,348 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:28:11,367 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:28:11,374 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:28:11,474 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:28:11,581 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:28:12,587 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:28:13,185 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:28:18,108 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:28:18,109 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:28:18,114 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:28:18,114 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:28:18,121 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:28:18,122 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:28:18,126 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:28:18,126 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:28:18,247 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:28:18,249 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:28:18,253 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:28:18,253 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:28:18,285 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:28:18,286 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:28:18,289 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:28:18,289 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:28:18,424 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:28:18,425 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:28:18,428 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:28:18,428 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:28:18,486 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:28:18,487 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:28:18,493 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:28:18,493 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.09s/it]2024-06-04:09:28:22,845 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:28:22,846 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:28:22,852 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:28:22,853 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.93s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.93s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.83s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.00s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.98s/it]2024-06-04:09:28:23,349 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:28:23,352 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:28:23,362 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:28:23,362 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.93s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.09s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.20s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.26s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.23s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.46s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.35s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.44s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.47s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.43s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.52s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.83s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.72s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:09:29:48,759 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:29:48,844 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:29:48,846 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:29:49,113 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 143.68it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 39%|███▊      | 32/83 [00:00<00:00, 157.45it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:29:49,352 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:29:49,355 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 58%|█████▊    | 48/83 [00:00<00:00, 147.84it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:29:49,539 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:29:49,541 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 76%|███████▌  | 63/83 [00:00<00:00, 139.96it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:29:49,616 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:29:49,619 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:29:49,660 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:29:49,661 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:29:49,672 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
 94%|█████████▍| 78/83 [00:00<00:00, 136.37it/s]  0%|          | 0/83 [00:00<?, ?it/s]100%|██████████| 83/83 [00:00<00:00, 140.20it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 19%|█▉        | 16/83 [00:00<00:00, 156.41it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:29:49,814 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:29:49,817 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:29:49,880 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 39%|███▊      | 32/83 [00:00<00:00, 146.92it/s]2024-06-04:09:29:49,937 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 133.58it/s]2024-06-04:09:29:50,019 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
 57%|█████▋    | 47/83 [00:00<00:00, 137.72it/s]  0%|          | 0/83 [00:00<?, ?it/s] 20%|█▉        | 16/82 [00:00<00:00, 150.67it/s] 34%|███▍      | 28/82 [00:00<00:00, 134.27it/s]2024-06-04:09:29:50,128 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
 73%|███████▎  | 61/83 [00:00<00:00, 133.82it/s]  0%|          | 0/82 [00:00<?, ?it/s] 14%|█▍        | 12/83 [00:00<00:00, 110.45it/s] 51%|█████     | 42/82 [00:00<00:00, 134.26it/s] 39%|███▉      | 32/82 [00:00<00:00, 119.09it/s] 90%|█████████ | 75/83 [00:00<00:00, 133.00it/s] 17%|█▋        | 14/82 [00:00<00:00, 132.83it/s] 29%|██▉       | 24/83 [00:00<00:00, 114.24it/s]100%|██████████| 83/83 [00:00<00:00, 136.29it/s]
 68%|██████▊   | 56/82 [00:00<00:00, 133.07it/s] 55%|█████▍    | 45/82 [00:00<00:00, 112.39it/s] 34%|███▍      | 28/82 [00:00<00:00, 135.04it/s] 45%|████▍     | 37/83 [00:00<00:00, 117.16it/s] 85%|████████▌ | 70/82 [00:00<00:00, 133.51it/s] 51%|█████     | 42/82 [00:00<00:00, 136.81it/s] 70%|██████▉   | 57/82 [00:00<00:00, 109.07it/s] 59%|█████▉    | 49/83 [00:00<00:00, 118.10it/s]100%|██████████| 82/82 [00:00<00:00, 131.41it/s]
 68%|██████▊   | 56/82 [00:00<00:00, 135.33it/s] 83%|████████▎ | 68/82 [00:00<00:00, 107.39it/s] 73%|███████▎  | 61/83 [00:00<00:00, 118.43it/s] 88%|████████▊ | 73/83 [00:00<00:00, 118.91it/s] 96%|█████████▋| 79/82 [00:00<00:00, 106.47it/s] 85%|████████▌ | 70/82 [00:00<00:00, 129.32it/s]100%|██████████| 82/82 [00:00<00:00, 110.55it/s]
100%|██████████| 83/83 [00:00<00:00, 117.99it/s]
100%|██████████| 82/82 [00:00<00:00, 129.32it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:30:24,295 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:30:24,298 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:30:24,905 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s]  9%|▊         | 7/82 [00:00<00:01, 49.54it/s] 16%|█▌        | 13/82 [00:00<00:01, 47.93it/s] 23%|██▎       | 19/82 [00:00<00:01, 49.31it/s] 29%|██▉       | 24/82 [00:00<00:01, 49.54it/s] 39%|███▉      | 32/82 [00:00<00:00, 59.48it/s] 50%|█████     | 41/82 [00:00<00:00, 66.72it/s] 59%|█████▊    | 48/82 [00:00<00:00, 66.48it/s] 68%|██████▊   | 56/82 [00:00<00:00, 70.41it/s] 78%|███████▊  | 64/82 [00:01<00:00, 65.22it/s] 87%|████████▋ | 71/82 [00:01<00:00, 64.75it/s] 95%|█████████▌| 78/82 [00:01<00:00, 59.49it/s]100%|██████████| 82/82 [00:01<00:00, 60.11it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:30:51,213 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:30:51,215 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:30:51,903 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 16%|█▌        | 13/83 [00:00<00:00, 120.28it/s] 31%|███▏      | 26/83 [00:00<00:00, 120.63it/s] 47%|████▋     | 39/83 [00:00<00:00, 121.23it/s] 63%|██████▎   | 52/83 [00:00<00:00, 120.95it/s] 78%|███████▊  | 65/83 [00:00<00:00, 121.11it/s] 94%|█████████▍| 78/83 [00:00<00:00, 120.48it/s]100%|██████████| 83/83 [00:00<00:00, 120.72it/s]
2024-06-04:09:31:02,270 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:31:02,271 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:31:02,271 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:31:02,271 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:31:02,271 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:31:02,271 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:31:02,271 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:31:02,271 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:12<17:15, 12.62s/it]Running generate_until requests:   2%|▏         | 2/83 [00:30<20:59, 15.55s/it]Running generate_until requests:   4%|▎         | 3/83 [00:39<16:58, 12.74s/it]Running generate_until requests:   5%|▍         | 4/83 [00:45<13:13, 10.04s/it]Running generate_until requests:   6%|▌         | 5/83 [00:53<12:14,  9.42s/it]Running generate_until requests:   7%|▋         | 6/83 [01:00<10:53,  8.49s/it]Running generate_until requests:   8%|▊         | 7/83 [01:13<12:43, 10.05s/it]Running generate_until requests:  10%|▉         | 8/83 [01:22<11:58,  9.59s/it]Running generate_until requests:  11%|█         | 9/83 [01:33<12:35, 10.21s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:40<11:02,  9.08s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:48<10:32,  8.78s/it]Running generate_until requests:  14%|█▍        | 12/83 [02:01<11:43,  9.90s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:08<10:38,  9.13s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:16<10:08,  8.83s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:30<11:46, 10.40s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:45<13:05, 11.72s/it]Running generate_until requests:  20%|██        | 17/83 [02:55<12:19, 11.20s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:04<11:21, 10.49s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:12<10:33,  9.89s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:26<11:44, 11.18s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:32<09:43,  9.41s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:40<09:08,  8.98s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:49<09:02,  9.05s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:57<08:32,  8.68s/it]Running generate_until requests:  30%|███       | 25/83 [04:08<09:05,  9.41s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:16<08:36,  9.05s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:25<08:26,  9.04s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:33<07:59,  8.72s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:42<07:55,  8.80s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:53<08:13,  9.32s/it]Running generate_until requests:  37%|███▋      | 31/83 [05:00<07:36,  8.78s/it]Running generate_until requests:  39%|███▊      | 32/83 [05:10<07:49,  9.20s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:16<06:45,  8.11s/it]Running generate_until requests:  41%|████      | 34/83 [05:26<07:06,  8.71s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:33<06:38,  8.31s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:42<06:31,  8.33s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:50<06:20,  8.27s/it]Running generate_until requests:  46%|████▌     | 38/83 [06:01<06:53,  9.20s/it]Running generate_until requests:  47%|████▋     | 39/83 [06:08<06:19,  8.62s/it]Running generate_until requests:  48%|████▊     | 40/83 [06:21<06:57,  9.70s/it]Running generate_until requests:  49%|████▉     | 41/83 [06:27<05:59,  8.56s/it]Running generate_until requests:  51%|█████     | 42/83 [06:35<05:48,  8.51s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:45<05:54,  8.87s/it]Running generate_until requests:  53%|█████▎    | 44/83 [06:51<05:14,  8.07s/it]Running generate_until requests:  54%|█████▍    | 45/83 [06:58<05:01,  7.93s/it]Running generate_until requests:  55%|█████▌    | 46/83 [07:06<04:49,  7.83s/it]Running generate_until requests:  57%|█████▋    | 47/83 [07:12<04:17,  7.16s/it]Running generate_until requests:  58%|█████▊    | 48/83 [07:21<04:30,  7.73s/it]Running generate_until requests:  59%|█████▉    | 49/83 [07:26<03:55,  6.92s/it]Running generate_until requests:  60%|██████    | 50/83 [07:36<04:20,  7.89s/it]Running generate_until requests:  61%|██████▏   | 51/83 [07:43<04:08,  7.77s/it]Running generate_until requests:  63%|██████▎   | 52/83 [07:49<03:39,  7.07s/it]Running generate_until requests:  64%|██████▍   | 53/83 [07:57<03:42,  7.43s/it]Running generate_until requests:  65%|██████▌   | 54/83 [08:03<03:22,  6.99s/it]Running generate_until requests:  66%|██████▋   | 55/83 [08:09<03:10,  6.82s/it]Running generate_until requests:  67%|██████▋   | 56/83 [08:27<04:31, 10.07s/it]Running generate_until requests:  69%|██████▊   | 57/83 [08:39<04:37, 10.69s/it]Running generate_until requests:  70%|██████▉   | 58/83 [08:48<04:13, 10.14s/it]Running generate_until requests:  71%|███████   | 59/83 [08:55<03:36,  9.03s/it]Running generate_until requests:  72%|███████▏  | 60/83 [09:01<03:07,  8.15s/it]Running generate_until requests:  73%|███████▎  | 61/83 [09:10<03:08,  8.57s/it]Running generate_until requests:  75%|███████▍  | 62/83 [09:16<02:41,  7.71s/it]Running generate_until requests:  76%|███████▌  | 63/83 [09:23<02:28,  7.42s/it]Running generate_until requests:  77%|███████▋  | 64/83 [09:33<02:39,  8.41s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:40<02:22,  7.94s/it]Running generate_until requests:  80%|███████▉  | 66/83 [09:46<02:01,  7.17s/it]Running generate_until requests:  81%|████████  | 67/83 [09:51<01:46,  6.65s/it]Running generate_until requests:  82%|████████▏ | 68/83 [09:57<01:38,  6.58s/it]Running generate_until requests:  83%|████████▎ | 69/83 [10:12<02:04,  8.89s/it]Running generate_until requests:  84%|████████▍ | 70/83 [10:22<01:59,  9.18s/it]Running generate_until requests:  86%|████████▌ | 71/83 [10:26<01:34,  7.88s/it]Running generate_until requests:  87%|████████▋ | 72/83 [10:32<01:17,  7.04s/it]Running generate_until requests:  88%|████████▊ | 73/83 [10:38<01:10,  7.02s/it]Running generate_until requests:  89%|████████▉ | 74/83 [10:45<01:02,  6.91s/it]Running generate_until requests:  90%|█████████ | 75/83 [10:57<01:06,  8.27s/it]Running generate_until requests:  92%|█████████▏| 76/83 [11:05<00:58,  8.32s/it]Running generate_until requests:  93%|█████████▎| 77/83 [11:11<00:45,  7.62s/it]Running generate_until requests:  94%|█████████▍| 78/83 [11:18<00:36,  7.37s/it]Running generate_until requests:  95%|█████████▌| 79/83 [11:28<00:32,  8.20s/it]Running generate_until requests:  96%|█████████▋| 80/83 [11:35<00:23,  7.94s/it]Running generate_until requests:  98%|█████████▊| 81/83 [11:44<00:16,  8.26s/it]Running generate_until requests:  99%|█████████▉| 82/83 [11:51<00:07,  7.75s/it]Running generate_until requests: 100%|██████████| 83/83 [11:57<00:00,  7.27s/it]Running generate_until requests: 100%|██████████| 83/83 [11:57<00:00,  8.64s/it]
