M	__pycache__/cache.cpython-312.pyc
M	__pycache__/xevaluator.cpython-312.pyc
M	__pycache__/xhuggingface.cpython-312.pyc
Your branch is up to date with 'origin/yangexp2'.
Already up to date.
/private/home/beidic/.conda/envs/griffin/bin/python
[1m[31mERROR! `huggingface-cli login` uses an outdated login mechanism that is not compatible with the Hugging Face Hub backend anymore. Please use `huggingface-cli login instead.[0m
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-hf,griffin=True,check=True,kernel_size=16,spr=0.5,thr=0.1', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-hf,griffin=True,check=True,kernel_size=16,spr=0.5,thr=0.1', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-hf,griffin=True,check=True,kernel_size=16,spr=0.5,thr=0.1', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-hf,griffin=True,check=True,kernel_size=16,spr=0.5,thr=0.1', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-hf,griffin=True,check=True,kernel_size=16,spr=0.5,thr=0.1', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-hf,griffin=True,check=True,kernel_size=16,spr=0.5,thr=0.1', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-hf,griffin=True,check=True,kernel_size=16,spr=0.5,thr=0.1', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
Namespace(model='xhf', tasks='gsm8k', model_args='pretrained=meta-llama/Llama-2-7b-hf,griffin=True,check=True,kernel_size=16,spr=0.5,thr=0.1', num_fewshot=None, batch_size='1', max_batch_size=None, device=None, output_path=None, limit=None, topp=1.1, topk=64, spr=0.25, sink=8, local=256, use_cache=None, cache_requests=None, check_integrity=False, write_out=False, log_samples=False, show_config=False, include_path=None, gen_kwargs=None, verbosity='INFO', wandb_args='', predict_only=False, seed=[0, 1234, 1234], trust_remote_code=False)
21062 1393 15.119885139985643
19215 1265 15.189723320158103
19963 1317 15.15793470007593
18925 1250 15.14
19322 1282 15.07176287051482
20556 1363 15.081438004402054
21137 1399 15.108649035025017
20307 1344 15.109375
