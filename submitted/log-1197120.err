wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in /opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/wandb/run-20240314_072318-v3efls2i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 2024-03-14_setting0_plain
wandb: ‚≠êÔ∏è View project at https://wandb.ai/stevenzhou0816100/llm160m
wandb: üöÄ View run at https://wandb.ai/stevenzhou0816100/llm160m/runs/v3efls2i
  0%|          | 0/8420 [00:00<?, ?it/s]You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/weird_training_ddpstep.py", line 1386, in <module>
    trainer.train(resume_from_checkpoint = args.resume_from_checkpoint)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/weird_training_ddpstep.py", line 398, in train
    return inner_training_loop(
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/trainer.py", line 1874, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/weird_training_ddpstep.py", line 554, in training_step
    loss = self.compute_loss(model, inputs, evaluation_mode = False)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/weird_training_ddpstep.py", line 728, in compute_loss
    SimpleSmallModel.plot_attention_map(outputs.attentions, layer, head, input_ids.shape[1] + addedon_length, plot_name)
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/models/llama/modeling_llama.py", line 7006, in plot_attention_map
    attention_map = attention_maps[layer_num][0][head_num].to(torch.float32).cpu().detach().numpy()
IndexError: index 11 is out of bounds for dimension 0 with size 8
wandb: - 1.784 MB of 1.784 MB uploadedwandb: \ 1.784 MB of 1.784 MB uploadedwandb: | 1.784 MB of 1.784 MB uploadedwandb: / 1.784 MB of 1.784 MB uploadedwandb: - 1.810 MB of 1.832 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:       group1.lr ‚ñÅ
wandb: iteration_count ‚ñÅ
wandb:            loss ‚ñÅ
wandb: 
wandb: Run summary:
wandb:       group1.lr 0.0
wandb: iteration_count 0
wandb:            loss 3.28169
wandb: 
wandb: üöÄ View run 2024-03-14_setting0_plain at: https://wandb.ai/stevenzhou0816100/llm160m/runs/v3efls2i
wandb: Ô∏è‚ö° View job at https://wandb.ai/stevenzhou0816100/llm160m/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNTcyNDE1MQ==/version_details/v120
wandb: Synced 6 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240314_072318-v3efls2i/logs
[2024-03-14 07:24:27,575] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2095635 closing signal SIGTERM
[2024-03-14 07:24:27,575] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2095636 closing signal SIGTERM
[2024-03-14 07:24:27,575] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2095637 closing signal SIGTERM
[2024-03-14 07:24:27,575] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2095638 closing signal SIGTERM
[2024-03-14 07:24:27,575] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2095639 closing signal SIGTERM
[2024-03-14 07:24:27,576] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2095640 closing signal SIGTERM
[2024-03-14 07:24:27,576] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2095641 closing signal SIGTERM
[2024-03-14 07:24:28,455] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2095634) of binary: /data/home/beidic/anaconda3/envs/yangllm2/bin/python3.9
Traceback (most recent call last):
  File "/data/home/beidic/anaconda3/envs/yangllm2/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1014, in launch_command
    multi_gpu_launcher(args)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/accelerate/commands/launch.py", line 672, in multi_gpu_launcher
    distrib_run.run(args)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
weird_training_ddpstep.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-14_07:24:27
  host      : a100-st-p4de24xlarge-1026.fair-a100.hpcaas
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2095634)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
