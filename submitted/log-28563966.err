Already on 'yangexp2'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/huggingface-cli", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py", line 51, in main
    service.run()
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/user.py", line 98, in run
    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 111, in login
    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 307, in _login
    raise ValueError("Invalid token passed!")
ValueError: Invalid token passed!
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:02:20:36,349 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:20:36,350 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:20:36,350 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:20:36,350 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:20:36,350 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:20:36,783 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:20:36,856 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:20:36,873 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:20:41,775 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:20:41,775 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:20:41,776 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:20:41,776 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:20:41,781 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:20:41,781 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-04:02:20:41,781 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:20:41,781 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-04:02:20:41,836 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:20:41,837 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:20:41,841 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:20:41,842 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-04:02:20:42,045 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:20:42,046 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:20:42,049 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:20:42,049 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-04:02:20:42,875 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:20:42,876 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:20:42,882 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:20:42,882 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-04:02:20:42,962 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:20:42,963 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:20:42,967 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:20:42,967 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:20:43,401 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:20:43,402 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:20:43,405 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:20:43,405 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-04:02:20:43,507 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:20:43,508 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:20:43,515 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:20:43,515 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:30<01:31, 30.59s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:31<01:35, 31.69s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:30<01:32, 30.68s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:31<01:35, 31.78s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:31<01:35, 31.73s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:31<01:33, 31.10s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:31<01:35, 31.82s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:30<01:32, 30.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:01<01:01, 30.51s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:02<01:01, 31.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:02<01:02, 31.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:01<01:01, 30.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:01<01:01, 30.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:02<01:02, 31.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:02<01:02, 31.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:01<01:01, 30.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:30<00:29, 29.86s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:30<00:30, 30.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:31<00:30, 30.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:31<00:30, 30.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:31<00:30, 30.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:31<00:30, 30.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:30<00:29, 29.98s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:30<00:29, 29.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 20.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 24.11s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 20.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 24.01s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 20.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 24.11s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 20.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 24.29s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 20.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 24.29s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 20.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 24.27s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 20.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 24.27s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 20.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 24.02s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:22:56,536 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:22:56,538 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:22:56,651 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:22:56,652 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:22:56,721 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 207.90it/s]2024-06-04:02:22:56,845 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 51%|█████     | 42/83 [00:00<00:00, 208.41it/s] 26%|██▌       | 21/82 [00:00<00:00, 203.37it/s] 76%|███████▌  | 63/83 [00:00<00:00, 208.55it/s] 51%|█████     | 42/82 [00:00<00:00, 205.86it/s]100%|██████████| 83/83 [00:00<00:00, 208.65it/s]
 77%|███████▋  | 63/82 [00:00<00:00, 207.56it/s]100%|██████████| 82/82 [00:00<00:00, 208.97it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:23:11,683 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:11,686 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:23:11,696 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:11,697 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:11,839 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:02:23:11,879 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 27%|██▋       | 22/82 [00:00<00:00, 212.92it/s] 27%|██▋       | 22/83 [00:00<00:00, 214.54it/s] 54%|█████▎    | 44/82 [00:00<00:00, 214.30it/s] 53%|█████▎    | 44/83 [00:00<00:00, 215.85it/s] 80%|████████  | 66/82 [00:00<00:00, 214.42it/s] 80%|███████▉  | 66/83 [00:00<00:00, 216.11it/s]100%|██████████| 82/82 [00:00<00:00, 214.32it/s]
100%|██████████| 83/83 [00:00<00:00, 216.28it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:23:18,490 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:23:18,564 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:18,566 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:18,797 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 19%|█▉        | 16/83 [00:00<00:00, 153.93it/s] 39%|███▊      | 32/83 [00:00<00:00, 154.39it/s] 58%|█████▊    | 48/83 [00:00<00:00, 154.91it/s] 77%|███████▋  | 64/83 [00:00<00:00, 154.99it/s] 96%|█████████▋| 80/83 [00:00<00:00, 155.11it/s]100%|██████████| 83/83 [00:00<00:00, 155.00it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:23:21,947 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:21,949 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:23:22,079 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:22,081 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:23:22,128 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:22,132 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:23:22,189 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 130.74it/s]2024-06-04:02:23:22,351 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 34%|███▍      | 28/82 [00:00<00:00, 125.36it/s]2024-06-04:02:23:22,442 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 137.78it/s] 18%|█▊        | 15/82 [00:00<00:00, 140.18it/s] 35%|███▍      | 29/83 [00:00<00:00, 139.28it/s] 50%|█████     | 41/82 [00:00<00:00, 104.23it/s] 37%|███▋      | 30/82 [00:00<00:00, 141.13it/s] 53%|█████▎    | 44/83 [00:00<00:00, 140.52it/s] 63%|██████▎   | 52/82 [00:00<00:00, 96.69it/s]  55%|█████▍    | 45/82 [00:00<00:00, 141.50it/s] 71%|███████   | 59/83 [00:00<00:00, 141.10it/s] 76%|███████▌  | 62/82 [00:00<00:00, 91.22it/s] 73%|███████▎  | 60/82 [00:00<00:00, 141.56it/s] 89%|████████▉ | 74/83 [00:00<00:00, 141.27it/s] 88%|████████▊ | 72/82 [00:00<00:00, 92.86it/s]100%|██████████| 83/83 [00:00<00:00, 140.92it/s]
 91%|█████████▏| 75/82 [00:00<00:00, 143.61it/s]100%|██████████| 82/82 [00:00<00:00, 102.03it/s]
100%|██████████| 82/82 [00:00<00:00, 143.18it/s]
2024-06-04:02:23:26,845 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:23:26,845 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:23:26,846 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:23:26,846 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:23:26,846 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:23:26,846 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:23:26,846 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:23:26,846 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:11<15:47, 11.55s/it]Running generate_until requests:   2%|▏         | 2/83 [00:23<16:12, 12.01s/it]Running generate_until requests:   4%|▎         | 3/83 [00:31<13:07,  9.84s/it]Running generate_until requests:   5%|▍         | 4/83 [00:35<10:14,  7.78s/it]Running generate_until requests:   6%|▌         | 5/83 [00:43<10:00,  7.69s/it]Running generate_until requests:   7%|▋         | 6/83 [00:49<09:01,  7.03s/it]Running generate_until requests:   8%|▊         | 7/83 [00:57<09:39,  7.63s/it]Running generate_until requests:  10%|▉         | 8/83 [01:04<09:09,  7.33s/it]Running generate_until requests:  11%|█         | 9/83 [01:13<09:43,  7.89s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:18<08:18,  6.83s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:24<07:57,  6.63s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:33<08:36,  7.28s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:37<07:24,  6.34s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:44<07:31,  6.54s/it]Running generate_until requests:  18%|█▊        | 15/83 [01:54<08:46,  7.74s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:08<10:40,  9.56s/it]Running generate_until requests:  20%|██        | 17/83 [02:15<09:30,  8.64s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:21<08:45,  8.08s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:29<08:37,  8.08s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:39<09:02,  8.61s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:45<08:01,  7.76s/it]Running generate_until requests:  27%|██▋       | 22/83 [02:51<07:17,  7.16s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:00<07:36,  7.62s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:06<07:09,  7.27s/it]Running generate_until requests:  30%|███       | 25/83 [03:13<07:01,  7.27s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:20<06:44,  7.09s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:26<06:23,  6.84s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:32<05:56,  6.49s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:39<06:03,  6.73s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:47<06:14,  7.07s/it]Running generate_until requests:  37%|███▋      | 31/83 [03:52<05:35,  6.46s/it]Running generate_until requests:  39%|███▊      | 32/83 [03:59<05:29,  6.46s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:02<04:37,  5.55s/it]Running generate_until requests:  41%|████      | 34/83 [04:11<05:16,  6.47s/it]Running generate_until requests:  42%|████▏     | 35/83 [04:17<05:13,  6.52s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:24<05:05,  6.51s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:32<05:26,  7.10s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:40<05:30,  7.34s/it]Running generate_until requests:  47%|████▋     | 39/83 [04:44<04:37,  6.30s/it]Running generate_until requests:  48%|████▊     | 40/83 [04:50<04:22,  6.11s/it]Running generate_until requests:  49%|████▉     | 41/83 [04:55<04:10,  5.96s/it]Running generate_until requests:  51%|█████     | 42/83 [05:06<05:05,  7.44s/it]Running generate_until requests:  52%|█████▏    | 43/83 [05:14<04:57,  7.45s/it]Running generate_until requests:  53%|█████▎    | 44/83 [05:18<04:19,  6.66s/it]Running generate_until requests:  54%|█████▍    | 45/83 [05:24<04:05,  6.46s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:30<03:53,  6.31s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:37<03:47,  6.31s/it]Running generate_until requests:  58%|█████▊    | 48/83 [05:43<03:45,  6.45s/it]Running generate_until requests:  59%|█████▉    | 49/83 [05:48<03:19,  5.85s/it]Running generate_until requests:  60%|██████    | 50/83 [05:54<03:17,  5.98s/it]Running generate_until requests:  61%|██████▏   | 51/83 [06:01<03:18,  6.20s/it]Running generate_until requests:  63%|██████▎   | 52/83 [06:19<05:01,  9.73s/it]Running generate_until requests:  64%|██████▍   | 53/83 [06:24<04:13,  8.45s/it]Running generate_until requests:  65%|██████▌   | 54/83 [06:29<03:34,  7.41s/it]Running generate_until requests:  66%|██████▋   | 55/83 [06:36<03:19,  7.14s/it]Running generate_until requests:  67%|██████▋   | 56/83 [06:49<04:03,  9.02s/it]Running generate_until requests:  69%|██████▊   | 57/83 [06:56<03:37,  8.36s/it]Running generate_until requests:  70%|██████▉   | 58/83 [07:03<03:15,  7.81s/it]Running generate_until requests:  71%|███████   | 59/83 [07:08<02:47,  6.97s/it]Running generate_until requests:  72%|███████▏  | 60/83 [07:13<02:28,  6.45s/it]Running generate_until requests:  73%|███████▎  | 61/83 [07:20<02:27,  6.68s/it]Running generate_until requests:  75%|███████▍  | 62/83 [07:26<02:14,  6.38s/it]Running generate_until requests:  76%|███████▌  | 63/83 [07:31<02:01,  6.07s/it]Running generate_until requests:  77%|███████▋  | 64/83 [07:48<02:59,  9.43s/it]Running generate_until requests:  78%|███████▊  | 65/83 [07:54<02:27,  8.20s/it]Running generate_until requests:  80%|███████▉  | 66/83 [08:02<02:17,  8.12s/it]Running generate_until requests:  81%|████████  | 67/83 [08:06<01:54,  7.15s/it]Running generate_until requests:  82%|████████▏ | 68/83 [08:12<01:40,  6.71s/it]Running generate_until requests:  83%|████████▎ | 69/83 [08:21<01:40,  7.20s/it]Running generate_until requests:  84%|████████▍ | 70/83 [08:28<01:35,  7.33s/it]Running generate_until requests:  86%|████████▌ | 71/83 [08:32<01:14,  6.23s/it]Running generate_until requests:  87%|████████▋ | 72/83 [08:38<01:07,  6.16s/it]Running generate_until requests:  88%|████████▊ | 73/83 [08:43<00:59,  5.97s/it]Running generate_until requests:  89%|████████▉ | 74/83 [08:48<00:50,  5.57s/it]Running generate_until requests:  90%|█████████ | 75/83 [08:56<00:50,  6.35s/it]Running generate_until requests:  92%|█████████▏| 76/83 [09:05<00:50,  7.18s/it]Running generate_until requests:  93%|█████████▎| 77/83 [09:11<00:39,  6.66s/it]Running generate_until requests:  94%|█████████▍| 78/83 [09:16<00:31,  6.22s/it]Running generate_until requests:  95%|█████████▌| 79/83 [09:26<00:29,  7.42s/it]Running generate_until requests:  96%|█████████▋| 80/83 [09:31<00:19,  6.51s/it]Running generate_until requests:  98%|█████████▊| 81/83 [09:38<00:13,  6.79s/it]Running generate_until requests:  99%|█████████▉| 82/83 [09:42<00:06,  6.06s/it]Running generate_until requests: 100%|██████████| 83/83 [09:46<00:00,  5.43s/it]Running generate_until requests: 100%|██████████| 83/83 [09:46<00:00,  7.07s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:02:37:00,957 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:00,957 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:00,960 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:01,001 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:01,174 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:01,205 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:01,217 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:01,243 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:37:05,705 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:05,706 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:05,711 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:05,711 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:02:37:05,943 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:05,944 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:05,948 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:05,948 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:02:37:06,524 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:06,525 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:06,529 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:06,529 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:37:07,799 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:07,799 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:07,803 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:07,803 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:02:37:07,823 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:07,825 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:07,831 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:07,831 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:02:37:07,874 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:07,877 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:07,886 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:07,886 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:02:37:08,039 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:08,040 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:08,044 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:37:08,045 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:37:08,046 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:08,047 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:02:37:08,049 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:37:08,049 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.93s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.98s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.27s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.51s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.55s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.48s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.53s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.56s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.50s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.55s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.48s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.88s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.74s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.37s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.41s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.03s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.07s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.08s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.98s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  2.00s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:37:49,491 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:37:49,493 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:37:49,734 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:37:49,736 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:37:49,738 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 20%|█▉        | 16/82 [00:00<00:00, 150.87it/s]2024-06-04:02:37:49,890 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 39%|███▉      | 32/82 [00:00<00:00, 151.32it/s] 26%|██▌       | 21/82 [00:00<00:00, 207.01it/s] 59%|█████▊    | 48/82 [00:00<00:00, 151.38it/s] 51%|█████     | 42/82 [00:00<00:00, 208.63it/s] 78%|███████▊  | 64/82 [00:00<00:00, 151.28it/s] 77%|███████▋  | 63/82 [00:00<00:00, 209.20it/s] 98%|█████████▊| 80/82 [00:00<00:00, 151.17it/s]100%|██████████| 82/82 [00:00<00:00, 209.17it/s]
100%|██████████| 82/82 [00:00<00:00, 150.41it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:38:04,838 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:04,840 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:05,029 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 27%|██▋       | 22/82 [00:00<00:00, 210.88it/s] 54%|█████▎    | 44/82 [00:00<00:00, 212.03it/s] 80%|████████  | 66/82 [00:00<00:00, 211.87it/s]100%|██████████| 82/82 [00:00<00:00, 212.16it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:38:06,471 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:38:06,543 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:06,545 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:06,709 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 27%|██▋       | 22/83 [00:00<00:00, 218.73it/s] 54%|█████▍    | 45/83 [00:00<00:00, 219.96it/s] 82%|████████▏ | 68/83 [00:00<00:00, 220.68it/s]100%|██████████| 83/83 [00:00<00:00, 220.53it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:38:18,384 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:18,386 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:38:18,482 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:18,484 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:38:18,504 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:18,506 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:38:18,529 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:18,531 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:38:18,594 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 135.53it/s]2024-06-04:02:38:18,771 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:02:38:18,811 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
2024-06-04:02:38:18,814 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
 34%|███▍      | 28/82 [00:00<00:00, 136.41it/s]  0%|          | 0/83 [00:00<?, ?it/s]  0%|          | 0/83 [00:00<?, ?it/s] 19%|█▉        | 16/83 [00:00<00:00, 150.51it/s] 52%|█████▏    | 43/82 [00:00<00:00, 138.85it/s] 18%|█▊        | 15/83 [00:00<00:00, 141.01it/s] 18%|█▊        | 15/83 [00:00<00:00, 142.51it/s] 39%|███▊      | 32/83 [00:00<00:00, 145.15it/s] 71%|███████   | 58/82 [00:00<00:00, 139.80it/s] 36%|███▌      | 30/83 [00:00<00:00, 141.55it/s] 36%|███▌      | 30/83 [00:00<00:00, 142.18it/s] 57%|█████▋    | 47/83 [00:00<00:00, 143.84it/s] 89%|████████▉ | 73/82 [00:00<00:00, 140.01it/s] 54%|█████▍    | 45/83 [00:00<00:00, 141.16it/s] 54%|█████▍    | 45/83 [00:00<00:00, 142.50it/s]100%|██████████| 82/82 [00:00<00:00, 139.32it/s]
 75%|███████▍  | 62/83 [00:00<00:00, 143.28it/s] 73%|███████▎  | 61/83 [00:00<00:00, 147.18it/s] 72%|███████▏  | 60/83 [00:00<00:00, 142.61it/s] 93%|█████████▎| 77/83 [00:00<00:00, 143.12it/s] 95%|█████████▌| 79/83 [00:00<00:00, 157.66it/s]100%|██████████| 83/83 [00:00<00:00, 143.88it/s]
 90%|█████████ | 75/83 [00:00<00:00, 142.99it/s]100%|██████████| 83/83 [00:00<00:00, 150.42it/s]
100%|██████████| 83/83 [00:00<00:00, 143.62it/s]
2024-06-04:02:38:22,946 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:38:22,946 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:38:22,946 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:38:22,946 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:38:22,946 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:38:22,946 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:38:22,946 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:38:22,946 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:09<12:27,  9.11s/it]Running generate_until requests:   2%|▏         | 2/83 [00:19<12:56,  9.59s/it]Running generate_until requests:   4%|▎         | 3/83 [00:25<10:36,  7.96s/it]Running generate_until requests:   5%|▍         | 4/83 [00:29<08:35,  6.52s/it]Running generate_until requests:   6%|▌         | 5/83 [00:35<08:24,  6.47s/it]Running generate_until requests:   7%|▋         | 6/83 [00:40<07:39,  5.97s/it]Running generate_until requests:   8%|▊         | 7/83 [00:47<08:03,  6.37s/it]Running generate_until requests:  10%|▉         | 8/83 [00:53<07:38,  6.12s/it]Running generate_until requests:  11%|█         | 9/83 [01:01<08:13,  6.67s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:06<07:28,  6.15s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:12<07:18,  6.09s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:19<07:36,  6.42s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:23<06:44,  5.79s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:29<06:37,  5.75s/it]Running generate_until requests:  18%|█▊        | 15/83 [01:41<08:46,  7.74s/it]Running generate_until requests:  19%|█▉        | 16/83 [01:52<09:45,  8.74s/it]Running generate_until requests:  20%|██        | 17/83 [01:58<08:37,  7.84s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:04<07:47,  7.19s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:10<07:27,  7.00s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:18<07:40,  7.31s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:24<07:02,  6.81s/it]Running generate_until requests:  27%|██▋       | 22/83 [02:29<06:19,  6.23s/it]Running generate_until requests:  28%|██▊       | 23/83 [02:37<06:44,  6.73s/it]Running generate_until requests:  29%|██▉       | 24/83 [02:42<06:14,  6.35s/it]Running generate_until requests:  30%|███       | 25/83 [02:48<05:52,  6.08s/it]Running generate_until requests:  31%|███▏      | 26/83 [02:53<05:38,  5.93s/it]Running generate_until requests:  33%|███▎      | 27/83 [02:59<05:23,  5.78s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:03<04:58,  5.43s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:09<05:03,  5.62s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:16<05:07,  5.79s/it]Running generate_until requests:  37%|███▋      | 31/83 [03:20<04:36,  5.32s/it]Running generate_until requests:  39%|███▊      | 32/83 [03:25<04:31,  5.32s/it]Running generate_until requests:  40%|███▉      | 33/83 [03:28<03:52,  4.65s/it]Running generate_until requests:  41%|████      | 34/83 [03:39<05:09,  6.32s/it]Running generate_until requests:  42%|████▏     | 35/83 [03:44<04:51,  6.08s/it]Running generate_until requests:  43%|████▎     | 36/83 [03:50<04:48,  6.13s/it]Running generate_until requests:  45%|████▍     | 37/83 [03:57<04:55,  6.41s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:05<05:04,  6.77s/it]Running generate_until requests:  47%|████▋     | 39/83 [04:08<04:12,  5.73s/it]Running generate_until requests:  48%|████▊     | 40/83 [04:13<03:53,  5.42s/it]Running generate_until requests:  49%|████▉     | 41/83 [04:18<03:38,  5.20s/it]Running generate_until requests:  51%|█████     | 42/83 [04:25<04:02,  5.92s/it]Running generate_until requests:  52%|█████▏    | 43/83 [04:31<03:59,  6.00s/it]Running generate_until requests:  53%|█████▎    | 44/83 [04:36<03:31,  5.43s/it]Running generate_until requests:  54%|█████▍    | 45/83 [04:40<03:19,  5.25s/it]Running generate_until requests:  55%|█████▌    | 46/83 [04:45<03:10,  5.14s/it]Running generate_until requests:  57%|█████▋    | 47/83 [04:50<03:01,  5.04s/it]Running generate_until requests:  58%|█████▊    | 48/83 [04:56<03:02,  5.21s/it]Running generate_until requests:  59%|█████▉    | 49/83 [04:59<02:43,  4.80s/it]Running generate_until requests:  60%|██████    | 50/83 [05:06<02:59,  5.43s/it]Running generate_until requests:  61%|██████▏   | 51/83 [05:12<02:54,  5.44s/it]Running generate_until requests:  63%|██████▎   | 52/83 [05:16<02:36,  5.04s/it]Running generate_until requests:  64%|██████▍   | 53/83 [05:21<02:27,  4.92s/it]Running generate_until requests:  65%|██████▌   | 54/83 [05:25<02:14,  4.65s/it]Running generate_until requests:  66%|██████▋   | 55/83 [05:31<02:26,  5.25s/it]Running generate_until requests:  67%|██████▋   | 56/83 [05:42<03:07,  6.93s/it]Running generate_until requests:  69%|██████▊   | 57/83 [05:48<02:49,  6.53s/it]Running generate_until requests:  70%|██████▉   | 58/83 [05:53<02:35,  6.20s/it]Running generate_until requests:  71%|███████   | 59/83 [05:57<02:13,  5.56s/it]Running generate_until requests:  72%|███████▏  | 60/83 [06:02<02:00,  5.23s/it]Running generate_until requests:  73%|███████▎  | 61/83 [06:08<02:01,  5.53s/it]Running generate_until requests:  75%|███████▍  | 62/83 [06:13<01:51,  5.30s/it]Running generate_until requests:  76%|███████▌  | 63/83 [06:17<01:41,  5.06s/it]Running generate_until requests:  77%|███████▋  | 64/83 [06:32<02:34,  8.12s/it]Running generate_until requests:  78%|███████▊  | 65/83 [06:37<02:06,  7.02s/it]Running generate_until requests:  80%|███████▉  | 66/83 [06:43<01:56,  6.88s/it]Running generate_until requests:  81%|████████  | 67/83 [06:47<01:36,  6.03s/it]Running generate_until requests:  82%|████████▏ | 68/83 [06:52<01:24,  5.64s/it]Running generate_until requests:  83%|████████▎ | 69/83 [06:59<01:24,  6.01s/it]Running generate_until requests:  84%|████████▍ | 70/83 [07:06<01:21,  6.30s/it]Running generate_until requests:  86%|████████▌ | 71/83 [07:09<01:04,  5.36s/it]Running generate_until requests:  87%|████████▋ | 72/83 [07:14<00:56,  5.16s/it]Running generate_until requests:  88%|████████▊ | 73/83 [07:19<00:50,  5.02s/it]Running generate_until requests:  89%|████████▉ | 74/83 [07:23<00:42,  4.69s/it]Running generate_until requests:  90%|█████████ | 75/83 [07:31<00:47,  5.93s/it]Running generate_until requests:  92%|█████████▏| 76/83 [07:37<00:41,  5.94s/it]Running generate_until requests:  93%|█████████▎| 77/83 [07:42<00:33,  5.52s/it]Running generate_until requests:  94%|█████████▍| 78/83 [07:46<00:25,  5.13s/it]Running generate_until requests:  95%|█████████▌| 79/83 [07:54<00:24,  6.08s/it]Running generate_until requests:  96%|█████████▋| 80/83 [07:58<00:16,  5.36s/it]Running generate_until requests:  98%|█████████▊| 81/83 [08:04<00:11,  5.63s/it]Running generate_until requests:  99%|█████████▉| 82/83 [08:09<00:05,  5.25s/it]Running generate_until requests: 100%|██████████| 83/83 [08:13<00:00,  5.09s/it]Running generate_until requests: 100%|██████████| 83/83 [08:13<00:00,  5.95s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:02:51:24,958 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:25,133 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:25,227 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:25,279 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:25,337 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:25,606 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:25,763 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:25,812 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:29,721 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:29,722 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:29,725 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:29,725 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-04:02:51:29,731 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:29,733 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:29,738 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:29,738 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-04:02:51:29,930 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:29,931 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:29,935 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:29,935 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:51:31,587 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:31,590 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:31,598 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:31,598 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-04:02:51:31,877 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:31,878 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:31,882 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:31,882 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-04:02:51:32,239 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:32,241 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:32,245 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:32,245 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-04:02:51:32,437 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:32,438 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:32,441 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:32,441 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-04:02:51:32,511 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:32,513 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:32,517 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:32,517 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.11s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.70s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.72s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.44s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.38s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.61s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.47s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.27s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.85s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.92s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.96s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.16s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.10s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.05s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.97s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.04s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:21,160 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:21,162 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:21,341 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 27%|██▋       | 22/82 [00:00<00:00, 215.44it/s] 54%|█████▎    | 44/82 [00:00<00:00, 215.37it/s] 80%|████████  | 66/82 [00:00<00:00, 212.91it/s]100%|██████████| 82/82 [00:00<00:00, 213.48it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:52:33,822 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:33,893 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:33,895 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:34,066 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 206.25it/s] 51%|█████     | 42/83 [00:00<00:00, 208.24it/s] 77%|███████▋  | 64/83 [00:00<00:00, 209.33it/s]100%|██████████| 83/83 [00:00<00:00, 209.24it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:36,594 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:36,596 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:36,834 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 19%|█▉        | 16/83 [00:00<00:00, 153.63it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 39%|███▊      | 32/83 [00:00<00:00, 154.27it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:37,122 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:37,124 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 58%|█████▊    | 48/83 [00:00<00:00, 153.23it/s] 77%|███████▋  | 64/83 [00:00<00:00, 153.85it/s]2024-06-04:02:52:37,296 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 96%|█████████▋| 80/83 [00:00<00:00, 154.20it/s]100%|██████████| 83/83 [00:00<00:00, 154.00it/s]
 24%|██▍       | 20/82 [00:00<00:00, 190.93it/s] 49%|████▉     | 40/82 [00:00<00:00, 191.68it/s] 73%|███████▎  | 60/82 [00:00<00:00, 189.58it/s] 96%|█████████▋| 79/82 [00:00<00:00, 188.95it/s]100%|██████████| 82/82 [00:00<00:00, 189.05it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:39,016 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:39,017 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:39,200 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 23%|██▎       | 19/82 [00:00<00:00, 184.87it/s] 46%|████▋     | 38/82 [00:00<00:00, 183.00it/s] 70%|██████▉   | 57/82 [00:00<00:00, 183.91it/s] 93%|█████████▎| 76/82 [00:00<00:00, 183.66it/s]100%|██████████| 82/82 [00:00<00:00, 183.79it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:42,876 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:42,878 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:42,926 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:42,928 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:43,118 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:02:52:43,203 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 20%|█▉        | 16/82 [00:00<00:00, 153.35it/s] 17%|█▋        | 14/83 [00:00<00:00, 132.60it/s] 39%|███▉      | 32/82 [00:00<00:00, 153.61it/s] 35%|███▍      | 29/83 [00:00<00:00, 140.80it/s] 59%|█████▊    | 48/82 [00:00<00:00, 153.95it/s] 53%|█████▎    | 44/83 [00:00<00:00, 135.13it/s] 78%|███████▊  | 64/82 [00:00<00:00, 154.08it/s] 98%|█████████▊| 80/82 [00:00<00:00, 154.07it/s] 70%|██████▉   | 58/83 [00:00<00:00, 132.46it/s]100%|██████████| 82/82 [00:00<00:00, 153.92it/s]
 88%|████████▊ | 73/83 [00:00<00:00, 136.74it/s]100%|██████████| 83/83 [00:00<00:00, 124.59it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:45,198 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:45,201 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:45,508 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 14%|█▍        | 12/83 [00:00<00:00, 118.08it/s] 30%|███       | 25/83 [00:00<00:00, 120.10it/s] 46%|████▌     | 38/83 [00:00<00:00, 121.95it/s] 61%|██████▏   | 51/83 [00:00<00:00, 120.27it/s] 77%|███████▋  | 64/83 [00:00<00:00, 123.51it/s] 93%|█████████▎| 77/83 [00:00<00:00, 124.36it/s]100%|██████████| 83/83 [00:00<00:00, 122.96it/s]
2024-06-04:02:52:50,717 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:52:50,718 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:52:50,717 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:52:50,717 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:52:50,718 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:52:50,718 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:52:50,718 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:52:50,718 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:09<12:35,  9.22s/it]Running generate_until requests:   2%|▏         | 2/83 [00:19<12:57,  9.59s/it]Running generate_until requests:   4%|▎         | 3/83 [00:25<10:34,  7.94s/it]Running generate_until requests:   5%|▍         | 4/83 [00:29<08:33,  6.50s/it]Running generate_until requests:   6%|▌         | 5/83 [00:35<08:22,  6.44s/it]Running generate_until requests:   7%|▋         | 6/83 [00:40<07:36,  5.93s/it]Running generate_until requests:   8%|▊         | 7/83 [00:47<07:56,  6.28s/it]Running generate_until requests:  10%|▉         | 8/83 [00:53<07:33,  6.04s/it]Running generate_until requests:  11%|█         | 9/83 [01:00<08:08,  6.60s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:06<07:28,  6.15s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:13<07:48,  6.51s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:20<07:55,  6.70s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:24<06:57,  5.97s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:30<06:45,  5.88s/it]Running generate_until requests:  18%|█▊        | 15/83 [01:43<09:08,  8.06s/it]Running generate_until requests:  19%|█▉        | 16/83 [01:54<09:58,  8.93s/it]Running generate_until requests:  20%|██        | 17/83 [02:00<08:45,  7.97s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:05<07:52,  7.26s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:12<07:30,  7.03s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:21<07:54,  7.53s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:26<07:10,  6.95s/it]Running generate_until requests:  27%|██▋       | 22/83 [02:31<06:25,  6.32s/it]Running generate_until requests:  28%|██▊       | 23/83 [02:38<06:32,  6.55s/it]Running generate_until requests:  29%|██▉       | 24/83 [02:44<06:06,  6.20s/it]Running generate_until requests:  30%|███       | 25/83 [02:49<05:45,  5.96s/it]Running generate_until requests:  31%|███▏      | 26/83 [02:55<05:33,  5.85s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:00<05:19,  5.70s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:04<04:55,  5.37s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:11<05:00,  5.56s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:17<05:04,  5.74s/it]Running generate_until requests:  37%|███▋      | 31/83 [03:23<05:05,  5.87s/it]Running generate_until requests:  39%|███▊      | 32/83 [03:30<05:15,  6.19s/it]Running generate_until requests:  40%|███▉      | 33/83 [03:33<04:22,  5.25s/it]Running generate_until requests:  41%|████      | 34/83 [03:43<05:28,  6.71s/it]Running generate_until requests:  42%|████▏     | 35/83 [03:48<05:04,  6.34s/it]Running generate_until requests:  43%|████▎     | 36/83 [03:55<04:55,  6.30s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:02<04:59,  6.51s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:10<05:20,  7.13s/it]Running generate_until requests:  47%|████▋     | 39/83 [04:13<04:22,  5.97s/it]Running generate_until requests:  48%|████▊     | 40/83 [04:23<05:07,  7.16s/it]Running generate_until requests:  49%|████▉     | 41/83 [04:31<05:04,  7.26s/it]Running generate_until requests:  51%|█████     | 42/83 [04:38<04:50,  7.09s/it]Running generate_until requests:  52%|█████▏    | 43/83 [04:44<04:32,  6.80s/it]Running generate_until requests:  53%|█████▎    | 44/83 [04:48<03:53,  5.99s/it]Running generate_until requests:  54%|█████▍    | 45/83 [04:53<03:33,  5.63s/it]Running generate_until requests:  55%|█████▌    | 46/83 [04:57<03:19,  5.39s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:02<03:07,  5.20s/it]Running generate_until requests:  58%|█████▊    | 48/83 [05:08<03:05,  5.31s/it]Running generate_until requests:  59%|█████▉    | 49/83 [05:12<02:45,  4.86s/it]Running generate_until requests:  60%|██████    | 50/83 [05:18<03:00,  5.45s/it]Running generate_until requests:  61%|██████▏   | 51/83 [05:24<02:54,  5.45s/it]Running generate_until requests:  63%|██████▎   | 52/83 [05:28<02:36,  5.04s/it]Running generate_until requests:  64%|██████▍   | 53/83 [05:33<02:27,  4.90s/it]Running generate_until requests:  65%|██████▌   | 54/83 [05:37<02:14,  4.63s/it]Running generate_until requests:  66%|██████▋   | 55/83 [05:43<02:26,  5.22s/it]Running generate_until requests:  67%|██████▋   | 56/83 [05:54<03:10,  7.05s/it]Running generate_until requests:  69%|██████▊   | 57/83 [06:01<03:02,  7.03s/it]Running generate_until requests:  70%|██████▉   | 58/83 [06:07<02:43,  6.54s/it]Running generate_until requests:  71%|███████   | 59/83 [06:11<02:19,  5.79s/it]Running generate_until requests:  72%|███████▏  | 60/83 [06:15<02:03,  5.39s/it]Running generate_until requests:  73%|███████▎  | 61/83 [06:21<02:03,  5.63s/it]Running generate_until requests:  75%|███████▍  | 62/83 [06:26<01:52,  5.35s/it]Running generate_until requests:  76%|███████▌  | 63/83 [06:31<01:41,  5.09s/it]Running generate_until requests:  77%|███████▋  | 64/83 [06:46<02:33,  8.06s/it]Running generate_until requests:  78%|███████▊  | 65/83 [06:50<02:05,  6.96s/it]Running generate_until requests:  80%|███████▉  | 66/83 [06:56<01:54,  6.72s/it]Running generate_until requests:  81%|████████  | 67/83 [07:00<01:34,  5.91s/it]Running generate_until requests:  82%|████████▏ | 68/83 [07:05<01:23,  5.55s/it]Running generate_until requests:  83%|████████▎ | 69/83 [07:12<01:23,  5.94s/it]Running generate_until requests:  84%|████████▍ | 70/83 [07:19<01:21,  6.24s/it]Running generate_until requests:  86%|████████▌ | 71/83 [07:22<01:03,  5.31s/it]Running generate_until requests:  87%|████████▋ | 72/83 [07:27<00:56,  5.12s/it]Running generate_until requests:  88%|████████▊ | 73/83 [07:32<00:52,  5.20s/it]Running generate_until requests:  89%|████████▉ | 74/83 [07:36<00:43,  4.82s/it]Running generate_until requests:  90%|█████████ | 75/83 [07:45<00:48,  6.02s/it]Running generate_until requests:  92%|█████████▏| 76/83 [07:51<00:41,  5.98s/it]Running generate_until requests:  93%|█████████▎| 77/83 [07:55<00:33,  5.54s/it]Running generate_until requests:  94%|█████████▍| 78/83 [07:59<00:25,  5.14s/it]Running generate_until requests:  95%|█████████▌| 79/83 [08:08<00:24,  6.14s/it]Running generate_until requests:  96%|█████████▋| 80/83 [08:11<00:16,  5.40s/it]Running generate_until requests:  98%|█████████▊| 81/83 [08:17<00:11,  5.59s/it]Running generate_until requests:  99%|█████████▉| 82/83 [08:21<00:05,  5.00s/it]Running generate_until requests: 100%|██████████| 83/83 [08:25<00:00,  4.64s/it]Running generate_until requests: 100%|██████████| 83/83 [08:25<00:00,  6.09s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:03:05:44,956 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:45,095 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:45,282 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:45,301 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:45,306 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:45,385 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:45,424 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:45,430 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:50,886 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:50,887 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:50,891 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:50,891 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-04:03:05:50,961 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:50,962 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:50,967 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:50,967 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-04:03:05:51,028 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:51,029 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:51,033 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:51,033 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-04:03:05:51,128 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:51,129 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:51,133 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:51,133 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-04:03:05:51,263 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:51,264 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:51,267 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:51,267 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-04:03:05:51,308 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:51,309 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:51,314 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:51,314 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-04:03:05:51,385 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:51,386 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:51,389 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:51,389 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-04:03:05:51,392 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:51,393 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:51,396 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:51,396 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.72s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.56s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.62s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.69s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.65s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.56s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.11s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.05s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.06s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.06s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.12s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:34,767 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:34,769 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:06:35,006 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:35,065 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:35,067 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 20%|█▉        | 16/82 [00:00<00:00, 153.07it/s]2024-06-04:03:06:35,223 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
 39%|███▉      | 32/82 [00:00<00:00, 155.24it/s]  0%|          | 0/83 [00:00<?, ?it/s] 59%|█████▊    | 48/82 [00:00<00:00, 156.26it/s] 25%|██▌       | 21/83 [00:00<00:00, 208.61it/s] 78%|███████▊  | 64/82 [00:00<00:00, 157.11it/s] 51%|█████     | 42/83 [00:00<00:00, 209.38it/s] 98%|█████████▊| 80/82 [00:00<00:00, 156.89it/s] 76%|███████▌  | 63/83 [00:00<00:00, 209.39it/s]100%|██████████| 82/82 [00:00<00:00, 156.39it/s]
100%|██████████| 83/83 [00:00<00:00, 209.52it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:51,154 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:51,156 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:51,315 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:51,418 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:51,419 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 27%|██▋       | 22/82 [00:00<00:00, 210.57it/s] 54%|█████▎    | 44/82 [00:00<00:00, 211.70it/s]2024-06-04:03:06:51,571 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/82 [00:00<?, ?it/s] 80%|████████  | 66/82 [00:00<00:00, 212.76it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:51,647 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:51,649 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 26%|██▌       | 21/82 [00:00<00:00, 209.31it/s]100%|██████████| 82/82 [00:00<00:00, 211.58it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 52%|█████▏    | 43/82 [00:00<00:00, 212.17it/s]2024-06-04:03:06:51,811 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:51,825 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:51,827 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 79%|███████▉  | 65/82 [00:00<00:00, 212.33it/s] 25%|██▌       | 21/83 [00:00<00:00, 208.55it/s]100%|██████████| 82/82 [00:00<00:00, 213.22it/s]
 51%|█████     | 42/83 [00:00<00:00, 209.24it/s] 76%|███████▌  | 63/83 [00:00<00:00, 209.51it/s]2024-06-04:03:06:52,137 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s]100%|██████████| 83/83 [00:00<00:00, 209.43it/s]
 18%|█▊        | 15/83 [00:00<00:00, 145.50it/s] 36%|███▌      | 30/83 [00:00<00:00, 148.00it/s] 54%|█████▍    | 45/83 [00:00<00:00, 148.59it/s] 73%|███████▎  | 61/83 [00:00<00:00, 149.19it/s] 93%|█████████▎| 77/83 [00:00<00:00, 149.61it/s]100%|██████████| 83/83 [00:00<00:00, 148.91it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:06:56,047 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:56,117 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:56,118 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:56,307 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 27%|██▋       | 22/83 [00:00<00:00, 215.58it/s] 53%|█████▎    | 44/83 [00:00<00:00, 215.96it/s] 80%|███████▉  | 66/83 [00:00<00:00, 216.67it/s]100%|██████████| 83/83 [00:00<00:00, 216.54it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:59,076 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:59,078 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:59,323 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 18%|█▊        | 15/82 [00:00<00:00, 148.66it/s] 37%|███▋      | 30/82 [00:00<00:00, 149.08it/s] 55%|█████▍    | 45/82 [00:00<00:00, 149.05it/s] 73%|███████▎  | 60/82 [00:00<00:00, 149.28it/s] 91%|█████████▏| 75/82 [00:00<00:00, 149.39it/s]100%|██████████| 82/82 [00:00<00:00, 149.25it/s]
2024-06-04:03:07:03,098 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:03,097 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:03,097 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:03,097 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:03,098 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:03,097 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:03,098 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:03,098 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:15<21:24, 15.66s/it]Running generate_until requests:   2%|▏         | 2/83 [00:30<20:15, 15.01s/it]Running generate_until requests:   4%|▎         | 3/83 [00:38<15:56, 11.95s/it]Running generate_until requests:   5%|▍         | 4/83 [00:44<12:29,  9.49s/it]Running generate_until requests:   6%|▌         | 5/83 [00:54<12:38,  9.72s/it]Running generate_until requests:   7%|▋         | 6/83 [01:02<11:52,  9.25s/it]Running generate_until requests:   8%|▊         | 7/83 [01:12<12:02,  9.50s/it]Running generate_until requests:  10%|▉         | 8/83 [01:20<11:09,  8.92s/it]Running generate_until requests:  11%|█         | 9/83 [01:31<11:54,  9.65s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:38<10:47,  8.87s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:49<11:17,  9.41s/it]Running generate_until requests:  14%|█▍        | 12/83 [02:01<12:13, 10.32s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:07<10:28,  8.98s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:15<09:59,  8.68s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:40<15:29, 13.67s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:57<16:10, 14.48s/it]Running generate_until requests:  20%|██        | 17/83 [03:05<13:49, 12.57s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:13<12:05, 11.16s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:22<11:23, 10.67s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:35<11:50, 11.27s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:43<10:39, 10.32s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:50<09:27,  9.30s/it]Running generate_until requests:  28%|██▊       | 23/83 [04:00<09:37,  9.62s/it]Running generate_until requests:  29%|██▉       | 24/83 [04:09<09:12,  9.37s/it]Running generate_until requests:  30%|███       | 25/83 [04:17<08:40,  8.97s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:25<08:13,  8.65s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:33<07:48,  8.37s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:40<07:26,  8.11s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:55<09:03, 10.07s/it]Running generate_until requests:  36%|███▌      | 30/83 [05:08<09:43, 11.02s/it]Running generate_until requests:  37%|███▋      | 31/83 [05:24<10:46, 12.44s/it]Running generate_until requests:  39%|███▊      | 32/83 [05:36<10:35, 12.46s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:40<08:16,  9.92s/it]Running generate_until requests:  41%|████      | 34/83 [05:56<09:24, 11.52s/it]Running generate_until requests:  42%|████▏     | 35/83 [06:05<08:43, 10.91s/it]Running generate_until requests:  43%|████▎     | 36/83 [06:15<08:18, 10.60s/it]Running generate_until requests:  45%|████▍     | 37/83 [06:26<08:05, 10.54s/it]Running generate_until requests:  46%|████▌     | 38/83 [06:40<08:52, 11.83s/it]Running generate_until requests:  47%|████▋     | 39/83 [06:45<07:04,  9.64s/it]Running generate_until requests:  48%|████▊     | 40/83 [07:02<08:30, 11.87s/it]Running generate_until requests:  49%|████▉     | 41/83 [07:13<08:08, 11.63s/it]Running generate_until requests:  51%|█████     | 42/83 [07:27<08:29, 12.42s/it]Running generate_until requests:  52%|█████▏    | 43/83 [07:36<07:34, 11.37s/it]Running generate_until requests:  53%|█████▎    | 44/83 [07:42<06:18,  9.72s/it]Running generate_until requests:  54%|█████▍    | 45/83 [07:49<05:37,  8.88s/it]Running generate_until requests:  55%|█████▌    | 46/83 [07:57<05:14,  8.49s/it]Running generate_until requests:  57%|█████▋    | 47/83 [08:03<04:47,  7.99s/it]Running generate_until requests:  58%|█████▊    | 48/83 [08:12<04:41,  8.04s/it]Running generate_until requests:  59%|█████▉    | 49/83 [08:17<04:05,  7.21s/it]Running generate_until requests:  60%|██████    | 50/83 [08:29<04:44,  8.64s/it]Running generate_until requests:  61%|██████▏   | 51/83 [08:37<04:28,  8.39s/it]Running generate_until requests:  63%|██████▎   | 52/83 [08:42<03:55,  7.60s/it]Running generate_until requests:  64%|██████▍   | 53/83 [08:54<04:23,  8.80s/it]Running generate_until requests:  65%|██████▌   | 54/83 [09:00<03:47,  7.83s/it]Running generate_until requests:  66%|██████▋   | 55/83 [09:11<04:05,  8.78s/it]Running generate_until requests:  67%|██████▋   | 56/83 [09:28<05:04, 11.28s/it]Running generate_until requests:  69%|██████▊   | 57/83 [09:38<04:44, 10.95s/it]Running generate_until requests:  70%|██████▉   | 58/83 [09:46<04:10, 10.00s/it]Running generate_until requests:  71%|███████   | 59/83 [09:51<03:28,  8.70s/it]Running generate_until requests:  72%|███████▏  | 60/83 [09:58<03:03,  7.97s/it]Running generate_until requests:  73%|███████▎  | 61/83 [10:07<03:02,  8.28s/it]Running generate_until requests:  75%|███████▍  | 62/83 [10:13<02:43,  7.81s/it]Running generate_until requests:  76%|███████▌  | 63/83 [10:20<02:27,  7.36s/it]Running generate_until requests:  77%|███████▋  | 64/83 [10:44<03:55, 12.37s/it]Running generate_until requests:  78%|███████▊  | 65/83 [10:50<03:09, 10.51s/it]Running generate_until requests:  80%|███████▉  | 66/83 [10:55<02:34,  9.07s/it]Running generate_until requests:  81%|████████  | 67/83 [11:01<02:08,  8.04s/it]Running generate_until requests:  82%|████████▏ | 68/83 [11:09<01:59,  7.96s/it]Running generate_until requests:  83%|████████▎ | 69/83 [11:21<02:08,  9.15s/it]Running generate_until requests:  84%|████████▍ | 70/83 [11:32<02:06,  9.75s/it]Running generate_until requests:  86%|████████▌ | 71/83 [11:36<01:37,  8.11s/it]Running generate_until requests:  87%|████████▋ | 72/83 [11:43<01:24,  7.71s/it]Running generate_until requests:  88%|████████▊ | 73/83 [11:51<01:16,  7.65s/it]Running generate_until requests:  89%|████████▉ | 74/83 [11:56<01:03,  7.00s/it]Running generate_until requests:  90%|█████████ | 75/83 [12:09<01:11,  8.89s/it]Running generate_until requests:  92%|█████████▏| 76/83 [12:18<01:01,  8.85s/it]Running generate_until requests:  93%|█████████▎| 77/83 [12:25<00:48,  8.16s/it]Running generate_until requests:  94%|█████████▍| 78/83 [12:30<00:36,  7.32s/it]Running generate_until requests:  95%|█████████▌| 79/83 [12:40<00:32,  8.14s/it]Running generate_until requests:  96%|█████████▋| 80/83 [12:45<00:21,  7.25s/it]Running generate_until requests:  98%|█████████▊| 81/83 [12:54<00:15,  7.72s/it]Running generate_until requests:  99%|█████████▉| 82/83 [12:59<00:06,  6.96s/it]Running generate_until requests: 100%|██████████| 83/83 [13:05<00:00,  6.51s/it]Running generate_until requests: 100%|██████████| 83/83 [13:05<00:00,  9.46s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:03:22:29,744 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:22:29,745 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:22:29,746 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:22:29,816 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:22:29,828 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:22:29,965 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:22:30,064 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:22:30,237 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:22:34,295 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:22:34,296 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:22:34,300 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:22:34,300 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-04:03:22:34,865 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:22:34,866 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:22:34,870 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:22:34,870 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:22:35,521 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:22:35,523 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:22:35,530 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:22:35,530 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:22:36,285 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:22:36,286 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:22:36,291 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:22:36,291 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:22:36,625 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:22:36,627 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:22:36,633 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:22:36,633 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:22:36,902 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:22:36,903 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:22:36,907 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:22:36,908 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-04:03:22:36,943 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:22:36,944 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:22:36,948 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:22:36,948 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-04:03:22:36,949 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:22:36,950 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:22:36,955 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:22:36,955 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.90s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.15s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  2.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.89s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.68s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.56s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.39s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.94s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.50s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.47s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.55s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.44s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.54s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.99s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.40s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.50s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.99s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.41s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.05s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.05s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.95s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.00s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:23:20,269 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:23:20,271 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:23:20,472 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 205.94it/s] 51%|█████     | 42/82 [00:00<00:00, 205.56it/s] 77%|███████▋  | 63/82 [00:00<00:00, 205.69it/s]100%|██████████| 82/82 [00:00<00:00, 206.02it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:23:21,485 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:23:21,487 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:23:21,648 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 205.51it/s] 51%|█████     | 42/83 [00:00<00:00, 206.70it/s] 76%|███████▌  | 63/83 [00:00<00:00, 207.18it/s]100%|██████████| 83/83 [00:00<00:00, 207.18it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:23:34,614 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:23:34,616 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:23:34,771 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 27%|██▋       | 22/82 [00:00<00:00, 215.59it/s] 54%|█████▎    | 44/82 [00:00<00:00, 216.77it/s] 80%|████████  | 66/82 [00:00<00:00, 216.85it/s]100%|██████████| 82/82 [00:00<00:00, 216.77it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:23:35,680 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:23:35,682 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:23:35,877 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 27%|██▋       | 22/83 [00:00<00:00, 217.01it/s] 53%|█████▎    | 44/83 [00:00<00:00, 217.61it/s] 80%|███████▉  | 66/83 [00:00<00:00, 217.84it/s]100%|██████████| 83/83 [00:00<00:00, 217.97it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:23:46,915 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:23:46,967 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:23:46,969 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:23:46,986 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:23:46,988 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:23:47,201 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
2024-06-04:03:23:47,207 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/83 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:23:47,219 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:23:47,221 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
  0%|          | 0/82 [00:00<?, ?it/s] 19%|█▉        | 16/83 [00:00<00:00, 149.59it/s] 18%|█▊        | 15/82 [00:00<00:00, 149.43it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 38%|███▊      | 31/82 [00:00<00:00, 153.73it/s] 37%|███▋      | 31/83 [00:00<00:00, 142.72it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:23:47,477 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:23:47,480 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:23:47,516 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
 61%|██████    | 50/82 [00:00<00:00, 166.09it/s]  0%|          | 0/82 [00:00<?, ?it/s] 55%|█████▌    | 46/83 [00:00<00:00, 139.91it/s] 17%|█▋        | 14/82 [00:00<00:00, 139.41it/s] 73%|███████▎  | 61/83 [00:00<00:00, 140.07it/s] 82%|████████▏ | 67/82 [00:00<00:00, 153.89it/s] 34%|███▍      | 28/82 [00:00<00:00, 139.75it/s] 92%|█████████▏| 76/83 [00:00<00:00, 140.00it/s]2024-06-04:03:23:47,760 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
100%|██████████| 82/82 [00:00<00:00, 151.92it/s]
  0%|          | 0/83 [00:00<?, ?it/s]100%|██████████| 83/83 [00:00<00:00, 140.75it/s]
 54%|█████▎    | 44/82 [00:00<00:00, 144.98it/s] 22%|██▏       | 18/83 [00:00<00:00, 179.15it/s] 72%|███████▏  | 59/82 [00:00<00:00, 136.99it/s] 43%|████▎     | 36/83 [00:00<00:00, 151.88it/s] 89%|████████▉ | 73/82 [00:00<00:00, 129.49it/s] 63%|██████▎   | 52/83 [00:00<00:00, 147.40it/s]100%|██████████| 82/82 [00:00<00:00, 134.19it/s]
 81%|████████  | 67/83 [00:00<00:00, 132.06it/s] 98%|█████████▊| 81/83 [00:00<00:00, 112.06it/s]100%|██████████| 83/83 [00:00<00:00, 122.51it/s]
2024-06-04:03:23:51,961 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:23:51,961 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:23:51,961 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:23:51,961 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:23:51,962 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:23:51,962 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:03:23:51,964 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:23:51,965 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:13<17:52, 13.09s/it]Running generate_until requests:   2%|▏         | 2/83 [00:25<16:55, 12.54s/it]Running generate_until requests:   4%|▎         | 3/83 [00:32<13:27, 10.09s/it]Running generate_until requests:   5%|▍         | 4/83 [00:39<11:45,  8.93s/it]Running generate_until requests:   6%|▌         | 5/83 [00:48<11:28,  8.82s/it]Running generate_until requests:   7%|▋         | 6/83 [00:55<10:33,  8.22s/it]Running generate_until requests:   8%|▊         | 7/83 [01:11<13:46, 10.87s/it]Running generate_until requests:  10%|▉         | 8/83 [01:18<11:52,  9.50s/it]Running generate_until requests:  11%|█         | 9/83 [01:28<11:52,  9.62s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:34<10:29,  8.62s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:43<10:27,  8.71s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:53<10:58,  9.28s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:00<09:56,  8.52s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:07<09:11,  8.00s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:24<12:02, 10.63s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:39<13:34, 12.16s/it]Running generate_until requests:  20%|██        | 17/83 [02:46<11:38, 10.58s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:53<10:21,  9.56s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:05<10:43, 10.05s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:20<12:18, 11.72s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:27<10:37, 10.28s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:33<09:04,  8.92s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:42<08:50,  8.84s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:49<08:23,  8.54s/it]Running generate_until requests:  30%|███       | 25/83 [04:00<08:44,  9.04s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:07<08:12,  8.64s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:20<09:14,  9.90s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:27<08:07,  8.86s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:42<09:36, 10.68s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:53<09:36, 10.88s/it]Running generate_until requests:  37%|███▋      | 31/83 [05:04<09:22, 10.82s/it]Running generate_until requests:  39%|███▊      | 32/83 [05:16<09:34, 11.27s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:19<07:27,  8.96s/it]Running generate_until requests:  41%|████      | 34/83 [05:35<09:02, 11.07s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:44<08:12, 10.26s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:58<08:51, 11.31s/it]Running generate_until requests:  45%|████▍     | 37/83 [06:07<08:10, 10.67s/it]Running generate_until requests:  46%|████▌     | 38/83 [06:19<08:24, 11.22s/it]Running generate_until requests:  47%|████▋     | 39/83 [06:23<06:34,  8.97s/it]Running generate_until requests:  48%|████▊     | 40/83 [06:35<07:07,  9.93s/it]Running generate_until requests:  49%|████▉     | 41/83 [06:43<06:30,  9.31s/it]Running generate_until requests:  51%|█████     | 42/83 [06:57<07:19, 10.71s/it]Running generate_until requests:  52%|█████▏    | 43/83 [07:05<06:31,  9.79s/it]Running generate_until requests:  53%|█████▎    | 44/83 [07:10<05:26,  8.36s/it]Running generate_until requests:  54%|█████▍    | 45/83 [07:16<04:49,  7.62s/it]Running generate_until requests:  55%|█████▌    | 46/83 [07:22<04:29,  7.29s/it]Running generate_until requests:  57%|█████▋    | 47/83 [07:28<04:06,  6.85s/it]Running generate_until requests:  58%|█████▊    | 48/83 [07:35<04:03,  6.96s/it]Running generate_until requests:  59%|█████▉    | 49/83 [07:40<03:39,  6.46s/it]Running generate_until requests:  60%|██████    | 50/83 [07:51<04:15,  7.73s/it]Running generate_until requests:  61%|██████▏   | 51/83 [07:59<04:10,  7.84s/it]Running generate_until requests:  63%|██████▎   | 52/83 [08:07<04:06,  7.96s/it]Running generate_until requests:  64%|██████▍   | 53/83 [08:17<04:12,  8.42s/it]Running generate_until requests:  65%|██████▌   | 54/83 [08:25<03:58,  8.23s/it]Running generate_until requests:  66%|██████▋   | 55/83 [08:32<03:38,  7.81s/it]Running generate_until requests:  67%|██████▋   | 56/83 [08:48<04:36, 10.24s/it]Running generate_until requests:  69%|██████▊   | 57/83 [09:03<05:10, 11.94s/it]Running generate_until requests:  70%|██████▉   | 58/83 [09:10<04:20, 10.41s/it]Running generate_until requests:  71%|███████   | 59/83 [09:15<03:31,  8.82s/it]Running generate_until requests:  72%|███████▏  | 60/83 [09:22<03:08,  8.18s/it]Running generate_until requests:  73%|███████▎  | 61/83 [09:30<03:01,  8.23s/it]Running generate_until requests:  75%|███████▍  | 62/83 [09:38<02:47,  7.99s/it]Running generate_until requests:  76%|███████▌  | 63/83 [09:43<02:24,  7.22s/it]Running generate_until requests:  77%|███████▋  | 64/83 [09:54<02:35,  8.17s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:59<02:12,  7.36s/it]Running generate_until requests:  80%|███████▉  | 66/83 [10:04<01:52,  6.62s/it]Running generate_until requests:  81%|████████  | 67/83 [10:09<01:38,  6.15s/it]Running generate_until requests:  82%|████████▏ | 68/83 [10:15<01:32,  6.19s/it]Running generate_until requests:  83%|████████▎ | 69/83 [10:28<01:53,  8.08s/it]Running generate_until requests:  84%|████████▍ | 70/83 [10:39<01:57,  9.07s/it]Running generate_until requests:  86%|████████▌ | 71/83 [10:43<01:29,  7.49s/it]Running generate_until requests:  87%|████████▋ | 72/83 [10:57<01:43,  9.38s/it]Running generate_until requests:  88%|████████▊ | 73/83 [11:05<01:30,  9.02s/it]Running generate_until requests:  89%|████████▉ | 74/83 [11:11<01:11,  7.99s/it]Running generate_until requests:  90%|█████████ | 75/83 [11:27<01:23, 10.48s/it]Running generate_until requests:  92%|█████████▏| 76/83 [11:34<01:06,  9.55s/it]Running generate_until requests:  93%|█████████▎| 77/83 [11:43<00:55,  9.19s/it]Running generate_until requests:  94%|█████████▍| 78/83 [11:47<00:39,  7.83s/it]Running generate_until requests:  95%|█████████▌| 79/83 [11:56<00:32,  8.06s/it]Running generate_until requests:  96%|█████████▋| 80/83 [12:03<00:23,  7.87s/it]Running generate_until requests:  98%|█████████▊| 81/83 [12:11<00:15,  7.75s/it]Running generate_until requests:  99%|█████████▉| 82/83 [12:15<00:06,  6.72s/it]Running generate_until requests: 100%|██████████| 83/83 [12:20<00:00,  6.15s/it]Running generate_until requests: 100%|██████████| 83/83 [12:20<00:00,  8.92s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:03:38:54,256 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:38:54,256 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:38:54,256 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:38:54,323 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:38:54,506 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:38:54,648 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:38:54,714 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:38:54,788 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:38:59,304 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:38:59,305 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:38:59,309 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:38:59,309 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-04:03:38:59,385 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:38:59,386 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:38:59,390 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:38:59,390 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-04:03:38:59,669 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:38:59,671 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:38:59,675 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:38:59,675 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-04:03:39:00,355 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:39:00,355 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:39:00,359 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:39:00,359 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:39:01,034 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:39:01,035 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:39:01,039 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:39:01,039 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-04:03:39:01,198 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:39:01,199 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:39:01,204 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:39:01,204 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-04:03:39:01,273 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:39:01,275 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:39:01,279 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:39:01,279 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-04:03:39:01,512 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:39:01,513 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:39:01,519 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:39:01,519 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'griffin': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.89s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.30s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.46s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.87s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.57s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.52s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.82s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.54s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.44s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.56s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.55s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.45s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.98s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.03s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.41s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.50s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.47s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.05s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.99s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.06s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.98s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:39:42,760 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:39:42,762 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:39:42,932 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 209.38it/s] 52%|█████▏    | 43/83 [00:00<00:00, 210.46it/s] 78%|███████▊  | 65/83 [00:00<00:00, 210.83it/s]100%|██████████| 83/83 [00:00<00:00, 210.57it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:39:43,896 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:39:43,898 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:39:44,089 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 205.43it/s] 51%|█████     | 42/83 [00:00<00:00, 206.36it/s] 76%|███████▌  | 63/83 [00:00<00:00, 206.56it/s]100%|██████████| 83/83 [00:00<00:00, 206.54it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:39:58,742 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:39:58,744 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:39:58,967 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 20%|█▉        | 16/82 [00:00<00:00, 155.45it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 41%|████▏     | 34/82 [00:00<00:00, 169.41it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:39:59,210 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:39:59,212 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 62%|██████▏   | 51/82 [00:00<00:00, 167.76it/s] 83%|████████▎ | 68/82 [00:00<00:00, 156.15it/s]2024-06-04:03:39:59,485 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s]100%|██████████| 82/82 [00:00<00:00, 156.03it/s]
 27%|██▋       | 22/82 [00:00<00:00, 213.69it/s] 54%|█████▎    | 44/82 [00:00<00:00, 216.48it/s] 80%|████████  | 66/82 [00:00<00:00, 216.65it/s]100%|██████████| 82/82 [00:00<00:00, 216.78it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:40:07,755 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:40:07,829 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:40:07,831 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:40:08,076 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 19%|█▉        | 16/83 [00:00<00:00, 153.35it/s] 39%|███▊      | 32/83 [00:00<00:00, 154.00it/s] 58%|█████▊    | 48/83 [00:00<00:00, 154.16it/s] 77%|███████▋  | 64/83 [00:00<00:00, 154.17it/s] 96%|█████████▋| 80/83 [00:00<00:00, 154.23it/s]100%|██████████| 83/83 [00:00<00:00, 154.12it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:40:11,535 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:40:11,537 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:40:11,583 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:40:11,585 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:40:11,777 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:40:11,867 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 20%|█▉        | 16/82 [00:00<00:00, 153.50it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:40:11,929 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:40:11,931 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 21%|██        | 17/82 [00:00<00:00, 166.49it/s] 39%|███▉      | 32/82 [00:00<00:00, 151.93it/s] 41%|████▏     | 34/82 [00:00<00:00, 156.43it/s] 59%|█████▊    | 48/82 [00:00<00:00, 153.14it/s] 61%|██████    | 50/82 [00:00<00:00, 147.14it/s] 78%|███████▊  | 64/82 [00:00<00:00, 153.41it/s]2024-06-04:03:40:12,307 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
 98%|█████████▊| 80/82 [00:00<00:00, 154.03it/s]100%|██████████| 82/82 [00:00<00:00, 153.59it/s]
  0%|          | 0/83 [00:00<?, ?it/s] 79%|███████▉  | 65/82 [00:00<00:00, 133.64it/s]100%|██████████| 82/82 [00:00<00:00, 149.46it/s]
 19%|█▉        | 16/83 [00:00<00:00, 154.25it/s] 39%|███▊      | 32/83 [00:00<00:00, 154.64it/s] 58%|█████▊    | 48/83 [00:00<00:00, 154.38it/s] 77%|███████▋  | 64/83 [00:00<00:00, 154.44it/s] 96%|█████████▋| 80/83 [00:00<00:00, 154.23it/s]100%|██████████| 83/83 [00:00<00:00, 154.26it/s]
2024-06-04:03:40:17,211 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:40:17,212 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:40:17,212 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:40:17,212 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:40:17,212 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:40:17,212 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:40:17,212 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:40:17,212 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:12<17:11, 12.57s/it]Running generate_until requests:   2%|▏         | 2/83 [00:28<19:26, 14.40s/it]Running generate_until requests:   4%|▎         | 3/83 [00:35<14:42, 11.03s/it]Running generate_until requests:   5%|▍         | 4/83 [00:41<11:56,  9.07s/it]Running generate_until requests:   6%|▌         | 5/83 [00:48<10:46,  8.29s/it]Running generate_until requests:   7%|▋         | 6/83 [00:55<10:07,  7.89s/it]Running generate_until requests:   8%|▊         | 7/83 [01:11<13:35, 10.73s/it]Running generate_until requests:  10%|▉         | 8/83 [01:18<11:56,  9.55s/it]Running generate_until requests:  11%|█         | 9/83 [01:29<12:18,  9.98s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:36<10:43,  8.81s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:47<11:31,  9.61s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:57<11:38,  9.84s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:07<11:22,  9.75s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:14<10:11,  8.86s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:28<11:51, 10.46s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:44<13:26, 12.03s/it]Running generate_until requests:  20%|██        | 17/83 [02:54<12:42, 11.56s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:01<11:01, 10.18s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:13<11:21, 10.65s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:27<12:25, 11.83s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:35<11:01, 10.67s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:43<09:54,  9.74s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:52<09:26,  9.44s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:59<08:43,  8.87s/it]Running generate_until requests:  30%|███       | 25/83 [04:10<09:13,  9.54s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:18<08:28,  8.92s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:33<10:03, 10.78s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:41<09:02,  9.87s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:56<10:24, 11.57s/it]Running generate_until requests:  36%|███▌      | 30/83 [05:06<09:51, 11.16s/it]Running generate_until requests:  37%|███▋      | 31/83 [05:22<10:47, 12.45s/it]Running generate_until requests:  39%|███▊      | 32/83 [05:36<10:57, 12.89s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:40<08:38, 10.38s/it]Running generate_until requests:  41%|████      | 34/83 [06:00<10:47, 13.21s/it]Running generate_until requests:  42%|████▏     | 35/83 [06:09<09:30, 11.89s/it]Running generate_until requests:  43%|████▎     | 36/83 [06:23<09:45, 12.47s/it]Running generate_until requests:  45%|████▍     | 37/83 [06:33<08:58, 11.70s/it]Running generate_until requests:  46%|████▌     | 38/83 [06:46<09:04, 12.10s/it]Running generate_until requests:  47%|████▋     | 39/83 [06:53<07:49, 10.68s/it]Running generate_until requests:  48%|████▊     | 40/83 [07:05<07:59, 11.14s/it]Running generate_until requests:  49%|████▉     | 41/83 [07:12<06:53,  9.84s/it]Running generate_until requests:  51%|█████     | 42/83 [07:27<07:53, 11.54s/it]Running generate_until requests:  52%|█████▏    | 43/83 [07:35<06:57, 10.45s/it]Running generate_until requests:  53%|█████▎    | 44/83 [07:40<05:41,  8.75s/it]Running generate_until requests:  54%|█████▍    | 45/83 [07:47<05:05,  8.05s/it]Running generate_until requests:  55%|█████▌    | 46/83 [07:57<05:25,  8.79s/it]Running generate_until requests:  57%|█████▋    | 47/83 [08:03<04:42,  7.84s/it]Running generate_until requests:  58%|█████▊    | 48/83 [08:10<04:23,  7.54s/it]Running generate_until requests:  59%|█████▉    | 49/83 [08:15<03:55,  6.94s/it]Running generate_until requests:  60%|██████    | 50/83 [08:26<04:25,  8.03s/it]Running generate_until requests:  61%|██████▏   | 51/83 [08:39<05:11,  9.73s/it]Running generate_until requests:  63%|██████▎   | 52/83 [08:48<04:52,  9.44s/it]Running generate_until requests:  64%|██████▍   | 53/83 [08:59<04:59,  9.97s/it]Running generate_until requests:  65%|██████▌   | 54/83 [09:08<04:33,  9.43s/it]Running generate_until requests:  66%|██████▋   | 55/83 [09:18<04:30,  9.68s/it]Running generate_until requests:  67%|██████▋   | 56/83 [09:32<04:55, 10.93s/it]Running generate_until requests:  69%|██████▊   | 57/83 [09:46<05:14, 12.11s/it]Running generate_until requests:  70%|██████▉   | 58/83 [09:54<04:29, 10.78s/it]Running generate_until requests:  71%|███████   | 59/83 [09:59<03:35,  8.97s/it]Running generate_until requests:  72%|███████▏  | 60/83 [10:05<03:09,  8.23s/it]Running generate_until requests:  73%|███████▎  | 61/83 [10:15<03:12,  8.75s/it]Running generate_until requests:  75%|███████▍  | 62/83 [10:25<03:07,  8.94s/it]Running generate_until requests:  76%|███████▌  | 63/83 [10:30<02:37,  7.88s/it]Running generate_until requests:  77%|███████▋  | 64/83 [10:48<03:26, 10.88s/it]Running generate_until requests:  78%|███████▊  | 65/83 [10:53<02:45,  9.20s/it]Running generate_until requests:  80%|███████▉  | 66/83 [10:59<02:18,  8.17s/it]Running generate_until requests:  81%|████████  | 67/83 [11:05<01:59,  7.47s/it]Running generate_until requests:  82%|████████▏ | 68/83 [11:13<01:53,  7.54s/it]Running generate_until requests:  83%|████████▎ | 69/83 [11:28<02:16,  9.74s/it]Running generate_until requests:  84%|████████▍ | 70/83 [11:40<02:16, 10.53s/it]Running generate_until requests:  86%|████████▌ | 71/83 [11:44<01:42,  8.51s/it]Running generate_until requests:  87%|████████▋ | 72/83 [11:58<01:52, 10.19s/it]Running generate_until requests:  88%|████████▊ | 73/83 [12:07<01:39,  9.98s/it]Running generate_until requests:  89%|████████▉ | 74/83 [12:14<01:20,  8.98s/it]Running generate_until requests:  90%|█████████ | 75/83 [12:31<01:30, 11.33s/it]Running generate_until requests:  92%|█████████▏| 76/83 [12:49<01:34, 13.48s/it]Running generate_until requests:  93%|█████████▎| 77/83 [12:58<01:13, 12.20s/it]Running generate_until requests:  94%|█████████▍| 78/83 [13:05<00:52, 10.43s/it]Running generate_until requests:  95%|█████████▌| 79/83 [13:14<00:40, 10.13s/it]Running generate_until requests:  96%|█████████▋| 80/83 [13:21<00:27,  9.28s/it]Running generate_until requests:  98%|█████████▊| 81/83 [13:30<00:17,  8.97s/it]Running generate_until requests:  99%|█████████▉| 82/83 [13:35<00:07,  7.85s/it]Running generate_until requests: 100%|██████████| 83/83 [13:41<00:00,  7.26s/it]Running generate_until requests: 100%|██████████| 83/83 [13:41<00:00,  9.90s/it]
