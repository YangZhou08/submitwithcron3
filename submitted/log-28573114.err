Already on 'yangexp2'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/huggingface-cli", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py", line 51, in main
    service.run()
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/user.py", line 98, in run
    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 111, in login
    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 307, in _login
    raise ValueError("Invalid token passed!")
ValueError: Invalid token passed!
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:06:39:26,316 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:39:26,317 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:39:26,317 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:39:26,317 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:39:26,317 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:39:26,317 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:39:26,317 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:39:26,317 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:39:33,531 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:39:33,531 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:39:33,532 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:39:33,533 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:39:33,537 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:39:33,537 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:39:33,538 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:39:33,539 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:39:33,538 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:39:33,539 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:39:33,540 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:39:33,540 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:39:33,542 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:39:33,542 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:39:33,542 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:39:33,542 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:39:33,543 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:39:33,544 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:39:33,544 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:39:33,545 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:39:33,545 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:39:33,545 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:39:33,545 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:39:33,545 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:39:33,546 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:39:33,547 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:39:33,546 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:39:33,549 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:39:33,551 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:39:33,551 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:39:33,558 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:39:33,559 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:39<01:59, 39.78s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:40<02:01, 40.48s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:40<02:02, 40.71s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:40<02:01, 40.48s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:40<02:01, 40.51s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:41<02:03, 41.25s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:40<02:01, 40.51s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:40<02:02, 40.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:18<01:18, 39.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:19<01:19, 39.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:19<01:19, 39.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:20<01:19, 39.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:19<01:19, 39.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:19<01:19, 39.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:19<01:19, 39.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:19<01:19, 39.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:56<00:38, 38.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:58<00:39, 39.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:57<00:39, 39.05s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:58<00:39, 39.25s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:58<00:39, 39.44s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:58<00:39, 39.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:58<00:39, 39.21s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:58<00:39, 39.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:05<00:00, 26.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:05<00:00, 31.30s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:05<00:00, 26.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:05<00:00, 31.41s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:05<00:00, 26.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:05<00:00, 31.33s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:06<00:00, 26.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:06<00:00, 31.53s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:05<00:00, 26.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:05<00:00, 31.35s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:05<00:00, 26.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:05<00:00, 31.34s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:05<00:00, 26.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:05<00:00, 26.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:05<00:00, 31.37s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:05<00:00, 31.43s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:42:22,253 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:42:22,256 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:42:23,290 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:42:23,293 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
[2024-06-04 06:42:29,509] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1548118 closing signal SIGTERM
[2024-06-04 06:42:29,509] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1548119 closing signal SIGTERM
[2024-06-04 06:42:29,509] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1548120 closing signal SIGTERM
[2024-06-04 06:42:29,509] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1548123 closing signal SIGTERM
[2024-06-04 06:42:29,509] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1548124 closing signal SIGTERM
[2024-06-04 06:42:29,510] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1548125 closing signal SIGTERM
[2024-06-04 06:42:30,404] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -7) local_rank: 3 (pid: 1548121) of binary: /private/home/beidic/.conda/envs/griffin/bin/python
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 985, in launch_command
    multi_gpu_launcher(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/accelerate/commands/launch.py", line 654, in multi_gpu_launcher
    distrib_run.run(args)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
main.py FAILED
-------------------------------------------------------
Failures:
[1]:
  time      : 2024-06-04_06:42:29
  host      : learnfair5066.h2.fair
  rank      : 4 (local_rank: 4)
  exitcode  : -7 (pid: 1548122)
  error_file: <N/A>
  traceback : Signal 7 (SIGBUS) received by PID 1548122
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-04_06:42:29
  host      : learnfair5066.h2.fair
  rank      : 3 (local_rank: 3)
  exitcode  : -7 (pid: 1548121)
  error_file: <N/A>
  traceback : Signal 7 (SIGBUS) received by PID 1548121
=======================================================
/var/spool/slurm//job28573114/slurm_script: line 61: 1548095 Bus error               (core dumped) accelerate launch --main_process_port 29510 --num_processes 8 --num_machines 1 main.py --model xhf --model_args pretrained=meta-llama/Meta-Llama-3-8B-Instruct,cats=True,spr=$sparsitylevel,check=True,kernel_size=16,thr=0.1 --tasks gsm8k --batch_size 1 --limit 0.5
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:06:42:42,417 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:42:42,649 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:42:42,836 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:42:43,091 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:42:43,369 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:42:43,662 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:42:43,732 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:42:43,853 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:06:42:47,447 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:42:47,448 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:42:47,451 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:42:47,451 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:42:48,479 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:42:48,480 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:42:48,485 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:42:48,485 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:42:48,500 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:42:48,501 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:42:48,505 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:42:48,505 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:06:42:49,858 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:42:49,860 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:42:49,866 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:42:49,866 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:42:49,997 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:42:49,998 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:42:50,003 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:42:50,003 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:06:42:50,546 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:42:50,548 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:42:50,556 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:42:50,556 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.20s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:06:42:51,218 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:42:51,220 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:42:51,225 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:42:51,225 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:06:42:51,385 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:06:42:51,386 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:06:42:51,390 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:06:42:51,390 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.22s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.41s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.11s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.06s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.03s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.68s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.83s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:01,  1.98s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.65s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.19s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.20s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.17s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.07s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:43:37,540 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:37,542 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:37,916 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 132.28it/s] 34%|███▍      | 28/82 [00:00<00:00, 132.27it/s] 51%|█████     | 42/82 [00:00<00:00, 134.30it/s] 70%|██████▉   | 57/82 [00:00<00:00, 139.95it/s] 88%|████████▊ | 72/82 [00:00<00:00, 135.00it/s]100%|██████████| 82/82 [00:00<00:00, 134.31it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:43:38,821 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:38,823 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:39,032 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 191.57it/s] 49%|████▉     | 40/82 [00:00<00:00, 194.29it/s] 73%|███████▎  | 60/82 [00:00<00:00, 195.14it/s] 98%|█████████▊| 80/82 [00:00<00:00, 195.41it/s]100%|██████████| 82/82 [00:00<00:00, 194.90it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:43:57,681 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:57,682 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:57,849 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 207.89it/s] 51%|█████     | 42/82 [00:00<00:00, 208.83it/s] 78%|███████▊  | 64/82 [00:00<00:00, 209.45it/s]100%|██████████| 82/82 [00:00<00:00, 209.35it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:43:58,971 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:58,974 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:43:59,142 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 206.98it/s] 51%|█████     | 42/83 [00:00<00:00, 207.80it/s] 76%|███████▌  | 63/83 [00:00<00:00, 207.99it/s]100%|██████████| 83/83 [00:00<00:00, 207.72it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:44:10,836 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:44:10,839 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:44:11,050 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 208.70it/s] 51%|█████     | 42/83 [00:00<00:00, 208.07it/s] 76%|███████▌  | 63/83 [00:00<00:00, 208.46it/s]100%|██████████| 83/83 [00:00<00:00, 208.74it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:44:11,550 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:44:11,552 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:44:11,845 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 141.26it/s] 36%|███▌      | 30/83 [00:00<00:00, 141.60it/s] 54%|█████▍    | 45/83 [00:00<00:00, 139.07it/s] 72%|███████▏  | 60/83 [00:00<00:00, 139.85it/s] 90%|█████████ | 75/83 [00:00<00:00, 140.65it/s]100%|██████████| 83/83 [00:00<00:00, 140.54it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:06:44:13,803 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:44:13,876 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:44:13,877 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:44:14,038 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 205.79it/s] 51%|█████     | 42/83 [00:00<00:00, 207.65it/s] 76%|███████▌  | 63/83 [00:00<00:00, 208.01it/s]100%|██████████| 83/83 [00:00<00:00, 207.84it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:06:44:15,263 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:44:15,265 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:06:44:15,692 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 108.23it/s] 27%|██▋       | 22/82 [00:00<00:00, 108.37it/s] 40%|████      | 33/82 [00:00<00:00, 108.82it/s] 54%|█████▎    | 44/82 [00:00<00:00, 109.09it/s] 67%|██████▋   | 55/82 [00:00<00:00, 109.35it/s] 80%|████████  | 66/82 [00:00<00:00, 109.52it/s] 94%|█████████▍| 77/82 [00:00<00:00, 109.38it/s]100%|██████████| 82/82 [00:00<00:00, 109.21it/s]
2024-06-04:06:44:25,749 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:44:25,749 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:44:25,749 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:44:25,749 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:44:25,749 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:44:25,750 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:06:44:25,750 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:06:44:25,750 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:47<1:05:05, 47.62s/it]Running generate_until requests:   2%|▏         | 2/83 [01:25<56:40, 41.98s/it]  Running generate_until requests:   4%|▎         | 3/83 [01:47<43:59, 32.99s/it]Running generate_until requests:   5%|▍         | 4/83 [02:02<33:54, 25.75s/it]Running generate_until requests:   6%|▌         | 5/83 [02:16<27:58, 21.52s/it]Running generate_until requests:   7%|▋         | 6/83 [02:33<25:25, 19.81s/it]Running generate_until requests:   8%|▊         | 7/83 [03:18<35:42, 28.19s/it]Running generate_until requests:  10%|▉         | 8/83 [03:40<32:55, 26.33s/it]Running generate_until requests:  11%|█         | 9/83 [04:12<34:20, 27.85s/it]Running generate_until requests:  12%|█▏        | 10/83 [04:24<28:01, 23.03s/it]Running generate_until requests:  13%|█▎        | 11/83 [04:43<26:06, 21.75s/it]Running generate_until requests:  14%|█▍        | 12/83 [05:12<28:31, 24.10s/it]Running generate_until requests:  16%|█▌        | 13/83 [05:20<22:31, 19.31s/it]Running generate_until requests:  17%|█▋        | 14/83 [05:45<23:55, 20.80s/it]Running generate_until requests:  18%|█▊        | 15/83 [06:32<32:31, 28.70s/it]Running generate_until requests:  19%|█▉        | 16/83 [07:22<39:14, 35.15s/it]Running generate_until requests:  20%|██        | 17/83 [07:45<34:38, 31.49s/it]Running generate_until requests:  22%|██▏       | 18/83 [08:09<31:38, 29.20s/it]Running generate_until requests:  23%|██▎       | 19/83 [08:35<30:19, 28.43s/it]Running generate_until requests:  24%|██▍       | 20/83 [09:13<32:45, 31.20s/it]Running generate_until requests:  25%|██▌       | 21/83 [09:33<28:41, 27.76s/it]Running generate_until requests:  27%|██▋       | 22/83 [09:52<25:45, 25.33s/it]Running generate_until requests:  28%|██▊       | 23/83 [10:34<30:19, 30.32s/it]Running generate_until requests:  29%|██▉       | 24/83 [10:54<26:40, 27.12s/it]Running generate_until requests:  30%|███       | 25/83 [11:14<24:04, 24.91s/it]Running generate_until requests:  31%|███▏      | 26/83 [11:39<23:40, 24.92s/it]Running generate_until requests:  33%|███▎      | 27/83 [12:28<30:05, 32.23s/it]Running generate_until requests:  34%|███▎      | 28/83 [12:46<25:30, 27.83s/it]Running generate_until requests:  35%|███▍      | 29/83 [13:26<28:20, 31.49s/it]Running generate_until requests:  36%|███▌      | 30/83 [14:03<29:18, 33.17s/it]Running generate_until requests:  37%|███▋      | 31/83 [14:21<24:54, 28.75s/it]Running generate_until requests:  39%|███▊      | 32/83 [14:52<24:54, 29.31s/it]Running generate_until requests:  40%|███▉      | 33/83 [15:01<19:30, 23.41s/it]Running generate_until requests:  41%|████      | 34/83 [15:49<24:56, 30.54s/it]Running generate_until requests:  42%|████▏     | 35/83 [16:06<21:19, 26.66s/it]Running generate_until requests:  43%|████▎     | 36/83 [16:35<21:22, 27.29s/it]Running generate_until requests:  45%|████▍     | 37/83 [16:48<17:37, 22.98s/it]Running generate_until requests:  46%|████▌     | 38/83 [17:26<20:45, 27.67s/it]Running generate_until requests:  47%|████▋     | 39/83 [17:35<16:05, 21.94s/it]Running generate_until requests:  48%|████▊     | 40/83 [18:10<18:32, 25.87s/it]Running generate_until requests:  49%|████▉     | 41/83 [18:30<16:52, 24.12s/it]Running generate_until requests:  51%|█████     | 42/83 [19:03<18:20, 26.85s/it]Running generate_until requests:  52%|█████▏    | 43/83 [19:22<16:13, 24.35s/it]Running generate_until requests:  53%|█████▎    | 44/83 [19:39<14:27, 22.24s/it]Running generate_until requests:  54%|█████▍    | 45/83 [19:55<12:58, 20.48s/it]Running generate_until requests:  55%|█████▌    | 46/83 [20:20<13:26, 21.79s/it]Running generate_until requests:  57%|█████▋    | 47/83 [20:43<13:12, 22.03s/it]Running generate_until requests:  58%|█████▊    | 48/83 [20:57<11:30, 19.72s/it]Running generate_until requests:  59%|█████▉    | 49/83 [21:14<10:40, 18.83s/it]Running generate_until requests:  60%|██████    | 50/83 [21:37<11:05, 20.17s/it]Running generate_until requests:  61%|██████▏   | 51/83 [21:58<10:49, 20.31s/it]Running generate_until requests:  63%|██████▎   | 52/83 [22:12<09:27, 18.31s/it]Running generate_until requests:  64%|██████▍   | 53/83 [22:38<10:26, 20.89s/it]Running generate_until requests:  65%|██████▌   | 54/83 [22:58<09:55, 20.54s/it]Running generate_until requests:  66%|██████▋   | 55/83 [23:20<09:46, 20.96s/it]Running generate_until requests:  67%|██████▋   | 56/83 [24:02<12:16, 27.27s/it]Running generate_until requests:  69%|██████▊   | 57/83 [24:42<13:24, 30.92s/it]Running generate_until requests:  70%|██████▉   | 58/83 [25:02<11:35, 27.83s/it]Running generate_until requests:  71%|███████   | 59/83 [25:18<09:44, 24.35s/it]Running generate_until requests:  72%|███████▏  | 60/83 [25:37<08:41, 22.65s/it]Running generate_until requests:  73%|███████▎  | 61/83 [26:10<09:26, 25.76s/it]Running generate_until requests:  75%|███████▍  | 62/83 [26:24<07:45, 22.19s/it]Running generate_until requests:  76%|███████▌  | 63/83 [26:42<06:57, 20.90s/it]Running generate_until requests:  77%|███████▋  | 64/83 [27:26<08:49, 27.85s/it]Running generate_until requests:  78%|███████▊  | 65/83 [27:40<07:06, 23.71s/it]Running generate_until requests:  80%|███████▉  | 66/83 [27:54<05:51, 20.68s/it]Running generate_until requests:  81%|████████  | 67/83 [28:07<04:54, 18.39s/it]Running generate_until requests:  82%|████████▏ | 68/83 [28:26<04:40, 18.69s/it]Running generate_until requests:  83%|████████▎ | 69/83 [29:06<05:50, 25.04s/it]Running generate_until requests:  84%|████████▍ | 70/83 [29:35<05:40, 26.16s/it]Running generate_until requests:  86%|████████▌ | 71/83 [29:43<04:08, 20.74s/it]Running generate_until requests:  87%|████████▋ | 72/83 [29:57<03:27, 18.88s/it]Running generate_until requests:  88%|████████▊ | 73/83 [30:11<02:54, 17.42s/it]Running generate_until requests:  89%|████████▉ | 74/83 [30:32<02:44, 18.31s/it]Running generate_until requests:  90%|█████████ | 75/83 [31:02<02:54, 21.80s/it]Running generate_until requests:  92%|█████████▏| 76/83 [31:23<02:31, 21.60s/it]Running generate_until requests:  93%|█████████▎| 77/83 [31:43<02:06, 21.09s/it]Running generate_until requests:  94%|█████████▍| 78/83 [32:00<01:40, 20.02s/it]Running generate_until requests:  95%|█████████▌| 79/83 [32:30<01:31, 22.99s/it]Running generate_until requests:  96%|█████████▋| 80/83 [32:59<01:14, 24.68s/it]Running generate_until requests:  98%|█████████▊| 81/83 [33:16<00:45, 22.54s/it]Running generate_until requests:  99%|█████████▉| 82/83 [33:35<00:21, 21.50s/it]Running generate_until requests: 100%|██████████| 83/83 [33:55<00:00, 20.96s/it]Running generate_until requests: 100%|██████████| 83/83 [33:55<00:00, 24.52s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:07:28:32,732 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:28:32,732 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:28:33,429 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:28:33,470 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:28:33,489 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:28:33,516 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:28:33,522 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:28:33,632 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:28:38,256 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:28:38,258 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:28:38,263 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:28:38,263 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:28:38,413 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:28:38,415 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:28:38,420 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:28:38,420 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:07:28:39,754 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:28:39,755 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:28:39,759 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:28:39,759 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:28:40,030 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:28:40,032 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:28:40,037 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:28:40,037 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:28:40,278 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:28:40,279 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:28:40,283 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:28:40,283 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:28:40,374 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:28:40,375 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:28:40,379 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:28:40,379 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:28:40,408 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:28:40,409 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:28:40,413 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:28:40,413 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:28:40,458 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:28:40,459 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:28:40,464 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:28:40,464 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.19s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.18s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.09s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.10s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.72s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.72s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.73s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.75s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.77s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.68s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.68s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.72s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.17s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.17s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.19s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.11s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.11s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.14s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:29:27,166 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:29:27,168 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:29:27,458 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 139.78it/s] 35%|███▌      | 29/82 [00:00<00:00, 141.36it/s] 54%|█████▎    | 44/82 [00:00<00:00, 141.80it/s] 72%|███████▏  | 59/82 [00:00<00:00, 142.36it/s] 90%|█████████ | 74/82 [00:00<00:00, 142.78it/s]100%|██████████| 82/82 [00:00<00:00, 142.44it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:29:32,540 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:29:32,541 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:29:32,709 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/83 [00:00<00:00, 197.96it/s] 48%|████▊     | 40/83 [00:00<00:00, 198.64it/s] 72%|███████▏  | 60/83 [00:00<00:00, 197.58it/s] 96%|█████████▋| 80/83 [00:00<00:00, 194.41it/s]100%|██████████| 83/83 [00:00<00:00, 195.65it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:29:43,974 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:29:43,975 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:29:44,139 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 200.76it/s] 51%|█████     | 42/83 [00:00<00:00, 202.12it/s] 76%|███████▌  | 63/83 [00:00<00:00, 202.90it/s]100%|██████████| 83/83 [00:00<00:00, 202.78it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:29:48,126 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:29:48,129 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:29:48,310 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 203.86it/s] 51%|█████     | 42/82 [00:00<00:00, 205.33it/s] 77%|███████▋  | 63/82 [00:00<00:00, 206.01it/s]100%|██████████| 82/82 [00:00<00:00, 205.85it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:07:29:51,851 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:29:51,923 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:29:51,925 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:29:52,096 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/83 [00:00<00:00, 198.59it/s] 49%|████▉     | 41/83 [00:00<00:00, 199.52it/s] 75%|███████▍  | 62/83 [00:00<00:00, 200.34it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
100%|██████████| 83/83 [00:00<00:00, 202.03it/s]100%|██████████| 83/83 [00:00<00:00, 201.11it/s]
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:29:52,530 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:29:52,532 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:29:52,915 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 133.12it/s] 34%|███▍      | 28/82 [00:00<00:00, 133.59it/s] 51%|█████     | 42/82 [00:00<00:00, 133.82it/s] 68%|██████▊   | 56/82 [00:00<00:00, 133.38it/s] 85%|████████▌ | 70/82 [00:00<00:00, 133.38it/s]100%|██████████| 82/82 [00:00<00:00, 133.51it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:30:14,143 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:30:14,145 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:30:14,639 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 13%|█▎        | 11/82 [00:00<00:00, 101.39it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:30:14,870 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:30:14,873 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 27%|██▋       | 22/82 [00:00<00:00, 101.54it/s] 40%|████      | 33/82 [00:00<00:00, 101.87it/s] 54%|█████▎    | 44/82 [00:00<00:00, 102.22it/s] 67%|██████▋   | 55/82 [00:00<00:00, 102.01it/s]2024-06-04:07:30:15,226 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 80%|████████  | 66/82 [00:00<00:00, 102.21it/s] 17%|█▋        | 14/83 [00:00<00:00, 133.57it/s] 94%|█████████▍| 77/82 [00:00<00:00, 102.16it/s] 34%|███▎      | 28/83 [00:00<00:00, 134.26it/s]100%|██████████| 82/82 [00:00<00:00, 102.07it/s]
 51%|█████     | 42/83 [00:00<00:00, 131.79it/s] 67%|██████▋   | 56/83 [00:00<00:00, 131.31it/s] 84%|████████▍ | 70/83 [00:00<00:00, 130.53it/s]100%|██████████| 83/83 [00:00<00:00, 131.02it/s]
2024-06-04:07:30:26,096 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:30:26,096 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:30:26,096 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:30:26,096 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:30:26,096 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:30:26,097 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:30:26,098 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:07:30:26,096 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:23<31:55, 23.36s/it]Running generate_until requests:   2%|▏         | 2/83 [00:48<33:07, 24.54s/it]Running generate_until requests:   4%|▎         | 3/83 [00:59<24:20, 18.26s/it]Running generate_until requests:   5%|▍         | 4/83 [01:10<20:16, 15.40s/it]Running generate_until requests:   6%|▌         | 5/83 [01:21<18:06, 13.93s/it]Running generate_until requests:   7%|▋         | 6/83 [01:31<15:54, 12.40s/it]Running generate_until requests:   8%|▊         | 7/83 [01:58<21:56, 17.32s/it]Running generate_until requests:  10%|▉         | 8/83 [02:11<19:53, 15.92s/it]Running generate_until requests:  11%|█         | 9/83 [02:24<18:27, 14.97s/it]Running generate_until requests:  12%|█▏        | 10/83 [02:32<15:43, 12.93s/it]Running generate_until requests:  13%|█▎        | 11/83 [02:46<15:50, 13.19s/it]Running generate_until requests:  14%|█▍        | 12/83 [03:06<18:01, 15.24s/it]Running generate_until requests:  16%|█▌        | 13/83 [03:16<15:45, 13.51s/it]Running generate_until requests:  17%|█▋        | 14/83 [03:29<15:38, 13.61s/it]Running generate_until requests:  18%|█▊        | 15/83 [04:10<24:34, 21.68s/it]Running generate_until requests:  19%|█▉        | 16/83 [04:42<27:47, 24.89s/it]Running generate_until requests:  20%|██        | 17/83 [04:57<24:06, 21.92s/it]Running generate_until requests:  22%|██▏       | 18/83 [05:07<19:46, 18.25s/it]Running generate_until requests:  23%|██▎       | 19/83 [05:21<18:12, 17.07s/it]Running generate_until requests:  24%|██▍       | 20/83 [05:46<20:15, 19.29s/it]Running generate_until requests:  25%|██▌       | 21/83 [06:02<19:00, 18.40s/it]Running generate_until requests:  27%|██▋       | 22/83 [06:13<16:31, 16.26s/it]Running generate_until requests:  28%|██▊       | 23/83 [06:42<19:52, 19.88s/it]Running generate_until requests:  29%|██▉       | 24/83 [06:56<17:54, 18.22s/it]Running generate_until requests:  30%|███       | 25/83 [07:17<18:25, 19.05s/it]Running generate_until requests:  31%|███▏      | 26/83 [07:30<16:15, 17.11s/it]Running generate_until requests:  33%|███▎      | 27/83 [07:53<17:37, 18.88s/it]Running generate_until requests:  34%|███▎      | 28/83 [08:04<15:21, 16.76s/it]Running generate_until requests:  35%|███▍      | 29/83 [08:27<16:39, 18.50s/it]Running generate_until requests:  36%|███▌      | 30/83 [08:43<15:41, 17.77s/it]Running generate_until requests:  37%|███▋      | 31/83 [09:01<15:20, 17.70s/it]Running generate_until requests:  39%|███▊      | 32/83 [09:18<15:05, 17.76s/it]Running generate_until requests:  40%|███▉      | 33/83 [09:25<11:54, 14.30s/it]Running generate_until requests:  41%|████      | 34/83 [09:45<13:16, 16.26s/it]Running generate_until requests:  42%|████▏     | 35/83 [10:00<12:38, 15.80s/it]Running generate_until requests:  43%|████▎     | 36/83 [10:17<12:30, 15.97s/it]Running generate_until requests:  45%|████▍     | 37/83 [10:30<11:35, 15.13s/it]Running generate_until requests:  46%|████▌     | 38/83 [11:02<15:06, 20.14s/it]Running generate_until requests:  47%|████▋     | 39/83 [11:09<11:52, 16.19s/it]Running generate_until requests:  48%|████▊     | 40/83 [11:21<10:46, 15.04s/it]Running generate_until requests:  49%|████▉     | 41/83 [11:32<09:45, 13.93s/it]Running generate_until requests:  51%|█████     | 42/83 [12:22<16:51, 24.66s/it]Running generate_until requests:  52%|█████▏    | 43/83 [12:33<13:38, 20.47s/it]Running generate_until requests:  53%|█████▎    | 44/83 [12:44<11:35, 17.84s/it]Running generate_until requests:  54%|█████▍    | 45/83 [12:54<09:39, 15.24s/it]Running generate_until requests:  55%|█████▌    | 46/83 [13:04<08:29, 13.77s/it]Running generate_until requests:  57%|█████▋    | 47/83 [13:18<08:20, 13.90s/it]Running generate_until requests:  58%|█████▊    | 48/83 [13:33<08:16, 14.18s/it]Running generate_until requests:  59%|█████▉    | 49/83 [13:41<07:02, 12.44s/it]Running generate_until requests:  60%|██████    | 50/83 [13:59<07:40, 13.95s/it]Running generate_until requests:  61%|██████▏   | 51/83 [14:10<07:05, 13.29s/it]Running generate_until requests:  63%|██████▎   | 52/83 [14:20<06:21, 12.29s/it]Running generate_until requests:  64%|██████▍   | 53/83 [14:31<05:50, 11.69s/it]Running generate_until requests:  65%|██████▌   | 54/83 [14:46<06:09, 12.74s/it]Running generate_until requests:  66%|██████▋   | 55/83 [14:56<05:36, 12.01s/it]Running generate_until requests:  67%|██████▋   | 56/83 [15:30<08:19, 18.51s/it]Running generate_until requests:  69%|██████▊   | 57/83 [16:01<09:41, 22.38s/it]Running generate_until requests:  70%|██████▉   | 58/83 [16:11<07:41, 18.45s/it]Running generate_until requests:  71%|███████   | 59/83 [16:19<06:12, 15.53s/it]Running generate_until requests:  72%|███████▏  | 60/83 [16:34<05:54, 15.42s/it]Running generate_until requests:  73%|███████▎  | 61/83 [16:51<05:45, 15.70s/it]Running generate_until requests:  75%|███████▍  | 62/83 [17:04<05:16, 15.08s/it]Running generate_until requests:  76%|███████▌  | 63/83 [17:16<04:40, 14.04s/it]Running generate_until requests:  77%|███████▋  | 64/83 [17:31<04:33, 14.38s/it]Running generate_until requests:  78%|███████▊  | 65/83 [17:40<03:51, 12.84s/it]Running generate_until requests:  80%|███████▉  | 66/83 [17:50<03:20, 11.78s/it]Running generate_until requests:  81%|████████  | 67/83 [17:58<02:49, 10.59s/it]Running generate_until requests:  82%|████████▏ | 68/83 [18:06<02:30, 10.05s/it]Running generate_until requests:  83%|████████▎ | 69/83 [18:29<03:15, 13.95s/it]Running generate_until requests:  84%|████████▍ | 70/83 [18:44<03:05, 14.27s/it]Running generate_until requests:  86%|████████▌ | 71/83 [18:51<02:22, 11.84s/it]Running generate_until requests:  87%|████████▋ | 72/83 [18:55<01:45,  9.60s/it]Running generate_until requests:  88%|████████▊ | 73/83 [19:07<01:41, 10.20s/it]Running generate_until requests:  89%|████████▉ | 74/83 [19:19<01:38, 10.94s/it]Running generate_until requests:  90%|█████████ | 75/83 [19:42<01:56, 14.58s/it]Running generate_until requests:  92%|█████████▏| 76/83 [19:53<01:33, 13.30s/it]Running generate_until requests:  93%|█████████▎| 77/83 [20:04<01:16, 12.76s/it]Running generate_until requests:  94%|█████████▍| 78/83 [20:13<00:58, 11.63s/it]Running generate_until requests:  95%|█████████▌| 79/83 [20:34<00:57, 14.44s/it]Running generate_until requests:  96%|█████████▋| 80/83 [20:45<00:39, 13.23s/it]Running generate_until requests:  98%|█████████▊| 81/83 [20:55<00:24, 12.30s/it]Running generate_until requests:  99%|█████████▉| 82/83 [21:05<00:11, 11.76s/it]Running generate_until requests: 100%|██████████| 83/83 [21:12<00:00, 10.23s/it]Running generate_until requests: 100%|██████████| 83/83 [21:12<00:00, 15.33s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:07:56:57,792 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:56:57,800 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:56:58,047 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:56:58,351 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:56:58,370 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:56:58,494 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:56:58,503 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:56:58,548 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:07:57:03,358 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:57:03,360 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:57:03,365 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:57:03,365 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:57:03,566 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:57:03,568 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:57:03,575 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:57:03,575 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:07:57:04,823 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:57:04,824 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:57:04,829 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:57:04,829 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:57:05,147 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:57:05,148 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:57:05,153 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:57:05,153 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:57:05,171 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:57:05,173 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:57:05,178 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:57:05,178 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:57:05,216 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:57:05,217 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:57:05,221 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:57:05,221 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:57:05,363 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:57:05,364 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:57:05,369 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:57:05,369 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:07:57:05,415 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:07:57:05,416 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:07:57:05,420 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:07:57:05,420 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.28s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.41s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.11s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.76s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.37s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.64s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.70s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:03,  2.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.28s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.74s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.79s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.86s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.62s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.87s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.23s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.19s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:57:53,071 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:57:53,073 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:57:53,363 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 140.71it/s] 36%|███▌      | 30/83 [00:00<00:00, 141.00it/s] 54%|█████▍    | 45/83 [00:00<00:00, 141.31it/s] 72%|███████▏  | 60/83 [00:00<00:00, 141.28it/s] 90%|█████████ | 75/83 [00:00<00:00, 136.40it/s]100%|██████████| 83/83 [00:00<00:00, 139.02it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:57:54,573 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:57:54,575 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:57:54,862 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 136.21it/s] 34%|███▍      | 28/82 [00:00<00:00, 137.20it/s] 51%|█████     | 42/82 [00:00<00:00, 137.39it/s] 68%|██████▊   | 56/82 [00:00<00:00, 137.53it/s] 85%|████████▌ | 70/82 [00:00<00:00, 137.48it/s]100%|██████████| 82/82 [00:00<00:00, 137.42it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:58:11,984 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:58:11,986 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:58:12,153 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 207.36it/s] 51%|█████     | 42/82 [00:00<00:00, 208.19it/s] 77%|███████▋  | 63/82 [00:00<00:00, 208.40it/s]100%|██████████| 82/82 [00:00<00:00, 208.25it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:58:13,016 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:58:13,018 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:58:13,185 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 206.89it/s] 51%|█████     | 42/82 [00:00<00:00, 200.73it/s] 77%|███████▋  | 63/82 [00:00<00:00, 198.08it/s]100%|██████████| 82/82 [00:00<00:00, 198.97it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:07:58:15,264 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:58:15,339 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:58:15,341 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:58:15,628 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 136.24it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 34%|███▎      | 28/83 [00:00<00:00, 137.22it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:58:15,912 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:58:15,915 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 52%|█████▏    | 43/83 [00:00<00:00, 142.78it/s] 70%|██████▉   | 58/83 [00:00<00:00, 136.77it/s] 87%|████████▋ | 72/83 [00:00<00:00, 133.84it/s]2024-06-04:07:58:16,225 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s]100%|██████████| 83/83 [00:00<00:00, 135.07it/s]
 16%|█▌        | 13/82 [00:00<00:00, 120.96it/s] 32%|███▏      | 26/82 [00:00<00:00, 119.90it/s] 46%|████▋     | 38/82 [00:00<00:00, 119.62it/s] 63%|██████▎   | 52/82 [00:00<00:00, 125.63it/s] 80%|████████  | 66/82 [00:00<00:00, 128.77it/s] 98%|█████████▊| 80/82 [00:00<00:00, 130.71it/s]100%|██████████| 82/82 [00:00<00:00, 127.28it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:58:35,566 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:58:35,568 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:58:36,065 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 100.82it/s] 27%|██▋       | 22/83 [00:00<00:00, 101.99it/s] 40%|███▉      | 33/83 [00:00<00:00, 102.75it/s] 53%|█████▎    | 44/83 [00:00<00:00, 102.96it/s] 66%|██████▋   | 55/83 [00:00<00:00, 103.14it/s] 80%|███████▉  | 66/83 [00:00<00:00, 103.15it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 93%|█████████▎| 77/83 [00:00<00:00, 102.84it/s]100%|██████████| 83/83 [00:00<00:00, 102.89it/s]
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:07:58:36,928 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:58:36,930 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:07:58:37,508 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 101.11it/s] 27%|██▋       | 22/83 [00:00<00:00, 101.63it/s] 40%|███▉      | 33/83 [00:00<00:00, 102.17it/s] 53%|█████▎    | 44/83 [00:00<00:00, 102.50it/s] 66%|██████▋   | 55/83 [00:00<00:00, 102.43it/s] 80%|███████▉  | 66/83 [00:00<00:00, 102.53it/s] 93%|█████████▎| 77/83 [00:00<00:00, 102.45it/s]100%|██████████| 83/83 [00:00<00:00, 102.29it/s]
2024-06-04:07:58:47,721 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:58:47,721 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:58:47,721 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:58:47,721 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:58:47,722 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:58:47,721 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:07:58:47,723 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:07:58:47,733 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:15<21:07, 15.46s/it]Running generate_until requests:   2%|▏         | 2/83 [00:28<19:12, 14.23s/it]Running generate_until requests:   4%|▎         | 3/83 [00:36<15:00, 11.25s/it]Running generate_until requests:   5%|▍         | 4/83 [00:41<11:35,  8.80s/it]Running generate_until requests:   6%|▌         | 5/83 [00:49<10:53,  8.37s/it]Running generate_until requests:   7%|▋         | 6/83 [00:57<10:44,  8.37s/it]Running generate_until requests:   8%|▊         | 7/83 [01:09<12:04,  9.53s/it]Running generate_until requests:  10%|▉         | 8/83 [01:17<11:09,  8.93s/it]Running generate_until requests:  11%|█         | 9/83 [01:25<10:44,  8.72s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:32<09:50,  8.09s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:42<10:26,  8.70s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:53<11:15,  9.52s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:58<09:27,  8.11s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:05<08:52,  7.72s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:21<11:48, 10.42s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:37<13:18, 11.91s/it]Running generate_until requests:  20%|██        | 17/83 [02:45<12:01, 10.93s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:52<10:23,  9.59s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:00<09:48,  9.20s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:11<10:16,  9.78s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:21<09:56,  9.63s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:27<08:51,  8.71s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:33<07:55,  7.93s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:39<07:13,  7.34s/it]Running generate_until requests:  30%|███       | 25/83 [03:48<07:35,  7.85s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:55<07:01,  7.39s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:05<07:51,  8.41s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:12<07:04,  7.72s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:22<07:35,  8.44s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:29<07:12,  8.16s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:37<07:05,  8.18s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:46<06:58,  8.21s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:49<05:40,  6.81s/it]Running generate_until requests:  41%|████      | 34/83 [05:00<06:34,  8.06s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:07<06:12,  7.76s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:14<05:56,  7.58s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:23<06:06,  7.97s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:34<06:35,  8.80s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:39<05:32,  7.55s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:46<05:18,  7.41s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:51<04:43,  6.75s/it]Running generate_until requests:  51%|█████     | 42/83 [06:03<05:38,  8.26s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:10<05:16,  7.90s/it]Running generate_until requests:  53%|█████▎    | 44/83 [06:15<04:36,  7.08s/it]Running generate_until requests:  54%|█████▍    | 45/83 [06:21<04:17,  6.76s/it]Running generate_until requests:  55%|█████▌    | 46/83 [06:26<03:54,  6.34s/it]Running generate_until requests:  57%|█████▋    | 47/83 [06:34<04:07,  6.87s/it]Running generate_until requests:  58%|█████▊    | 48/83 [06:44<04:27,  7.65s/it]Running generate_until requests:  59%|█████▉    | 49/83 [06:48<03:47,  6.69s/it]Running generate_until requests:  60%|██████    | 50/83 [06:59<04:20,  7.90s/it]Running generate_until requests:  61%|██████▏   | 51/83 [07:08<04:24,  8.26s/it]Running generate_until requests:  63%|██████▎   | 52/83 [07:15<04:01,  7.78s/it]Running generate_until requests:  64%|██████▍   | 53/83 [07:25<04:15,  8.52s/it]Running generate_until requests:  65%|██████▌   | 54/83 [07:31<03:45,  7.79s/it]Running generate_until requests:  66%|██████▋   | 55/83 [07:37<03:25,  7.34s/it]Running generate_until requests:  67%|██████▋   | 56/83 [07:55<04:40, 10.38s/it]Running generate_until requests:  69%|██████▊   | 57/83 [08:04<04:18,  9.95s/it]Running generate_until requests:  70%|██████▉   | 58/83 [08:12<03:54,  9.36s/it]Running generate_until requests:  71%|███████   | 59/83 [08:18<03:24,  8.54s/it]Running generate_until requests:  72%|███████▏  | 60/83 [08:25<03:01,  7.88s/it]Running generate_until requests:  73%|███████▎  | 61/83 [08:36<03:16,  8.91s/it]Running generate_until requests:  75%|███████▍  | 62/83 [08:42<02:46,  7.95s/it]Running generate_until requests:  76%|███████▌  | 63/83 [08:50<02:42,  8.11s/it]Running generate_until requests:  77%|███████▋  | 64/83 [09:05<03:12, 10.13s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:11<02:41,  8.94s/it]Running generate_until requests:  80%|███████▉  | 66/83 [09:17<02:15,  7.97s/it]Running generate_until requests:  81%|████████  | 67/83 [09:21<01:46,  6.68s/it]Running generate_until requests:  82%|████████▏ | 68/83 [09:27<01:39,  6.61s/it]Running generate_until requests:  83%|████████▎ | 69/83 [09:40<02:00,  8.63s/it]Running generate_until requests:  84%|████████▍ | 70/83 [09:50<01:57,  9.00s/it]Running generate_until requests:  86%|████████▌ | 71/83 [09:56<01:34,  7.87s/it]Running generate_until requests:  87%|████████▋ | 72/83 [10:02<01:22,  7.48s/it]Running generate_until requests:  88%|████████▊ | 73/83 [10:09<01:13,  7.39s/it]Running generate_until requests:  89%|████████▉ | 74/83 [10:15<01:02,  6.98s/it]Running generate_until requests:  90%|█████████ | 75/83 [10:31<01:16,  9.60s/it]Running generate_until requests:  92%|█████████▏| 76/83 [10:36<00:58,  8.32s/it]Running generate_until requests:  93%|█████████▎| 77/83 [10:43<00:46,  7.83s/it]Running generate_until requests:  94%|█████████▍| 78/83 [10:50<00:38,  7.70s/it]Running generate_until requests:  95%|█████████▌| 79/83 [10:58<00:30,  7.67s/it]Running generate_until requests:  96%|█████████▋| 80/83 [11:03<00:20,  6.89s/it]Running generate_until requests:  98%|█████████▊| 81/83 [11:11<00:14,  7.10s/it]Running generate_until requests:  99%|█████████▉| 82/83 [11:17<00:06,  6.89s/it]Running generate_until requests: 100%|██████████| 83/83 [11:23<00:00,  6.48s/it]Running generate_until requests: 100%|██████████| 83/83 [11:23<00:00,  8.23s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:08:15:43,261 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:15:43,261 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:15:43,261 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:15:43,388 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:15:43,486 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:15:43,612 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:15:43,628 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:15:43,666 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:15:48,459 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:15:48,461 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:15:48,467 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:15:48,467 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:15:48,522 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:15:48,524 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:15:48,531 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:15:48,531 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:08:15:50,012 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:15:50,014 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:15:50,021 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:15:50,021 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:15:50,164 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:15:50,166 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:15:50,170 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:15:50,171 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:15:50,350 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:15:50,352 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:15:50,358 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:15:50,358 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:15:50,439 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:15:50,440 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:15:50,445 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:15:50,445 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:15:50,539 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:15:50,539 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:15:50,540 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:15:50,540 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:15:50,544 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:15:50,544 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:15:50,544 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:15:50,544 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.18s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.28s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.49s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.54s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.14s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.67s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.77s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.84s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.80s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.05s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.70s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.73s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.99s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.16s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.04s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.21s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.19s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:08:16:37,208 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:16:37,280 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:16:37,281 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:16:37,452 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 25%|██▌       | 21/83 [00:00<00:00, 202.63it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:16:37,578 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:16:37,580 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 51%|█████     | 42/83 [00:00<00:00, 201.50it/s]2024-06-04:08:16:37,746 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 76%|███████▌  | 63/83 [00:00<00:00, 202.64it/s] 25%|██▌       | 21/83 [00:00<00:00, 203.53it/s]100%|██████████| 83/83 [00:00<00:00, 203.14it/s]
 51%|█████     | 42/83 [00:00<00:00, 187.77it/s] 76%|███████▌  | 63/83 [00:00<00:00, 194.64it/s]100%|██████████| 83/83 [00:00<00:00, 196.30it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:17:17,500 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:17:17,502 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:17:17,862 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 135.40it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 35%|███▌      | 29/82 [00:00<00:00, 143.69it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:17:18,125 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:17:18,128 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:17:18,163 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:17:18,165 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 54%|█████▎    | 44/82 [00:00<00:00, 145.34it/s] 72%|███████▏  | 59/82 [00:00<00:00, 138.60it/s] 89%|████████▉ | 73/82 [00:00<00:00, 135.40it/s]2024-06-04:08:17:18,458 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s]100%|██████████| 82/82 [00:00<00:00, 136.97it/s]
2024-06-04:08:17:18,518 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 132.30it/s] 17%|█▋        | 14/82 [00:00<00:00, 133.55it/s] 34%|███▍      | 28/82 [00:00<00:00, 98.16it/s]  34%|███▍      | 28/82 [00:00<00:00, 134.26it/s] 52%|█████▏    | 43/82 [00:00<00:00, 116.14it/s] 51%|█████     | 42/82 [00:00<00:00, 128.51it/s] 71%|███████   | 58/82 [00:00<00:00, 126.60it/s] 67%|██████▋   | 55/82 [00:00<00:00, 127.22it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 88%|████████▊ | 72/82 [00:00<00:00, 130.07it/s] 84%|████████▍ | 69/82 [00:00<00:00, 129.14it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:17:19,108 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:17:19,110 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
100%|██████████| 82/82 [00:00<00:00, 124.90it/s]
100%|██████████| 82/82 [00:00<00:00, 130.80it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:17:19,336 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:17:19,338 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:17:19,460 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:17:19,462 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:17:19,511 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 16%|█▌        | 13/83 [00:00<00:00, 126.09it/s] 31%|███▏      | 26/83 [00:00<00:00, 126.94it/s]2024-06-04:08:17:19,779 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
2024-06-04:08:17:19,779 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s]  0%|          | 0/83 [00:00<?, ?it/s] 47%|████▋     | 39/83 [00:00<00:00, 127.96it/s] 17%|█▋        | 14/82 [00:00<00:00, 132.86it/s] 13%|█▎        | 11/83 [00:00<00:00, 104.98it/s] 64%|██████▍   | 53/83 [00:00<00:00, 129.96it/s] 34%|███▍      | 28/82 [00:00<00:00, 133.10it/s] 27%|██▋       | 22/83 [00:00<00:00, 105.25it/s] 81%|████████  | 67/83 [00:00<00:00, 131.22it/s] 51%|█████     | 42/82 [00:00<00:00, 132.69it/s] 40%|███▉      | 33/83 [00:00<00:00, 105.52it/s] 98%|█████████▊| 81/83 [00:00<00:00, 131.10it/s]100%|██████████| 83/83 [00:00<00:00, 130.07it/s]
 68%|██████▊   | 56/82 [00:00<00:00, 131.67it/s] 53%|█████▎    | 44/83 [00:00<00:00, 105.59it/s] 85%|████████▌ | 70/82 [00:00<00:00, 131.08it/s] 66%|██████▋   | 55/83 [00:00<00:00, 105.56it/s]100%|██████████| 82/82 [00:00<00:00, 131.22it/s]
 80%|███████▉  | 66/83 [00:00<00:00, 105.61it/s] 93%|█████████▎| 77/83 [00:00<00:00, 105.55it/s]100%|██████████| 83/83 [00:00<00:00, 105.52it/s]
2024-06-04:08:17:30,706 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:17:30,706 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:17:30,706 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:17:30,706 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:17:30,706 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:17:30,706 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:17:30,706 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:08:17:30,707 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:14<19:32, 14.30s/it]Running generate_until requests:   2%|▏         | 2/83 [00:29<20:18, 15.04s/it]Running generate_until requests:   4%|▎         | 3/83 [00:36<15:04, 11.30s/it]Running generate_until requests:   5%|▍         | 4/83 [00:41<11:22,  8.64s/it]Running generate_until requests:   6%|▌         | 5/83 [00:48<10:35,  8.15s/it]Running generate_until requests:   7%|▋         | 6/83 [00:53<09:00,  7.02s/it]Running generate_until requests:   8%|▊         | 7/83 [01:01<09:24,  7.43s/it]Running generate_until requests:  10%|▉         | 8/83 [01:07<08:50,  7.07s/it]Running generate_until requests:  11%|█         | 9/83 [01:16<09:07,  7.40s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:21<08:23,  6.89s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:27<07:53,  6.57s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:37<08:52,  7.50s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:42<07:48,  6.70s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:49<07:59,  6.95s/it]Running generate_until requests:  18%|█▊        | 15/83 [01:59<08:55,  7.87s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:14<11:16, 10.10s/it]Running generate_until requests:  20%|██        | 17/83 [02:22<10:15,  9.33s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:28<09:10,  8.47s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:34<08:13,  7.71s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:44<08:45,  8.34s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:51<08:11,  7.92s/it]Running generate_until requests:  27%|██▋       | 22/83 [02:57<07:23,  7.27s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:07<07:58,  7.98s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:13<07:20,  7.47s/it]Running generate_until requests:  30%|███       | 25/83 [03:21<07:28,  7.74s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:27<06:44,  7.10s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:36<07:12,  7.72s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:42<06:31,  7.12s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:48<06:10,  6.86s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:57<06:36,  7.47s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:03<06:07,  7.06s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:09<05:46,  6.80s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:13<05:00,  6.01s/it]Running generate_until requests:  41%|████      | 34/83 [04:23<05:46,  7.08s/it]Running generate_until requests:  42%|████▏     | 35/83 [04:29<05:18,  6.65s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:34<04:56,  6.30s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:40<04:50,  6.32s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:52<05:57,  7.94s/it]Running generate_until requests:  47%|████▋     | 39/83 [04:57<05:05,  6.95s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:03<04:49,  6.74s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:08<04:22,  6.25s/it]Running generate_until requests:  51%|█████     | 42/83 [05:13<04:05,  5.98s/it]Running generate_until requests:  52%|█████▏    | 43/83 [05:20<04:11,  6.28s/it]Running generate_until requests:  53%|█████▎    | 44/83 [05:25<03:44,  5.76s/it]Running generate_until requests:  54%|█████▍    | 45/83 [05:31<03:41,  5.83s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:36<03:31,  5.72s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:42<03:24,  5.69s/it]Running generate_until requests:  58%|█████▊    | 48/83 [05:50<03:44,  6.42s/it]Running generate_until requests:  59%|█████▉    | 49/83 [05:54<03:11,  5.63s/it]Running generate_until requests:  60%|██████    | 50/83 [06:03<03:35,  6.52s/it]Running generate_until requests:  61%|██████▏   | 51/83 [06:10<03:39,  6.87s/it]Running generate_until requests:  63%|██████▎   | 52/83 [06:14<03:08,  6.09s/it]Running generate_until requests:  64%|██████▍   | 53/83 [06:21<03:05,  6.19s/it]Running generate_until requests:  65%|██████▌   | 54/83 [06:25<02:45,  5.69s/it]Running generate_until requests:  66%|██████▋   | 55/83 [06:31<02:34,  5.54s/it]Running generate_until requests:  67%|██████▋   | 56/83 [06:43<03:27,  7.68s/it]Running generate_until requests:  69%|██████▊   | 57/83 [06:55<03:52,  8.93s/it]Running generate_until requests:  70%|██████▉   | 58/83 [07:01<03:21,  8.06s/it]Running generate_until requests:  71%|███████   | 59/83 [07:06<02:48,  7.04s/it]Running generate_until requests:  72%|███████▏  | 60/83 [07:10<02:24,  6.29s/it]Running generate_until requests:  73%|███████▎  | 61/83 [07:18<02:23,  6.54s/it]Running generate_until requests:  75%|███████▍  | 62/83 [07:23<02:10,  6.20s/it]Running generate_until requests:  76%|███████▌  | 63/83 [07:28<02:00,  6.00s/it]Running generate_until requests:  77%|███████▋  | 64/83 [07:36<02:01,  6.40s/it]Running generate_until requests:  78%|███████▊  | 65/83 [07:41<01:47,  5.99s/it]Running generate_until requests:  80%|███████▉  | 66/83 [07:45<01:34,  5.58s/it]Running generate_until requests:  81%|████████  | 67/83 [07:49<01:17,  4.85s/it]Running generate_until requests:  82%|████████▏ | 68/83 [07:54<01:15,  5.04s/it]Running generate_until requests:  83%|████████▎ | 69/83 [08:03<01:25,  6.11s/it]Running generate_until requests:  84%|████████▍ | 70/83 [08:10<01:26,  6.62s/it]Running generate_until requests:  86%|████████▌ | 71/83 [08:15<01:10,  5.91s/it]Running generate_until requests:  87%|████████▋ | 72/83 [08:20<01:02,  5.68s/it]Running generate_until requests:  88%|████████▊ | 73/83 [08:25<00:55,  5.55s/it]Running generate_until requests:  89%|████████▉ | 74/83 [08:30<00:48,  5.35s/it]Running generate_until requests:  90%|█████████ | 75/83 [08:40<00:53,  6.74s/it]Running generate_until requests:  92%|█████████▏| 76/83 [08:44<00:41,  6.00s/it]Running generate_until requests:  93%|█████████▎| 77/83 [08:50<00:36,  6.01s/it]Running generate_until requests:  94%|█████████▍| 78/83 [08:57<00:31,  6.23s/it]Running generate_until requests:  95%|█████████▌| 79/83 [09:05<00:26,  6.68s/it]Running generate_until requests:  96%|█████████▋| 80/83 [09:09<00:17,  5.92s/it]Running generate_until requests:  98%|█████████▊| 81/83 [09:14<00:11,  5.74s/it]Running generate_until requests:  99%|█████████▉| 82/83 [09:19<00:05,  5.36s/it]Running generate_until requests: 100%|██████████| 83/83 [09:22<00:00,  4.82s/it]Running generate_until requests: 100%|██████████| 83/83 [09:22<00:00,  6.78s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:08:33:33,585 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:33:33,719 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:33:33,741 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:33:34,107 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:33:34,109 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:33:34,131 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:33:34,150 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:33:34,372 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:33:39,299 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:33:39,301 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:33:39,301 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:33:39,302 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:33:39,307 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:33:39,307 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:33:39,308 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:33:39,308 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:08:33:40,661 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:33:40,663 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:33:40,668 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:33:40,668 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:33:41,061 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:33:41,062 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:33:41,068 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:33:41,068 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:33:41,087 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:33:41,089 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:33:41,094 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:33:41,094 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:33:41,108 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:33:41,109 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:33:41,114 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:33:41,115 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:33:41,142 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:33:41,143 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:33:41,149 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:33:41,149 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:33:41,270 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:33:41,271 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:33:41,275 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:33:41,275 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.07s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.18s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.08s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.77s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.77s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.82s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.71s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:01,  1.97s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.64s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.78s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.19s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.74s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.18s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.24s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.24s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.22s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:08:34:27,724 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:34:27,799 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:34:27,801 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:08:34:27,980 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:34:28,016 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:34:28,018 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 24%|██▍       | 20/83 [00:00<00:00, 197.20it/s]2024-06-04:08:34:28,181 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 49%|████▉     | 41/83 [00:00<00:00, 202.09it/s] 25%|██▌       | 21/83 [00:00<00:00, 202.63it/s] 75%|███████▍  | 62/83 [00:00<00:00, 204.04it/s] 51%|█████     | 42/83 [00:00<00:00, 203.36it/s]100%|██████████| 83/83 [00:00<00:00, 205.06it/s]100%|██████████| 83/83 [00:00<00:00, 203.87it/s]
 76%|███████▌  | 63/83 [00:00<00:00, 176.10it/s]100%|██████████| 83/83 [00:00<00:00, 187.14it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:35:08,760 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:35:08,763 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:35:09,009 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 135.90it/s] 35%|███▍      | 29/83 [00:00<00:00, 140.04it/s] 53%|█████▎    | 44/83 [00:00<00:00, 141.21it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 71%|███████   | 59/83 [00:00<00:00, 141.79it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:35:09,507 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:35:09,509 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:35:09,518 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:35:09,520 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 92%|█████████▏| 76/83 [00:00<00:00, 149.07it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:35:09,578 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:35:09,580 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
100%|██████████| 83/83 [00:00<00:00, 143.43it/s]
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:35:09,629 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:35:09,632 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:08:35:09,846 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
2024-06-04:08:35:09,851 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/83 [00:00<?, ?it/s]  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:35:09,880 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:35:09,883 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:35:09,895 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 16%|█▌        | 13/83 [00:00<00:00, 126.13it/s] 16%|█▌        | 13/82 [00:00<00:00, 123.61it/s]2024-06-04:08:35:09,983 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 102.09it/s] 31%|███▏      | 26/83 [00:00<00:00, 123.70it/s] 32%|███▏      | 26/82 [00:00<00:00, 125.68it/s] 15%|█▍        | 12/82 [00:00<00:00, 119.84it/s] 27%|██▋       | 22/82 [00:00<00:00, 102.99it/s] 48%|████▊     | 39/82 [00:00<00:00, 126.75it/s]2024-06-04:08:35:10,215 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
 47%|████▋     | 39/83 [00:00<00:00, 107.93it/s] 29%|██▉       | 24/82 [00:00<00:00, 105.32it/s]  0%|          | 0/82 [00:00<?, ?it/s] 40%|████      | 33/82 [00:00<00:00, 103.47it/s] 65%|██████▍   | 53/82 [00:00<00:00, 128.76it/s] 17%|█▋        | 14/82 [00:00<00:00, 134.31it/s] 54%|█████▎    | 44/82 [00:00<00:00, 103.83it/s] 43%|████▎     | 35/82 [00:00<00:00, 85.80it/s]  82%|████████▏ | 67/82 [00:00<00:00, 131.24it/s] 61%|██████▏   | 51/83 [00:00<00:00, 86.92it/s]  34%|███▍      | 28/82 [00:00<00:00, 135.40it/s] 67%|██████▋   | 55/82 [00:00<00:00, 103.92it/s] 99%|█████████▉| 81/82 [00:00<00:00, 132.62it/s]100%|██████████| 82/82 [00:00<00:00, 130.28it/s]
 54%|█████▎    | 44/82 [00:00<00:00, 79.82it/s] 73%|███████▎  | 61/83 [00:00<00:00, 82.70it/s] 52%|█████▏    | 43/82 [00:00<00:00, 138.70it/s] 80%|████████  | 66/82 [00:00<00:00, 103.83it/s] 65%|██████▍   | 53/82 [00:00<00:00, 76.03it/s] 70%|██████▉   | 57/82 [00:00<00:00, 138.70it/s] 84%|████████▍ | 70/83 [00:00<00:00, 78.53it/s] 94%|█████████▍| 77/82 [00:00<00:00, 103.86it/s]100%|██████████| 82/82 [00:00<00:00, 103.70it/s]
 87%|████████▋ | 71/82 [00:00<00:00, 138.13it/s] 74%|███████▍  | 61/82 [00:00<00:00, 72.44it/s] 95%|█████████▌| 79/83 [00:00<00:00, 72.35it/s]100%|██████████| 82/82 [00:00<00:00, 137.24it/s]
100%|██████████| 83/83 [00:00<00:00, 85.22it/s]
 88%|████████▊ | 72/82 [00:00<00:00, 82.21it/s]100%|██████████| 82/82 [00:00<00:00, 87.29it/s]
2024-06-04:08:35:19,430 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:35:19,431 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:35:19,431 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:35:19,431 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:35:19,431 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:35:19,431 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:35:19,431 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:35:19,431 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:12<17:08, 12.54s/it]Running generate_until requests:   2%|▏         | 2/83 [00:23<15:46, 11.68s/it]Running generate_until requests:   4%|▎         | 3/83 [00:30<12:40,  9.50s/it]Running generate_until requests:   5%|▍         | 4/83 [00:35<10:16,  7.80s/it]Running generate_until requests:   6%|▌         | 5/83 [00:43<09:55,  7.63s/it]Running generate_until requests:   7%|▋         | 6/83 [00:48<08:48,  6.86s/it]Running generate_until requests:   8%|▊         | 7/83 [00:56<09:19,  7.36s/it]Running generate_until requests:  10%|▉         | 8/83 [01:03<08:48,  7.05s/it]Running generate_until requests:  11%|█         | 9/83 [01:11<09:08,  7.42s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:16<08:17,  6.82s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:24<08:31,  7.10s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:35<09:53,  8.36s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:40<08:28,  7.26s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:48<08:29,  7.38s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:03<10:59,  9.70s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:16<11:58, 10.73s/it]Running generate_until requests:  20%|██        | 17/83 [02:25<11:08, 10.13s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:33<10:16,  9.49s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:40<09:33,  8.96s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:50<09:32,  9.08s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:56<08:26,  8.16s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:02<07:34,  7.45s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:09<07:29,  7.48s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:15<06:49,  6.95s/it]Running generate_until requests:  30%|███       | 25/83 [03:20<06:20,  6.55s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:26<05:58,  6.30s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:35<06:35,  7.07s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:41<06:06,  6.66s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:48<06:03,  6.74s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:57<06:33,  7.42s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:01<05:41,  6.56s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:07<05:27,  6.42s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:12<04:47,  5.76s/it]Running generate_until requests:  41%|████      | 34/83 [04:21<05:34,  6.83s/it]Running generate_until requests:  42%|████▏     | 35/83 [04:26<05:07,  6.40s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:34<05:17,  6.75s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:41<05:12,  6.80s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:52<06:01,  8.03s/it]Running generate_until requests:  47%|████▋     | 39/83 [04:56<05:08,  7.02s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:02<04:38,  6.48s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:06<04:07,  5.89s/it]Running generate_until requests:  51%|█████     | 42/83 [05:11<03:55,  5.74s/it]Running generate_until requests:  52%|█████▏    | 43/83 [05:18<04:04,  6.12s/it]Running generate_until requests:  53%|█████▎    | 44/83 [05:23<03:40,  5.65s/it]Running generate_until requests:  54%|█████▍    | 45/83 [05:29<03:34,  5.65s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:34<03:29,  5.66s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:40<03:18,  5.51s/it]Running generate_until requests:  58%|█████▊    | 48/83 [05:46<03:23,  5.83s/it]Running generate_until requests:  59%|█████▉    | 49/83 [05:50<03:01,  5.35s/it]Running generate_until requests:  60%|██████    | 50/83 [05:58<03:14,  5.90s/it]Running generate_until requests:  61%|██████▏   | 51/83 [06:03<03:08,  5.89s/it]Running generate_until requests:  63%|██████▎   | 52/83 [06:09<02:56,  5.70s/it]Running generate_until requests:  64%|██████▍   | 53/83 [06:15<02:57,  5.91s/it]Running generate_until requests:  65%|██████▌   | 54/83 [06:20<02:38,  5.48s/it]Running generate_until requests:  66%|██████▋   | 55/83 [06:25<02:32,  5.44s/it]Running generate_until requests:  67%|██████▋   | 56/83 [06:37<03:24,  7.56s/it]Running generate_until requests:  69%|██████▊   | 57/83 [06:46<03:25,  7.90s/it]Running generate_until requests:  70%|██████▉   | 58/83 [06:52<03:03,  7.35s/it]Running generate_until requests:  71%|███████   | 59/83 [06:57<02:37,  6.55s/it]Running generate_until requests:  72%|███████▏  | 60/83 [07:01<02:16,  5.93s/it]Running generate_until requests:  73%|███████▎  | 61/83 [07:08<02:16,  6.23s/it]Running generate_until requests:  75%|███████▍  | 62/83 [07:14<02:10,  6.21s/it]Running generate_until requests:  76%|███████▌  | 63/83 [07:19<01:54,  5.73s/it]Running generate_until requests:  77%|███████▋  | 64/83 [07:28<02:05,  6.62s/it]Running generate_until requests:  78%|███████▊  | 65/83 [07:33<01:50,  6.15s/it]Running generate_until requests:  80%|███████▉  | 66/83 [07:38<01:41,  5.96s/it]Running generate_until requests:  81%|████████  | 67/83 [07:42<01:23,  5.22s/it]Running generate_until requests:  82%|████████▏ | 68/83 [07:47<01:18,  5.24s/it]Running generate_until requests:  83%|████████▎ | 69/83 [07:56<01:28,  6.29s/it]Running generate_until requests:  84%|████████▍ | 70/83 [08:03<01:25,  6.54s/it]Running generate_until requests:  86%|████████▌ | 71/83 [08:07<01:10,  5.86s/it]Running generate_until requests:  87%|████████▋ | 72/83 [08:15<01:10,  6.39s/it]Running generate_until requests:  88%|████████▊ | 73/83 [08:20<01:00,  6.03s/it]Running generate_until requests:  89%|████████▉ | 74/83 [08:25<00:51,  5.78s/it]Running generate_until requests:  90%|█████████ | 75/83 [08:33<00:52,  6.51s/it]Running generate_until requests:  92%|█████████▏| 76/83 [08:41<00:48,  6.86s/it]Running generate_until requests:  93%|█████████▎| 77/83 [08:46<00:37,  6.32s/it]Running generate_until requests:  94%|█████████▍| 78/83 [08:52<00:30,  6.07s/it]Running generate_until requests:  95%|█████████▌| 79/83 [08:59<00:26,  6.56s/it]Running generate_until requests:  96%|█████████▋| 80/83 [09:06<00:19,  6.49s/it]Running generate_until requests:  98%|█████████▊| 81/83 [09:11<00:12,  6.12s/it]Running generate_until requests:  99%|█████████▉| 82/83 [09:15<00:05,  5.53s/it]Running generate_until requests: 100%|██████████| 83/83 [09:20<00:00,  5.39s/it]Running generate_until requests: 100%|██████████| 83/83 [09:20<00:00,  6.75s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:08:50:03,948 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:50:03,949 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:50:04,001 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:50:04,062 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:50:04,079 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:50:04,155 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:50:04,160 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:50:04,172 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:08:50:08,886 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:50:08,887 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:50:08,892 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:50:08,892 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:50:09,129 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:50:09,131 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:50:09,137 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:50:09,138 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:08:50:10,921 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:50:10,922 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:50:10,927 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:50:10,927 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:50:10,937 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:50:10,939 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:50:10,944 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:50:10,945 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:50:10,978 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:50:10,980 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:50:10,980 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:50:10,981 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:50:10,985 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:50:10,985 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:50:10,986 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:50:10,986 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:50:11,039 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:50:11,041 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:50:11,045 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:50:11,045 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:08:50:11,220 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:08:50:11,222 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:08:50:11,226 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:08:50:11,226 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.04s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.10s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.06s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.97s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.79s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.87s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.85s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.78s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.74s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.63s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.26s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.20s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.14s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.16s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.25s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.99s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.29s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:50:57,838 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:50:57,840 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:50:58,123 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 132.12it/s] 34%|███▎      | 28/83 [00:00<00:00, 133.41it/s] 52%|█████▏    | 43/83 [00:00<00:00, 136.89it/s] 70%|██████▉   | 58/83 [00:00<00:00, 139.38it/s] 88%|████████▊ | 73/83 [00:00<00:00, 141.29it/s]100%|██████████| 83/83 [00:00<00:00, 138.38it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:51:03,703 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:51:03,705 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:51:03,880 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 195.90it/s] 49%|████▉     | 40/82 [00:00<00:00, 196.04it/s] 73%|███████▎  | 60/82 [00:00<00:00, 197.02it/s] 98%|█████████▊| 80/82 [00:00<00:00, 197.53it/s]100%|██████████| 82/82 [00:00<00:00, 197.12it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:51:15,051 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:51:15,053 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:51:15,219 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 203.11it/s] 51%|█████     | 42/82 [00:00<00:00, 203.74it/s] 77%|███████▋  | 63/82 [00:00<00:00, 203.88it/s]100%|██████████| 82/82 [00:00<00:00, 203.97it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:51:19,370 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:51:19,372 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:51:19,546 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 203.58it/s] 51%|█████     | 42/83 [00:00<00:00, 205.04it/s] 76%|███████▌  | 63/83 [00:00<00:00, 205.51it/s]100%|██████████| 83/83 [00:00<00:00, 205.70it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:08:51:22,895 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:51:22,967 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:51:22,969 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:51:23,252 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 136.09it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 34%|███▎      | 28/83 [00:00<00:00, 138.26it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:51:23,480 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:51:23,482 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 51%|█████     | 42/83 [00:00<00:00, 138.02it/s] 67%|██████▋   | 56/83 [00:00<00:00, 137.91it/s]2024-06-04:08:51:23,687 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 84%|████████▍ | 70/83 [00:00<00:00, 137.75it/s] 24%|██▍       | 20/83 [00:00<00:00, 194.74it/s]100%|██████████| 83/83 [00:00<00:00, 137.68it/s]
 48%|████▊     | 40/83 [00:00<00:00, 180.55it/s] 71%|███████   | 59/83 [00:00<00:00, 156.96it/s] 92%|█████████▏| 76/83 [00:00<00:00, 145.69it/s]100%|██████████| 83/83 [00:00<00:00, 151.69it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:51:45,013 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:51:45,015 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:51:45,765 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 11%|█         | 9/82 [00:00<00:00, 84.97it/s] 24%|██▍       | 20/82 [00:00<00:00, 96.60it/s] 38%|███▊      | 31/82 [00:00<00:00, 100.61it/s] 51%|█████     | 42/82 [00:00<00:00, 102.55it/s] 65%|██████▍   | 53/82 [00:00<00:00, 103.30it/s] 78%|███████▊  | 64/82 [00:00<00:00, 103.89it/s] 91%|█████████▏| 75/82 [00:00<00:00, 104.31it/s]100%|██████████| 82/82 [00:00<00:00, 102.43it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:08:51:47,029 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:51:47,032 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:08:51:47,479 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 104.29it/s] 27%|██▋       | 22/82 [00:00<00:00, 104.73it/s] 40%|████      | 33/82 [00:00<00:00, 105.09it/s] 54%|█████▎    | 44/82 [00:00<00:00, 104.86it/s] 67%|██████▋   | 55/82 [00:00<00:00, 105.07it/s] 80%|████████  | 66/82 [00:00<00:00, 103.99it/s] 94%|█████████▍| 77/82 [00:00<00:00, 104.26it/s]100%|██████████| 82/82 [00:00<00:00, 104.48it/s]
2024-06-04:08:51:59,030 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:51:59,030 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:51:59,030 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:51:59,030 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:51:59,030 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:51:59,030 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:08:51:59,031 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:08:51:59,032 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:12<16:52, 12.35s/it]Running generate_until requests:   2%|▏         | 2/83 [00:26<18:10, 13.46s/it]Running generate_until requests:   4%|▎         | 3/83 [00:35<14:56, 11.21s/it]Running generate_until requests:   5%|▍         | 4/83 [00:40<11:43,  8.91s/it]Running generate_until requests:   6%|▌         | 5/83 [00:49<11:37,  8.95s/it]Running generate_until requests:   7%|▋         | 6/83 [00:56<10:24,  8.12s/it]Running generate_until requests:   8%|▊         | 7/83 [01:05<10:54,  8.62s/it]Running generate_until requests:  10%|▉         | 8/83 [01:13<10:26,  8.35s/it]Running generate_until requests:  11%|█         | 9/83 [01:23<10:59,  8.92s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:29<09:51,  8.10s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:37<09:34,  7.98s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:48<10:35,  8.96s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:53<08:57,  7.68s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:01<08:47,  7.64s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:13<10:20,  9.13s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:26<11:32, 10.33s/it]Running generate_until requests:  20%|██        | 17/83 [02:35<10:57,  9.96s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:43<10:08,  9.36s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:52<09:53,  9.27s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:04<10:35, 10.09s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:11<09:14,  8.95s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:18<08:32,  8.40s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:26<08:20,  8.34s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:33<07:46,  7.90s/it]Running generate_until requests:  30%|███       | 25/83 [03:39<07:15,  7.51s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:47<07:06,  7.48s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:57<07:51,  8.41s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:05<07:21,  8.02s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:11<06:52,  7.64s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:23<07:45,  8.78s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:31<07:23,  8.52s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:39<07:06,  8.36s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:44<06:08,  7.37s/it]Running generate_until requests:  41%|████      | 34/83 [04:56<07:08,  8.74s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:02<06:30,  8.14s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:09<06:03,  7.74s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:17<05:59,  7.82s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:31<07:08,  9.51s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:36<06:08,  8.38s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:43<05:35,  7.79s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:48<04:56,  7.07s/it]Running generate_until requests:  51%|█████     | 42/83 [05:55<04:49,  7.05s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:04<05:03,  7.59s/it]Running generate_until requests:  53%|█████▎    | 44/83 [06:10<04:32,  6.99s/it]Running generate_until requests:  54%|█████▍    | 45/83 [06:17<04:24,  6.96s/it]Running generate_until requests:  55%|█████▌    | 46/83 [06:24<04:17,  6.96s/it]Running generate_until requests:  57%|█████▋    | 47/83 [06:29<03:52,  6.47s/it]Running generate_until requests:  58%|█████▊    | 48/83 [06:37<04:04,  6.99s/it]Running generate_until requests:  59%|█████▉    | 49/83 [06:42<03:33,  6.27s/it]Running generate_until requests:  60%|██████    | 50/83 [06:51<03:54,  7.09s/it]Running generate_until requests:  61%|██████▏   | 51/83 [06:58<03:50,  7.20s/it]Running generate_until requests:  63%|██████▎   | 52/83 [07:05<03:36,  6.99s/it]Running generate_until requests:  64%|██████▍   | 53/83 [07:14<03:48,  7.62s/it]Running generate_until requests:  65%|██████▌   | 54/83 [07:19<03:19,  6.88s/it]Running generate_until requests:  66%|██████▋   | 55/83 [07:24<03:01,  6.50s/it]Running generate_until requests:  67%|██████▋   | 56/83 [07:40<04:11,  9.33s/it]Running generate_until requests:  69%|██████▊   | 57/83 [07:50<04:05,  9.45s/it]Running generate_until requests:  70%|██████▉   | 58/83 [07:58<03:44,  8.98s/it]Running generate_until requests:  71%|███████   | 59/83 [08:04<03:12,  8.00s/it]Running generate_until requests:  72%|███████▏  | 60/83 [08:09<02:47,  7.26s/it]Running generate_until requests:  73%|███████▎  | 61/83 [08:18<02:48,  7.67s/it]Running generate_until requests:  75%|███████▍  | 62/83 [08:24<02:32,  7.25s/it]Running generate_until requests:  76%|███████▌  | 63/83 [08:30<02:17,  6.89s/it]Running generate_until requests:  77%|███████▋  | 64/83 [08:38<02:14,  7.07s/it]Running generate_until requests:  78%|███████▊  | 65/83 [08:44<02:02,  6.81s/it]Running generate_until requests:  80%|███████▉  | 66/83 [08:51<01:56,  6.84s/it]Running generate_until requests:  81%|████████  | 67/83 [08:56<01:39,  6.25s/it]Running generate_until requests:  82%|████████▏ | 68/83 [09:01<01:31,  6.10s/it]Running generate_until requests:  83%|████████▎ | 69/83 [09:11<01:38,  7.01s/it]Running generate_until requests:  84%|████████▍ | 70/83 [09:19<01:38,  7.56s/it]Running generate_until requests:  86%|████████▌ | 71/83 [09:25<01:22,  6.89s/it]Running generate_until requests:  87%|████████▋ | 72/83 [09:32<01:15,  6.89s/it]Running generate_until requests:  88%|████████▊ | 73/83 [09:38<01:06,  6.65s/it]Running generate_until requests:  89%|████████▉ | 74/83 [09:44<00:59,  6.60s/it]Running generate_until requests:  90%|█████████ | 75/83 [09:55<01:01,  7.71s/it]Running generate_until requests:  92%|█████████▏| 76/83 [10:03<00:54,  7.80s/it]Running generate_until requests:  93%|█████████▎| 77/83 [10:08<00:42,  7.03s/it]Running generate_until requests:  94%|█████████▍| 78/83 [10:14<00:33,  6.72s/it]Running generate_until requests:  95%|█████████▌| 79/83 [10:23<00:30,  7.55s/it]Running generate_until requests:  96%|█████████▋| 80/83 [10:30<00:21,  7.27s/it]Running generate_until requests:  98%|█████████▊| 81/83 [10:38<00:15,  7.54s/it]Running generate_until requests:  99%|█████████▉| 82/83 [10:43<00:06,  6.82s/it]Running generate_until requests: 100%|██████████| 83/83 [10:49<00:00,  6.45s/it]Running generate_until requests: 100%|██████████| 83/83 [10:49<00:00,  7.82s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:09:07:08,303 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:07:08,305 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:07:08,349 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:07:08,363 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:07:08,561 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:07:08,571 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:07:08,634 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:07:08,814 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:07:13,401 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:07:13,402 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:07:13,406 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:07:13,406 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:07:13,571 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:07:13,572 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:07:13,576 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:07:13,576 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:07:13,671 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:07:13,672 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:07:13,677 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:07:13,677 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:07:14,025 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:07:14,026 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:07:14,031 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:07:14,031 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:09:07:15,393 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:07:15,395 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:07:15,399 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:07:15,400 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:07:15,450 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:07:15,452 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:07:15,457 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:07:15,457 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:07:15,694 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:07:15,695 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:07:15,699 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:07:15,699 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:07:16,060 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:07:16,062 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:07:16,067 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:07:16,067 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.08s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.06s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.90s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.99s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.00s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.79s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.73s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.77s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.83s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.88s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.59s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.65s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.72s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.29s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.28s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.18s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.14s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.17s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.21s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:08:03,487 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:08:03,489 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:09:08:03,811 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:08:03,862 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:08:03,864 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 18%|█▊        | 15/82 [00:00<00:00, 149.51it/s] 37%|███▋      | 30/82 [00:00<00:00, 138.73it/s] 54%|█████▎    | 44/82 [00:00<00:00, 135.66it/s]2024-06-04:09:08:04,252 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
 71%|███████   | 58/82 [00:00<00:00, 134.07it/s]  0%|          | 0/82 [00:00<?, ?it/s] 88%|████████▊ | 72/82 [00:00<00:00, 132.96it/s] 16%|█▌        | 13/82 [00:00<00:00, 129.45it/s]100%|██████████| 82/82 [00:00<00:00, 134.39it/s]
 35%|███▌      | 29/82 [00:00<00:00, 145.94it/s] 60%|█████▉    | 49/82 [00:00<00:00, 169.10it/s] 84%|████████▍ | 69/82 [00:00<00:00, 179.87it/s]100%|██████████| 82/82 [00:00<00:00, 173.81it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:08:23,547 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:08:23,550 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:08:23,602 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:08:23,603 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:08:23,721 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:09:08:23,767 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 206.48it/s] 25%|██▌       | 21/83 [00:00<00:00, 207.37it/s] 51%|█████     | 42/82 [00:00<00:00, 207.87it/s] 52%|█████▏    | 43/83 [00:00<00:00, 208.90it/s] 77%|███████▋  | 63/82 [00:00<00:00, 207.54it/s] 77%|███████▋  | 64/83 [00:00<00:00, 204.03it/s]100%|██████████| 82/82 [00:00<00:00, 207.48it/s]
100%|██████████| 83/83 [00:00<00:00, 205.98it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:08:41,001 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:08:41,003 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:08:41,241 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 16%|█▌        | 13/83 [00:00<00:00, 125.12it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 35%|███▍      | 29/83 [00:00<00:00, 141.31it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:08:41,529 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:08:41,531 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 53%|█████▎    | 44/83 [00:00<00:00, 144.28it/s] 71%|███████   | 59/83 [00:00<00:00, 143.47it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:08:41,767 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:08:41,770 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:08:41,778 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
 93%|█████████▎| 77/83 [00:00<00:00, 155.22it/s]  0%|          | 0/82 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:09:08:41,819 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
100%|██████████| 83/83 [00:00<00:00, 147.45it/s]
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:08:41,893 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:08:41,895 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 22%|██▏       | 18/82 [00:00<00:00, 174.77it/s]2024-06-04:09:08:41,961 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 44%|████▍     | 36/82 [00:00<00:00, 144.68it/s] 25%|██▌       | 21/83 [00:00<00:00, 203.53it/s] 62%|██████▏   | 51/82 [00:00<00:00, 138.75it/s] 51%|█████     | 42/83 [00:00<00:00, 203.40it/s]2024-06-04:09:08:42,191 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 80%|████████  | 66/82 [00:00<00:00, 136.86it/s] 76%|███████▌  | 63/83 [00:00<00:00, 203.81it/s] 17%|█▋        | 14/83 [00:00<00:00, 134.92it/s] 98%|█████████▊| 80/82 [00:00<00:00, 136.46it/s]100%|██████████| 83/83 [00:00<00:00, 204.15it/s]
100%|██████████| 82/82 [00:00<00:00, 139.73it/s]
 36%|███▌      | 30/83 [00:00<00:00, 149.04it/s] 61%|██████▏   | 51/83 [00:00<00:00, 176.54it/s] 87%|████████▋ | 72/83 [00:00<00:00, 189.07it/s]100%|██████████| 83/83 [00:00<00:00, 181.45it/s]
2024-06-04:09:08:51,911 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:08:51,911 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:08:51,911 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:08:51,911 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:08:51,911 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:08:51,911 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:09:08:51,913 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:08:51,914 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:08<11:58,  8.77s/it]Running generate_until requests:   2%|▏         | 2/83 [00:21<14:56, 11.07s/it]Running generate_until requests:   4%|▎         | 3/83 [00:29<13:01,  9.77s/it]Running generate_until requests:   5%|▍         | 4/83 [00:34<10:05,  7.67s/it]Running generate_until requests:   6%|▌         | 5/83 [00:40<09:11,  7.06s/it]Running generate_until requests:   7%|▋         | 6/83 [00:45<08:16,  6.44s/it]Running generate_until requests:   8%|▊         | 7/83 [00:52<08:32,  6.75s/it]Running generate_until requests:  10%|▉         | 8/83 [00:58<08:13,  6.57s/it]Running generate_until requests:  11%|█         | 9/83 [01:06<08:39,  7.02s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:11<07:44,  6.36s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:18<07:34,  6.31s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:26<08:20,  7.05s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:29<06:49,  5.86s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:35<06:42,  5.84s/it]Running generate_until requests:  18%|█▊        | 15/83 [01:46<08:10,  7.22s/it]Running generate_until requests:  19%|█▉        | 16/83 [01:55<08:56,  8.01s/it]Running generate_until requests:  20%|██        | 17/83 [02:02<08:29,  7.72s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:09<07:52,  7.26s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:16<07:55,  7.42s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:26<08:20,  7.94s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:32<07:41,  7.45s/it]Running generate_until requests:  27%|██▋       | 22/83 [02:38<07:00,  6.89s/it]Running generate_until requests:  28%|██▊       | 23/83 [02:43<06:26,  6.44s/it]Running generate_until requests:  29%|██▉       | 24/83 [02:48<06:03,  6.16s/it]Running generate_until requests:  30%|███       | 25/83 [02:55<06:07,  6.34s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:01<05:52,  6.19s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:07<05:44,  6.16s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:13<05:30,  6.00s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:18<05:12,  5.78s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:27<05:53,  6.66s/it]Running generate_until requests:  37%|███▋      | 31/83 [03:33<05:35,  6.45s/it]Running generate_until requests:  39%|███▊      | 32/83 [03:40<05:37,  6.62s/it]Running generate_until requests:  40%|███▉      | 33/83 [03:44<04:53,  5.87s/it]Running generate_until requests:  41%|████      | 34/83 [03:53<05:29,  6.72s/it]Running generate_until requests:  42%|████▏     | 35/83 [03:58<05:01,  6.28s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:03<04:40,  5.97s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:09<04:36,  6.02s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:19<05:26,  7.26s/it]Running generate_until requests:  47%|████▋     | 39/83 [04:24<04:48,  6.55s/it]Running generate_until requests:  48%|████▊     | 40/83 [04:33<05:05,  7.11s/it]Running generate_until requests:  49%|████▉     | 41/83 [04:37<04:23,  6.26s/it]Running generate_until requests:  51%|█████     | 42/83 [04:42<04:01,  5.88s/it]Running generate_until requests:  52%|█████▏    | 43/83 [04:49<04:06,  6.15s/it]Running generate_until requests:  53%|█████▎    | 44/83 [04:53<03:39,  5.63s/it]Running generate_until requests:  54%|█████▍    | 45/83 [04:59<03:32,  5.60s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:04<03:26,  5.57s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:08<03:04,  5.12s/it]Running generate_until requests:  58%|█████▊    | 48/83 [05:14<03:11,  5.47s/it]Running generate_until requests:  59%|█████▉    | 49/83 [05:18<02:47,  4.93s/it]Running generate_until requests:  60%|██████    | 50/83 [05:25<03:00,  5.48s/it]Running generate_until requests:  61%|██████▏   | 51/83 [05:31<02:58,  5.58s/it]Running generate_until requests:  63%|██████▎   | 52/83 [05:37<03:03,  5.92s/it]Running generate_until requests:  64%|██████▍   | 53/83 [05:44<03:00,  6.03s/it]Running generate_until requests:  65%|██████▌   | 54/83 [05:48<02:39,  5.51s/it]Running generate_until requests:  66%|██████▋   | 55/83 [05:53<02:31,  5.40s/it]Running generate_until requests:  67%|██████▋   | 56/83 [06:06<03:22,  7.49s/it]Running generate_until requests:  69%|██████▊   | 57/83 [06:13<03:12,  7.41s/it]Running generate_until requests:  70%|██████▉   | 58/83 [06:19<02:56,  7.05s/it]Running generate_until requests:  71%|███████   | 59/83 [06:23<02:31,  6.30s/it]Running generate_until requests:  72%|███████▏  | 60/83 [06:28<02:11,  5.73s/it]Running generate_until requests:  73%|███████▎  | 61/83 [06:35<02:12,  6.03s/it]Running generate_until requests:  75%|███████▍  | 62/83 [06:40<01:59,  5.70s/it]Running generate_until requests:  76%|███████▌  | 63/83 [06:44<01:47,  5.36s/it]Running generate_until requests:  77%|███████▋  | 64/83 [06:50<01:44,  5.52s/it]Running generate_until requests:  78%|███████▊  | 65/83 [06:55<01:36,  5.33s/it]Running generate_until requests:  80%|███████▉  | 66/83 [06:59<01:23,  4.92s/it]Running generate_until requests:  81%|████████  | 67/83 [07:03<01:14,  4.65s/it]Running generate_until requests:  82%|████████▏ | 68/83 [07:07<01:09,  4.61s/it]Running generate_until requests:  83%|████████▎ | 69/83 [07:17<01:24,  6.00s/it]Running generate_until requests:  84%|████████▍ | 70/83 [07:23<01:21,  6.23s/it]Running generate_until requests:  86%|████████▌ | 71/83 [07:27<01:04,  5.41s/it]Running generate_until requests:  87%|████████▋ | 72/83 [07:33<01:00,  5.48s/it]Running generate_until requests:  88%|████████▊ | 73/83 [07:37<00:53,  5.30s/it]Running generate_until requests:  89%|████████▉ | 74/83 [07:42<00:46,  5.21s/it]Running generate_until requests:  90%|█████████ | 75/83 [07:50<00:48,  6.02s/it]Running generate_until requests:  92%|█████████▏| 76/83 [07:58<00:44,  6.39s/it]Running generate_until requests:  93%|█████████▎| 77/83 [08:02<00:34,  5.69s/it]Running generate_until requests:  94%|█████████▍| 78/83 [08:06<00:25,  5.16s/it]Running generate_until requests:  95%|█████████▌| 79/83 [08:13<00:23,  5.83s/it]Running generate_until requests:  96%|█████████▋| 80/83 [08:18<00:16,  5.61s/it]Running generate_until requests:  98%|█████████▊| 81/83 [08:24<00:11,  5.78s/it]Running generate_until requests:  99%|█████████▉| 82/83 [08:28<00:05,  5.30s/it]Running generate_until requests: 100%|██████████| 83/83 [08:33<00:00,  5.00s/it]Running generate_until requests: 100%|██████████| 83/83 [08:33<00:00,  6.18s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:09:23:47,651 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:23:47,873 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:23:47,978 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:23:48,149 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:23:48,275 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:23:48,306 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:23:48,552 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:23:48,638 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:09:23:53,425 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:23:53,426 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:23:53,431 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:23:53,431 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:23:54,049 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:23:54,050 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:23:54,054 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:23:54,054 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:23:54,059 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:23:54,061 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:23:54,066 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:23:54,066 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:23:54,109 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:23:54,110 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:23:54,113 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:23:54,114 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:23:54,121 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:23:54,122 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:23:54,126 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:23:54,126 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:23:54,179 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:23:54,180 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:23:54,185 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:23:54,185 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:23:54,552 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:23:54,553 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:23:54,558 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:23:54,558 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:09:23:54,733 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:09:23:54,734 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:09:23:54,739 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:09:23:54,739 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.05s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.05s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.76s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.82s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.98s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.95s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.60s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.59s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.17s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.20s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.25s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.20s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.15s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.22s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:24:42,771 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:24:42,773 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:24:43,059 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 140.52it/s] 36%|███▌      | 30/83 [00:00<00:00, 142.25it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 54%|█████▍    | 45/83 [00:00<00:00, 142.88it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:24:43,443 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:24:43,445 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 72%|███████▏  | 60/83 [00:00<00:00, 141.97it/s] 90%|█████████ | 75/83 [00:00<00:00, 142.32it/s]2024-06-04:09:24:43,652 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
100%|██████████| 83/83 [00:00<00:00, 142.16it/s]
  0%|          | 0/82 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 196.80it/s] 49%|████▉     | 40/82 [00:00<00:00, 197.84it/s] 73%|███████▎  | 60/82 [00:00<00:00, 198.07it/s] 98%|█████████▊| 80/82 [00:00<00:00, 197.25it/s]100%|██████████| 82/82 [00:00<00:00, 197.37it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:09:25:03,014 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:25:03,086 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:25:03,089 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:25:03,118 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:25:03,122 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:25:03,262 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:09:25:03,284 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 201.51it/s] 26%|██▌       | 21/82 [00:00<00:00, 203.72it/s] 51%|█████     | 42/83 [00:00<00:00, 202.66it/s] 51%|█████     | 42/82 [00:00<00:00, 204.41it/s] 76%|███████▌  | 63/83 [00:00<00:00, 202.83it/s] 77%|███████▋  | 63/82 [00:00<00:00, 204.91it/s]100%|██████████| 83/83 [00:00<00:00, 202.61it/s]
100%|██████████| 82/82 [00:00<00:00, 202.64it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:25:06,035 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:25:06,038 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:25:06,117 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:25:06,119 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:25:06,475 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
2024-06-04:09:25:06,483 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/83 [00:00<?, ?it/s]  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 105.64it/s] 17%|█▋        | 14/82 [00:00<00:00, 134.76it/s] 27%|██▋       | 22/83 [00:00<00:00, 105.92it/s] 34%|███▍      | 28/82 [00:00<00:00, 135.62it/s] 40%|███▉      | 33/83 [00:00<00:00, 106.46it/s] 51%|█████     | 42/82 [00:00<00:00, 135.83it/s] 53%|█████▎    | 44/83 [00:00<00:00, 106.47it/s] 68%|██████▊   | 56/82 [00:00<00:00, 135.93it/s] 66%|██████▋   | 55/83 [00:00<00:00, 106.45it/s] 85%|████████▌ | 70/82 [00:00<00:00, 135.76it/s]100%|██████████| 82/82 [00:00<00:00, 135.75it/s]
 80%|███████▉  | 66/83 [00:00<00:00, 106.49it/s] 93%|█████████▎| 77/83 [00:00<00:00, 106.43it/s]100%|██████████| 83/83 [00:00<00:00, 106.40it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:25:22,500 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:25:22,502 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:25:22,859 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/83 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:09:25:22,950 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:09:25:22,952 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 17%|█▋        | 14/83 [00:00<00:00, 129.49it/s] 33%|███▎      | 27/83 [00:00<00:00, 129.69it/s] 49%|████▉     | 41/83 [00:00<00:00, 130.11it/s] 66%|██████▋   | 55/83 [00:00<00:00, 130.27it/s]2024-06-04:09:25:23,386 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
 83%|████████▎ | 69/83 [00:00<00:00, 130.33it/s]  0%|          | 0/82 [00:00<?, ?it/s]100%|██████████| 83/83 [00:00<00:00, 130.48it/s]100%|██████████| 83/83 [00:00<00:00, 130.23it/s]
 13%|█▎        | 11/82 [00:00<00:00, 105.32it/s] 27%|██▋       | 22/82 [00:00<00:00, 105.51it/s] 40%|████      | 33/82 [00:00<00:00, 105.79it/s] 54%|█████▎    | 44/82 [00:00<00:00, 105.96it/s] 67%|██████▋   | 55/82 [00:00<00:00, 105.83it/s] 80%|████████  | 66/82 [00:00<00:00, 105.85it/s] 94%|█████████▍| 77/82 [00:00<00:00, 105.99it/s]100%|██████████| 82/82 [00:00<00:00, 105.83it/s]
2024-06-04:09:25:34,993 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:25:34,993 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:25:34,993 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:25:34,994 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:25:34,994 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:25:34,994 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:09:25:34,994 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:09:25:34,995 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:08<12:03,  8.82s/it]Running generate_until requests:   2%|▏         | 2/83 [00:20<14:29, 10.74s/it]Running generate_until requests:   4%|▎         | 3/83 [00:27<11:53,  8.91s/it]Running generate_until requests:   5%|▍         | 4/83 [00:32<09:25,  7.16s/it]Running generate_until requests:   6%|▌         | 5/83 [00:38<08:46,  6.75s/it]Running generate_until requests:   7%|▋         | 6/83 [00:43<07:51,  6.12s/it]Running generate_until requests:   8%|▊         | 7/83 [00:52<09:01,  7.13s/it]Running generate_until requests:  10%|▉         | 8/83 [00:58<08:32,  6.83s/it]Running generate_until requests:  11%|█         | 9/83 [01:06<08:56,  7.25s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:11<07:54,  6.50s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:17<07:33,  6.30s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:26<08:20,  7.05s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:31<07:37,  6.54s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:37<07:16,  6.33s/it]Running generate_until requests:  18%|█▊        | 15/83 [01:46<08:20,  7.36s/it]Running generate_until requests:  19%|█▉        | 16/83 [01:57<09:09,  8.20s/it]Running generate_until requests:  20%|██        | 17/83 [02:04<08:39,  7.87s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:10<08:01,  7.41s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:16<07:29,  7.03s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:26<08:15,  7.87s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:30<06:53,  6.67s/it]Running generate_until requests:  27%|██▋       | 22/83 [02:36<06:27,  6.35s/it]Running generate_until requests:  28%|██▊       | 23/83 [02:42<06:21,  6.36s/it]Running generate_until requests:  29%|██▉       | 24/83 [02:47<06:01,  6.13s/it]Running generate_until requests:  30%|███       | 25/83 [02:55<06:23,  6.61s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:01<06:03,  6.38s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:07<05:56,  6.37s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:13<05:37,  6.14s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:19<05:34,  6.19s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:27<05:46,  6.53s/it]Running generate_until requests:  37%|███▋      | 31/83 [03:32<05:21,  6.18s/it]Running generate_until requests:  39%|███▊      | 32/83 [03:39<05:28,  6.44s/it]Running generate_until requests:  40%|███▉      | 33/83 [03:43<04:47,  5.75s/it]Running generate_until requests:  41%|████      | 34/83 [03:50<05:00,  6.14s/it]Running generate_until requests:  42%|████▏     | 35/83 [03:56<04:42,  5.88s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:01<04:36,  5.89s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:07<04:30,  5.87s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:15<04:51,  6.49s/it]Running generate_until requests:  47%|████▋     | 39/83 [04:20<04:28,  6.10s/it]Running generate_until requests:  48%|████▊     | 40/83 [04:29<04:52,  6.81s/it]Running generate_until requests:  49%|████▉     | 41/83 [04:33<04:14,  6.06s/it]Running generate_until requests:  51%|█████     | 42/83 [04:39<04:06,  6.02s/it]Running generate_until requests:  52%|█████▏    | 43/83 [04:46<04:10,  6.26s/it]Running generate_until requests:  53%|█████▎    | 44/83 [04:50<03:42,  5.71s/it]Running generate_until requests:  54%|█████▍    | 45/83 [04:56<03:33,  5.63s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:01<03:25,  5.54s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:05<03:04,  5.12s/it]Running generate_until requests:  58%|█████▊    | 48/83 [05:12<03:12,  5.50s/it]Running generate_until requests:  59%|█████▉    | 49/83 [05:15<02:48,  4.96s/it]Running generate_until requests:  60%|██████    | 50/83 [05:22<03:04,  5.59s/it]Running generate_until requests:  61%|██████▏   | 51/83 [05:28<02:56,  5.50s/it]Running generate_until requests:  63%|██████▎   | 52/83 [05:32<02:36,  5.05s/it]Running generate_until requests:  64%|██████▍   | 53/83 [05:38<02:38,  5.29s/it]Running generate_until requests:  65%|██████▌   | 54/83 [05:42<02:24,  4.99s/it]Running generate_until requests:  66%|██████▋   | 55/83 [05:46<02:16,  4.86s/it]Running generate_until requests:  67%|██████▋   | 56/83 [05:58<03:08,  7.00s/it]Running generate_until requests:  69%|██████▊   | 57/83 [06:07<03:12,  7.41s/it]Running generate_until requests:  70%|██████▉   | 58/83 [06:13<02:56,  7.06s/it]Running generate_until requests:  71%|███████   | 59/83 [06:18<02:31,  6.31s/it]Running generate_until requests:  72%|███████▏  | 60/83 [06:22<02:12,  5.74s/it]Running generate_until requests:  73%|███████▎  | 61/83 [06:29<02:13,  6.06s/it]Running generate_until requests:  75%|███████▍  | 62/83 [06:33<01:55,  5.48s/it]Running generate_until requests:  76%|███████▌  | 63/83 [06:38<01:45,  5.29s/it]Running generate_until requests:  77%|███████▋  | 64/83 [06:45<01:52,  5.94s/it]Running generate_until requests:  78%|███████▊  | 65/83 [06:50<01:41,  5.63s/it]Running generate_until requests:  80%|███████▉  | 66/83 [06:54<01:27,  5.13s/it]Running generate_until requests:  81%|████████  | 67/83 [06:58<01:16,  4.81s/it]Running generate_until requests:  82%|████████▏ | 68/83 [07:03<01:10,  4.72s/it]Running generate_until requests:  83%|████████▎ | 69/83 [07:12<01:27,  6.22s/it]Running generate_until requests:  84%|████████▍ | 70/83 [07:19<01:23,  6.39s/it]Running generate_until requests:  86%|████████▌ | 71/83 [07:23<01:06,  5.53s/it]Running generate_until requests:  87%|████████▋ | 72/83 [07:26<00:54,  4.96s/it]Running generate_until requests:  88%|████████▊ | 73/83 [07:31<00:49,  4.96s/it]Running generate_until requests:  89%|████████▉ | 74/83 [07:36<00:44,  4.90s/it]Running generate_until requests:  90%|█████████ | 75/83 [07:44<00:46,  5.78s/it]Running generate_until requests:  92%|█████████▏| 76/83 [07:50<00:40,  5.79s/it]Running generate_until requests:  93%|█████████▎| 77/83 [07:54<00:31,  5.33s/it]Running generate_until requests:  94%|█████████▍| 78/83 [07:59<00:25,  5.16s/it]Running generate_until requests:  95%|█████████▌| 79/83 [08:06<00:22,  5.69s/it]Running generate_until requests:  96%|█████████▋| 80/83 [08:11<00:16,  5.52s/it]Running generate_until requests:  98%|█████████▊| 81/83 [08:17<00:11,  5.72s/it]Running generate_until requests:  99%|█████████▉| 82/83 [08:22<00:05,  5.41s/it]Running generate_until requests: 100%|██████████| 83/83 [08:26<00:00,  5.08s/it]Running generate_until requests: 100%|██████████| 83/83 [08:26<00:00,  6.10s/it]
