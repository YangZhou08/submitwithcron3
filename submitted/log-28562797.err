Already on 'yangexp2'
Your configuration specifies to merge with the ref 'refs/heads/yangexp2'
from the remote, but no such ref was fetched.
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/huggingface-cli", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py", line 51, in main
    service.run()
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/user.py", line 98, in run
    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 111, in login
    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 307, in _login
    raise ValueError("Invalid token passed!")
ValueError: Invalid token passed!
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:01:30:56,466 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:30:56,507 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:30:56,835 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:30:56,912 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:30:57,247 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:30:57,250 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:30:57,280 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:30:57,321 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:31:02,344 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:02,345 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:02,351 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:02,351 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
2024-06-04:01:31:02,561 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:02,563 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:02,570 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:02,570 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
2024-06-04:01:31:03,364 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:03,365 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:03,372 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:03,372 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:01:31:03,783 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:03,784 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:03,788 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:03,788 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:01:31:03,835 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:03,837 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:03,843 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:03,843 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
2024-06-04:01:31:04,168 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:04,169 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:04,173 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:04,174 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
2024-06-04:01:31:04,191 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:04,193 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:04,198 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:04,198 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
2024-06-04:01:31:04,306 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:31:04,308 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:31:04,312 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:31:04,312 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.1, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:33<01:41, 33.75s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:35<01:46, 35.50s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:35<01:46, 35.53s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:33<01:41, 33.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:33<01:41, 33.82s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:33<01:41, 33.83s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:34<01:43, 34.58s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:34<01:44, 34.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:06<01:05, 32.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:08<01:08, 34.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:07<01:07, 33.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:08<01:08, 34.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:07<01:07, 33.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:07<01:07, 33.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:08<01:07, 33.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:07<01:07, 33.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:36<00:31, 31.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:39<00:32, 32.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:38<00:32, 32.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:38<00:32, 32.40s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:37<00:32, 32.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:38<00:32, 32.45s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:39<00:32, 32.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:38<00:32, 32.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:44<00:00, 21.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:44<00:00, 26.20s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:43<00:00, 21.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:43<00:00, 25.83s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:43<00:00, 21.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:43<00:00, 25.83s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:43<00:00, 21.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:43<00:00, 25.84s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:45<00:00, 21.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:45<00:00, 26.28s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:44<00:00, 21.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:44<00:00, 26.02s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:44<00:00, 21.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:44<00:00, 26.01s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:44<00:00, 21.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:44<00:00, 26.08s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:29,710 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:29,713 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:30,017 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 141.75it/s] 36%|███▌      | 30/83 [00:00<00:00, 142.66it/s] 54%|█████▍    | 45/83 [00:00<00:00, 142.85it/s] 72%|███████▏  | 60/83 [00:00<00:00, 139.69it/s] 89%|████████▉ | 74/83 [00:00<00:00, 138.17it/s]100%|██████████| 83/83 [00:00<00:00, 139.96it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:31,875 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:31,876 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:32,087 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 193.80it/s] 49%|████▉     | 40/82 [00:00<00:00, 194.82it/s] 73%|███████▎  | 60/82 [00:00<00:00, 195.00it/s] 98%|█████████▊| 80/82 [00:00<00:00, 195.36it/s]100%|██████████| 82/82 [00:00<00:00, 195.08it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:48,287 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:48,289 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:01:33:48,465 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:48,531 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:48,532 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 24%|██▍       | 20/82 [00:00<00:00, 198.04it/s] 50%|█████     | 41/82 [00:00<00:00, 200.55it/s]2024-06-04:01:33:48,698 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 76%|███████▌  | 62/82 [00:00<00:00, 202.07it/s] 24%|██▍       | 20/82 [00:00<00:00, 196.58it/s]100%|██████████| 82/82 [00:00<00:00, 202.70it/s]
 50%|█████     | 41/82 [00:00<00:00, 200.88it/s] 76%|███████▌  | 62/82 [00:00<00:00, 203.61it/s]100%|██████████| 82/82 [00:00<00:00, 203.45it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:52,234 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:52,236 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:52,411 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 196.61it/s] 49%|████▉     | 40/82 [00:00<00:00, 196.86it/s] 73%|███████▎  | 60/82 [00:00<00:00, 197.33it/s] 99%|█████████▉| 81/82 [00:00<00:00, 198.70it/s]100%|██████████| 82/82 [00:00<00:00, 197.98it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:33:53,233 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:53,235 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:33:53,442 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 202.02it/s] 51%|█████     | 42/83 [00:00<00:00, 203.60it/s] 76%|███████▌  | 63/83 [00:00<00:00, 204.02it/s]100%|██████████| 83/83 [00:00<00:00, 204.11it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:01:34:04,045 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:34:04,120 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:04,124 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:01:34:04,183 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:04,185 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:01:34:04,300 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:01:34:04,377 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 207.17it/s] 25%|██▌       | 21/83 [00:00<00:00, 204.96it/s] 51%|█████     | 42/83 [00:00<00:00, 208.45it/s] 51%|█████     | 42/83 [00:00<00:00, 206.03it/s] 76%|███████▌  | 63/83 [00:00<00:00, 208.93it/s] 76%|███████▌  | 63/83 [00:00<00:00, 206.52it/s]100%|██████████| 83/83 [00:00<00:00, 208.88it/s]
100%|██████████| 83/83 [00:00<00:00, 182.00it/s]
2024-06-04:01:34:16,301 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:34:16,301 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:34:16,301 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:34:16,301 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:34:16,302 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:34:16,302 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:01:34:16,303 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:01:34:16,304 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:20<27:42, 20.27s/it]Running generate_until requests:   2%|▏         | 2/83 [00:38<25:52, 19.17s/it]Running generate_until requests:   4%|▎         | 3/83 [00:57<25:04, 18.80s/it]Running generate_until requests:   5%|▍         | 4/83 [01:15<24:27, 18.57s/it]Running generate_until requests:   6%|▌         | 5/83 [01:33<23:56, 18.42s/it]Running generate_until requests:   7%|▋         | 6/83 [01:51<23:30, 18.31s/it]Running generate_until requests:   8%|▊         | 7/83 [02:09<23:10, 18.30s/it]Running generate_until requests:  10%|▉         | 8/83 [02:27<22:48, 18.24s/it]Running generate_until requests:  11%|█         | 9/83 [02:46<22:28, 18.22s/it]Running generate_until requests:  12%|█▏        | 10/83 [03:04<22:08, 18.20s/it]Running generate_until requests:  13%|█▎        | 11/83 [03:22<21:49, 18.19s/it]Running generate_until requests:  14%|█▍        | 12/83 [03:40<21:29, 18.16s/it]Running generate_until requests:  16%|█▌        | 13/83 [03:58<21:11, 18.16s/it]Running generate_until requests:  17%|█▋        | 14/83 [04:16<20:52, 18.15s/it]Running generate_until requests:  18%|█▊        | 15/83 [04:34<20:30, 18.10s/it]Running generate_until requests:  19%|█▉        | 16/83 [04:52<20:12, 18.10s/it]Running generate_until requests:  20%|██        | 17/83 [05:10<19:53, 18.09s/it]Running generate_until requests:  22%|██▏       | 18/83 [05:28<19:34, 18.07s/it]Running generate_until requests:  23%|██▎       | 19/83 [05:47<19:17, 18.09s/it]Running generate_until requests:  24%|██▍       | 20/83 [06:05<18:59, 18.09s/it]Running generate_until requests:  25%|██▌       | 21/83 [06:23<18:38, 18.04s/it]Running generate_until requests:  27%|██▋       | 22/83 [06:41<18:18, 18.00s/it]Running generate_until requests:  28%|██▊       | 23/83 [06:58<17:58, 17.97s/it]Running generate_until requests:  29%|██▉       | 24/83 [07:16<17:38, 17.95s/it]Running generate_until requests:  30%|███       | 25/83 [07:34<17:20, 17.94s/it]Running generate_until requests:  31%|███▏      | 26/83 [07:52<17:01, 17.92s/it]Running generate_until requests:  33%|███▎      | 27/83 [08:10<16:43, 17.91s/it]Running generate_until requests:  34%|███▎      | 28/83 [08:28<16:24, 17.90s/it]Running generate_until requests:  35%|███▍      | 29/83 [08:46<16:06, 17.90s/it]Running generate_until requests:  36%|███▌      | 30/83 [09:04<15:48, 17.90s/it]Running generate_until requests:  37%|███▋      | 31/83 [09:22<15:30, 17.89s/it]Running generate_until requests:  39%|███▊      | 32/83 [09:39<15:12, 17.89s/it]Running generate_until requests:  40%|███▉      | 33/83 [09:57<14:54, 17.89s/it]Running generate_until requests:  41%|████      | 34/83 [10:15<14:36, 17.90s/it]Running generate_until requests:  42%|████▏     | 35/83 [10:33<14:19, 17.90s/it]Running generate_until requests:  43%|████▎     | 36/83 [10:51<14:01, 17.91s/it]Running generate_until requests:  45%|████▍     | 37/83 [11:03<12:16, 16.00s/it]Running generate_until requests:  46%|████▌     | 38/83 [11:20<12:24, 16.54s/it]Running generate_until requests:  47%|████▋     | 39/83 [11:38<12:25, 16.94s/it]Running generate_until requests:  48%|████▊     | 40/83 [11:56<12:20, 17.22s/it]Running generate_until requests:  49%|████▉     | 41/83 [12:14<12:11, 17.41s/it]Running generate_until requests:  51%|█████     | 42/83 [12:32<12:00, 17.58s/it]Running generate_until requests:  52%|█████▏    | 43/83 [12:50<11:46, 17.66s/it]Running generate_until requests:  53%|█████▎    | 44/83 [13:08<11:31, 17.73s/it]Running generate_until requests:  54%|█████▍    | 45/83 [13:26<11:14, 17.76s/it]Running generate_until requests:  55%|█████▌    | 46/83 [13:43<10:57, 17.78s/it]Running generate_until requests:  57%|█████▋    | 47/83 [14:01<10:41, 17.81s/it]Running generate_until requests:  58%|█████▊    | 48/83 [14:19<10:24, 17.84s/it]Running generate_until requests:  59%|█████▉    | 49/83 [14:37<10:07, 17.85s/it]Running generate_until requests:  60%|██████    | 50/83 [14:55<09:49, 17.85s/it]Running generate_until requests:  61%|██████▏   | 51/83 [15:13<09:30, 17.84s/it]Running generate_until requests:  63%|██████▎   | 52/83 [15:31<09:13, 17.85s/it]Running generate_until requests:  64%|██████▍   | 53/83 [15:48<08:55, 17.84s/it]Running generate_until requests:  65%|██████▌   | 54/83 [16:06<08:37, 17.84s/it]Running generate_until requests:  66%|██████▋   | 55/83 [16:24<08:18, 17.82s/it]Running generate_until requests:  67%|██████▋   | 56/83 [16:42<08:00, 17.81s/it]Running generate_until requests:  69%|██████▊   | 57/83 [17:00<07:43, 17.81s/it]Running generate_until requests:  70%|██████▉   | 58/83 [17:17<07:25, 17.82s/it]Running generate_until requests:  71%|███████   | 59/83 [17:35<07:07, 17.81s/it]Running generate_until requests:  72%|███████▏  | 60/83 [17:53<06:49, 17.81s/it]Running generate_until requests:  73%|███████▎  | 61/83 [18:11<06:31, 17.81s/it]Running generate_until requests:  75%|███████▍  | 62/83 [18:29<06:13, 17.80s/it]Running generate_until requests:  76%|███████▌  | 63/83 [18:46<05:55, 17.80s/it]Running generate_until requests:  77%|███████▋  | 64/83 [19:04<05:38, 17.80s/it]Running generate_until requests:  78%|███████▊  | 65/83 [19:22<05:20, 17.80s/it]Running generate_until requests:  80%|███████▉  | 66/83 [19:40<05:02, 17.79s/it]Running generate_until requests:  81%|████████  | 67/83 [19:58<04:44, 17.78s/it]Running generate_until requests:  82%|████████▏ | 68/83 [20:15<04:26, 17.76s/it]Running generate_until requests:  83%|████████▎ | 69/83 [20:33<04:08, 17.73s/it]Running generate_until requests:  84%|████████▍ | 70/83 [20:51<03:50, 17.73s/it]Running generate_until requests:  86%|████████▌ | 71/83 [21:08<03:32, 17.72s/it]Running generate_until requests:  87%|████████▋ | 72/83 [21:26<03:14, 17.71s/it]Running generate_until requests:  88%|████████▊ | 73/83 [21:44<02:57, 17.71s/it]Running generate_until requests:  89%|████████▉ | 74/83 [22:01<02:39, 17.71s/it]Running generate_until requests:  90%|█████████ | 75/83 [22:19<02:21, 17.72s/it]Running generate_until requests:  92%|█████████▏| 76/83 [22:37<02:04, 17.72s/it]Running generate_until requests:  93%|█████████▎| 77/83 [22:55<01:46, 17.69s/it]Running generate_until requests:  94%|█████████▍| 78/83 [23:12<01:28, 17.67s/it]Running generate_until requests:  95%|█████████▌| 79/83 [23:30<01:10, 17.64s/it]Running generate_until requests:  96%|█████████▋| 80/83 [23:47<00:52, 17.63s/it]Running generate_until requests:  98%|█████████▊| 81/83 [24:05<00:35, 17.64s/it]Running generate_until requests:  99%|█████████▉| 82/83 [24:22<00:17, 17.41s/it]Running generate_until requests: 100%|██████████| 83/83 [24:37<00:00, 16.60s/it]Running generate_until requests: 100%|██████████| 83/83 [24:37<00:00, 17.80s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:01:59:51,497 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:51,497 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:51,727 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:51,825 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:51,866 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:51,985 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:52,012 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:52,287 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:01:59:56,764 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:56,766 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:56,771 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:56,771 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
2024-06-04:01:59:56,901 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:56,902 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:56,907 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:56,908 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
2024-06-04:01:59:57,029 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:57,030 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:57,034 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:57,034 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
2024-06-04:01:59:58,314 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:58,315 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:58,320 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:58,320 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:01:59:58,377 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:58,378 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:58,383 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:58,383 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:01:59:58,448 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:58,449 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:58,453 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:58,453 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:01:59:58,540 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:58,542 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:58,547 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:58,547 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
2024-06-04:01:59:58,704 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:01:59:58,706 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:01:59:58,712 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:01:59:58,712 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.2, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.12s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.38s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.57s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.06s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.07s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.97s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.94s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.19s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.89s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.95s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.74s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.73s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.16s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.21s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.00s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.21s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.29s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.28s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:00:47,863 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:00:47,865 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:00:48,021 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:00:48,024 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:00:48,246 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:02:00:48,317 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 131.85it/s] 17%|█▋        | 14/82 [00:00<00:00, 130.21it/s] 34%|███▎      | 28/83 [00:00<00:00, 132.38it/s] 34%|███▍      | 28/82 [00:00<00:00, 133.81it/s] 51%|█████     | 42/83 [00:00<00:00, 132.63it/s] 51%|█████     | 42/82 [00:00<00:00, 136.14it/s] 67%|██████▋   | 56/83 [00:00<00:00, 132.88it/s] 70%|██████▉   | 57/82 [00:00<00:00, 138.05it/s] 84%|████████▍ | 70/83 [00:00<00:00, 133.02it/s] 87%|████████▋ | 71/82 [00:00<00:00, 138.53it/s]100%|██████████| 83/83 [00:00<00:00, 132.91it/s]
100%|██████████| 82/82 [00:00<00:00, 137.73it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:01:07,396 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:07,398 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:07,565 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/83 [00:00<00:00, 198.29it/s] 48%|████▊     | 40/83 [00:00<00:00, 197.60it/s] 73%|███████▎  | 61/83 [00:00<00:00, 200.35it/s] 99%|█████████▉| 82/83 [00:00<00:00, 201.57it/s]100%|██████████| 83/83 [00:00<00:00, 200.63it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:01:08,682 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:08,684 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:08,859 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 207.59it/s] 51%|█████     | 42/82 [00:00<00:00, 208.36it/s] 77%|███████▋  | 63/82 [00:00<00:00, 208.41it/s]100%|██████████| 82/82 [00:00<00:00, 208.41it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:01:09,539 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:09,541 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:09,711 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 198.24it/s] 50%|█████     | 41/82 [00:00<00:00, 199.38it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 76%|███████▌  | 62/82 [00:00<00:00, 200.64it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:01:10,086 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:10,088 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
100%|██████████| 82/82 [00:00<00:00, 201.10it/s]
2024-06-04:02:01:10,294 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 200.98it/s] 51%|█████     | 42/83 [00:00<00:00, 203.02it/s] 76%|███████▌  | 63/83 [00:00<00:00, 202.91it/s]100%|██████████| 83/83 [00:00<00:00, 202.88it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:01:20,465 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:20,467 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:20,640 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 203.96it/s] 51%|█████     | 42/82 [00:00<00:00, 204.94it/s] 77%|███████▋  | 63/82 [00:00<00:00, 205.21it/s]100%|██████████| 82/82 [00:00<00:00, 205.16it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:01:21,300 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:01:21,372 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:21,376 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:01:21,555 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 207.55it/s] 51%|█████     | 42/83 [00:00<00:00, 208.18it/s] 76%|███████▌  | 63/83 [00:00<00:00, 207.93it/s]100%|██████████| 83/83 [00:00<00:00, 208.27it/s]
2024-06-04:02:01:33,174 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:01:33,174 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:01:33,174 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:01:33,174 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:01:33,174 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:01:33,174 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:02:01:33,175 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:01:33,176 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:23<31:53, 23.33s/it]Running generate_until requests:   2%|▏         | 2/83 [00:39<26:02, 19.29s/it]Running generate_until requests:   4%|▎         | 3/83 [00:56<23:51, 17.89s/it]Running generate_until requests:   5%|▍         | 4/83 [01:12<22:39, 17.21s/it]Running generate_until requests:   6%|▌         | 5/83 [01:28<21:52, 16.83s/it]Running generate_until requests:   7%|▋         | 6/83 [01:44<21:15, 16.57s/it]Running generate_until requests:   8%|▊         | 7/83 [02:00<20:45, 16.39s/it]Running generate_until requests:  10%|▉         | 8/83 [02:16<20:21, 16.29s/it]Running generate_until requests:  11%|█         | 9/83 [02:32<19:59, 16.21s/it]Running generate_until requests:  12%|█▏        | 10/83 [02:40<16:44, 13.76s/it]Running generate_until requests:  13%|█▎        | 11/83 [02:56<17:22, 14.48s/it]Running generate_until requests:  14%|█▍        | 12/83 [03:12<17:41, 14.95s/it]Running generate_until requests:  16%|█▌        | 13/83 [03:28<17:48, 15.27s/it]Running generate_until requests:  17%|█▋        | 14/83 [03:44<17:47, 15.48s/it]Running generate_until requests:  18%|█▊        | 15/83 [03:54<15:36, 13.78s/it]Running generate_until requests:  19%|█▉        | 16/83 [04:10<16:06, 14.43s/it]Running generate_until requests:  20%|██        | 17/83 [04:26<16:22, 14.88s/it]Running generate_until requests:  22%|██▏       | 18/83 [04:42<16:28, 15.20s/it]Running generate_until requests:  23%|██▎       | 19/83 [04:58<16:28, 15.44s/it]Running generate_until requests:  24%|██▍       | 20/83 [05:14<16:22, 15.60s/it]Running generate_until requests:  25%|██▌       | 21/83 [05:30<16:10, 15.65s/it]Running generate_until requests:  27%|██▋       | 22/83 [05:46<15:56, 15.68s/it]Running generate_until requests:  28%|██▊       | 23/83 [06:01<15:41, 15.70s/it]Running generate_until requests:  29%|██▉       | 24/83 [06:17<15:27, 15.71s/it]Running generate_until requests:  30%|███       | 25/83 [06:33<15:11, 15.72s/it]Running generate_until requests:  31%|███▏      | 26/83 [06:41<12:50, 13.52s/it]Running generate_until requests:  33%|███▎      | 27/83 [06:57<13:14, 14.19s/it]Running generate_until requests:  34%|███▎      | 28/83 [07:13<13:26, 14.66s/it]Running generate_until requests:  35%|███▍      | 29/83 [07:28<13:29, 14.98s/it]Running generate_until requests:  36%|███▌      | 30/83 [07:44<13:26, 15.21s/it]Running generate_until requests:  37%|███▋      | 31/83 [08:00<13:19, 15.37s/it]Running generate_until requests:  39%|███▊      | 32/83 [08:16<13:09, 15.48s/it]Running generate_until requests:  40%|███▉      | 33/83 [08:31<12:58, 15.57s/it]Running generate_until requests:  41%|████      | 34/83 [08:47<12:45, 15.63s/it]Running generate_until requests:  42%|████▏     | 35/83 [09:03<12:31, 15.66s/it]Running generate_until requests:  43%|████▎     | 36/83 [09:19<12:17, 15.69s/it]Running generate_until requests:  45%|████▍     | 37/83 [09:29<10:48, 14.11s/it]Running generate_until requests:  46%|████▌     | 38/83 [09:45<10:55, 14.57s/it]Running generate_until requests:  47%|████▋     | 39/83 [09:50<08:39, 11.80s/it]Running generate_until requests:  48%|████▊     | 40/83 [10:06<09:17, 12.96s/it]Running generate_until requests:  49%|████▉     | 41/83 [10:21<09:38, 13.77s/it]Running generate_until requests:  51%|█████     | 42/83 [10:37<09:47, 14.34s/it]Running generate_until requests:  52%|█████▏    | 43/83 [10:53<09:49, 14.74s/it]Running generate_until requests:  53%|█████▎    | 44/83 [10:59<07:58, 12.27s/it]Running generate_until requests:  54%|█████▍    | 45/83 [11:15<08:22, 13.22s/it]Running generate_until requests:  55%|█████▌    | 46/83 [11:30<08:36, 13.95s/it]Running generate_until requests:  57%|█████▋    | 47/83 [11:46<08:40, 14.46s/it]Running generate_until requests:  58%|█████▊    | 48/83 [11:57<07:45, 13.29s/it]Running generate_until requests:  59%|█████▉    | 49/83 [12:12<07:55, 14.00s/it]Running generate_until requests:  60%|██████    | 50/83 [12:28<07:58, 14.49s/it]Running generate_until requests:  61%|██████▏   | 51/83 [12:43<07:54, 14.82s/it]Running generate_until requests:  63%|██████▎   | 52/83 [12:59<07:46, 15.06s/it]Running generate_until requests:  64%|██████▍   | 53/83 [13:15<07:36, 15.23s/it]Running generate_until requests:  65%|██████▌   | 54/83 [13:30<07:25, 15.35s/it]Running generate_until requests:  66%|██████▋   | 55/83 [13:46<07:11, 15.43s/it]Running generate_until requests:  67%|██████▋   | 56/83 [14:01<06:57, 15.48s/it]Running generate_until requests:  69%|██████▊   | 57/83 [14:17<06:43, 15.52s/it]Running generate_until requests:  70%|██████▉   | 58/83 [14:27<05:49, 13.98s/it]Running generate_until requests:  71%|███████   | 59/83 [14:43<05:47, 14.47s/it]Running generate_until requests:  72%|███████▏  | 60/83 [14:59<05:41, 14.83s/it]Running generate_until requests:  73%|███████▎  | 61/83 [15:14<05:31, 15.06s/it]Running generate_until requests:  75%|███████▍  | 62/83 [15:30<05:19, 15.23s/it]Running generate_until requests:  76%|███████▌  | 63/83 [15:46<05:06, 15.34s/it]Running generate_until requests:  77%|███████▋  | 64/83 [16:01<04:52, 15.42s/it]Running generate_until requests:  78%|███████▊  | 65/83 [16:17<04:38, 15.47s/it]Running generate_until requests:  80%|███████▉  | 66/83 [16:31<04:17, 15.18s/it]Running generate_until requests:  81%|████████  | 67/83 [16:47<04:04, 15.30s/it]Running generate_until requests:  82%|████████▏ | 68/83 [17:02<03:50, 15.37s/it]Running generate_until requests:  83%|████████▎ | 69/83 [17:18<03:35, 15.42s/it]Running generate_until requests:  84%|████████▍ | 70/83 [17:33<03:20, 15.45s/it]Running generate_until requests:  86%|████████▌ | 71/83 [17:49<03:05, 15.48s/it]Running generate_until requests:  87%|████████▋ | 72/83 [18:05<02:50, 15.49s/it]Running generate_until requests:  88%|████████▊ | 73/83 [18:20<02:35, 15.50s/it]Running generate_until requests:  89%|████████▉ | 74/83 [18:36<02:19, 15.53s/it]Running generate_until requests:  90%|█████████ | 75/83 [18:51<02:04, 15.53s/it]Running generate_until requests:  92%|█████████▏| 76/83 [19:07<01:48, 15.51s/it]Running generate_until requests:  93%|█████████▎| 77/83 [19:22<01:32, 15.41s/it]Running generate_until requests:  94%|█████████▍| 78/83 [19:37<01:17, 15.43s/it]Running generate_until requests:  95%|█████████▌| 79/83 [19:53<01:01, 15.43s/it]Running generate_until requests:  96%|█████████▋| 80/83 [20:08<00:46, 15.43s/it]Running generate_until requests:  98%|█████████▊| 81/83 [20:24<00:30, 15.43s/it]Running generate_until requests:  99%|█████████▉| 82/83 [20:39<00:15, 15.44s/it]Running generate_until requests: 100%|██████████| 83/83 [20:54<00:00, 15.44s/it]Running generate_until requests: 100%|██████████| 83/83 [20:54<00:00, 15.12s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:02:26:28,635 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:28,636 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:28,727 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:28,727 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:28,746 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:28,753 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:28,757 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:28,780 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:34,273 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:34,275 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:26:34,280 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:34,280 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
2024-06-04:02:26:34,282 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:34,283 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:26:34,287 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:34,287 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
2024-06-04:02:26:35,674 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:35,674 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:35,676 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:26:35,676 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:26:35,681 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:35,681 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:35,681 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
2024-06-04:02:26:35,681 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
2024-06-04:02:26:35,690 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:35,692 INFO     [main.py:378] Selected Tasks: ['gsm8k']
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:26:35,696 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:35,696 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
2024-06-04:02:26:35,713 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:35,714 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:26:35,719 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:35,719 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:26:35,737 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:35,738 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:26:35,742 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:35,743 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
2024-06-04:02:26:35,743 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:35,745 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:26:35,750 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:35,750 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.3, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.06s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.21s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.09s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.06s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.13s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.89s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.68s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.91s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.45s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.82s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:05,  2.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.62s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.01s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.93s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.88s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.27s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.28s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.37s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.28s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:27:23,431 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:23,434 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:23,747 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 140.89it/s] 36%|███▌      | 30/83 [00:00<00:00, 140.96it/s] 54%|█████▍    | 45/83 [00:00<00:00, 141.42it/s] 72%|███████▏  | 60/83 [00:00<00:00, 141.71it/s] 90%|█████████ | 75/83 [00:00<00:00, 142.04it/s]100%|██████████| 83/83 [00:00<00:00, 141.73it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:27:29,443 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:29,445 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:29,658 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/83 [00:00<00:00, 193.97it/s] 48%|████▊     | 40/83 [00:00<00:00, 194.65it/s] 72%|███████▏  | 60/83 [00:00<00:00, 195.32it/s] 96%|█████████▋| 80/83 [00:00<00:00, 195.61it/s]100%|██████████| 83/83 [00:00<00:00, 195.33it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:27:43,286 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:43,289 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:43,467 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 201.08it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 51%|█████     | 42/82 [00:00<00:00, 202.38it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:27:43,764 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:43,765 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 77%|███████▋  | 63/82 [00:00<00:00, 194.00it/s]100%|██████████| 82/82 [00:00<00:00, 175.89it/s]
2024-06-04:02:27:44,006 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 206.33it/s] 51%|█████     | 42/83 [00:00<00:00, 208.00it/s] 76%|███████▌  | 63/83 [00:00<00:00, 208.22it/s]100%|██████████| 83/83 [00:00<00:00, 208.11it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:27:49,203 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:27:49,274 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:49,277 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:27:49,315 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:49,318 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:49,499 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:02:27:49,515 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 196.94it/s] 24%|██▍       | 20/83 [00:00<00:00, 196.68it/s] 49%|████▉     | 40/82 [00:00<00:00, 197.76it/s] 48%|████▊     | 40/83 [00:00<00:00, 197.38it/s] 73%|███████▎  | 60/82 [00:00<00:00, 198.26it/s] 72%|███████▏  | 60/83 [00:00<00:00, 197.78it/s] 98%|█████████▊| 80/82 [00:00<00:00, 198.63it/s]100%|██████████| 82/82 [00:00<00:00, 198.30it/s]
 96%|█████████▋| 80/83 [00:00<00:00, 198.11it/s]100%|██████████| 83/83 [00:00<00:00, 198.25it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:27:59,929 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:59,932 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:28:00,228 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 137.59it/s] 34%|███▍      | 28/82 [00:00<00:00, 138.48it/s] 51%|█████     | 42/82 [00:00<00:00, 138.67it/s] 68%|██████▊   | 56/82 [00:00<00:00, 138.88it/s] 85%|████████▌ | 70/82 [00:00<00:00, 138.92it/s]100%|██████████| 82/82 [00:00<00:00, 138.73it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:28:06,411 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:28:06,413 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:28:06,861 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 104.79it/s] 27%|██▋       | 22/82 [00:00<00:00, 106.09it/s] 40%|████      | 33/82 [00:00<00:00, 106.08it/s] 54%|█████▎    | 44/82 [00:00<00:00, 106.51it/s] 67%|██████▋   | 55/82 [00:00<00:00, 106.60it/s] 80%|████████  | 66/82 [00:00<00:00, 106.31it/s] 94%|█████████▍| 77/82 [00:00<00:00, 105.83it/s]100%|██████████| 82/82 [00:00<00:00, 106.04it/s]
2024-06-04:02:28:18,585 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:28:18,586 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:28:18,585 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:28:18,586 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:28:18,586 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:28:18,586 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:28:18,586 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:28:18,586 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:19<27:12, 19.91s/it]Running generate_until requests:   2%|▏         | 2/83 [00:38<25:38, 19.00s/it]Running generate_until requests:   4%|▎         | 3/83 [00:56<24:55, 18.69s/it]Running generate_until requests:   5%|▍         | 4/83 [01:00<16:52, 12.82s/it]Running generate_until requests:   6%|▌         | 5/83 [01:18<19:07, 14.71s/it]Running generate_until requests:   7%|▋         | 6/83 [01:36<20:21, 15.87s/it]Running generate_until requests:   8%|▊         | 7/83 [01:54<21:01, 16.60s/it]Running generate_until requests:  10%|▉         | 8/83 [02:12<21:20, 17.08s/it]Running generate_until requests:  11%|█         | 9/83 [02:27<20:13, 16.40s/it]Running generate_until requests:  12%|█▏        | 10/83 [02:35<16:41, 13.72s/it]Running generate_until requests:  13%|█▎        | 11/83 [02:53<18:05, 15.07s/it]Running generate_until requests:  14%|█▍        | 12/83 [03:11<18:54, 15.98s/it]Running generate_until requests:  16%|█▌        | 13/83 [03:17<15:03, 12.91s/it]Running generate_until requests:  17%|█▋        | 14/83 [03:35<16:37, 14.46s/it]Running generate_until requests:  18%|█▊        | 15/83 [03:53<17:38, 15.56s/it]Running generate_until requests:  19%|█▉        | 16/83 [04:11<18:14, 16.34s/it]Running generate_until requests:  20%|██        | 17/83 [04:29<18:31, 16.84s/it]Running generate_until requests:  22%|██▏       | 18/83 [04:47<18:36, 17.18s/it]Running generate_until requests:  23%|██▎       | 19/83 [05:05<18:35, 17.44s/it]Running generate_until requests:  24%|██▍       | 20/83 [05:23<18:28, 17.60s/it]Running generate_until requests:  25%|██▌       | 21/83 [05:41<18:15, 17.67s/it]Running generate_until requests:  27%|██▋       | 22/83 [05:50<15:25, 15.17s/it]Running generate_until requests:  28%|██▊       | 23/83 [06:08<15:57, 15.96s/it]Running generate_until requests:  29%|██▉       | 24/83 [06:26<16:13, 16.51s/it]Running generate_until requests:  30%|███       | 25/83 [06:44<16:20, 16.90s/it]Running generate_until requests:  31%|███▏      | 26/83 [07:02<16:18, 17.17s/it]Running generate_until requests:  33%|███▎      | 27/83 [07:12<14:14, 15.26s/it]Running generate_until requests:  34%|███▎      | 28/83 [07:21<12:01, 13.11s/it]Running generate_until requests:  35%|███▍      | 29/83 [07:28<10:12, 11.34s/it]Running generate_until requests:  36%|███▌      | 30/83 [07:46<11:45, 13.32s/it]Running generate_until requests:  37%|███▋      | 31/83 [08:04<12:43, 14.68s/it]Running generate_until requests:  39%|███▊      | 32/83 [08:11<10:44, 12.64s/it]Running generate_until requests:  40%|███▉      | 33/83 [08:29<11:48, 14.17s/it]Running generate_until requests:  41%|████      | 34/83 [08:47<12:28, 15.28s/it]Running generate_until requests:  42%|████▏     | 35/83 [09:05<12:50, 16.04s/it]Running generate_until requests:  43%|████▎     | 36/83 [09:23<12:58, 16.57s/it]Running generate_until requests:  45%|████▍     | 37/83 [09:41<13:00, 16.96s/it]Running generate_until requests:  46%|████▌     | 38/83 [09:58<12:54, 17.20s/it]Running generate_until requests:  47%|████▋     | 39/83 [10:05<10:23, 14.16s/it]Running generate_until requests:  48%|████▊     | 40/83 [10:23<10:54, 15.21s/it]Running generate_until requests:  49%|████▉     | 41/83 [10:27<08:20, 11.91s/it]Running generate_until requests:  51%|█████     | 42/83 [10:45<09:21, 13.68s/it]Running generate_until requests:  52%|█████▏    | 43/83 [10:57<08:49, 13.24s/it]Running generate_until requests:  53%|█████▎    | 44/83 [11:15<09:30, 14.62s/it]Running generate_until requests:  54%|█████▍    | 45/83 [11:24<08:09, 12.89s/it]Running generate_until requests:  55%|█████▌    | 46/83 [11:42<08:50, 14.35s/it]Running generate_until requests:  57%|█████▋    | 47/83 [11:50<07:29, 12.48s/it]Running generate_until requests:  58%|█████▊    | 48/83 [11:57<06:16, 10.75s/it]Running generate_until requests:  59%|█████▉    | 49/83 [12:07<06:05, 10.74s/it]Running generate_until requests:  60%|██████    | 50/83 [12:14<05:18,  9.67s/it]Running generate_until requests:  61%|██████▏   | 51/83 [12:29<05:52, 11.02s/it]Running generate_until requests:  63%|██████▎   | 52/83 [12:38<05:28, 10.60s/it]Running generate_until requests:  64%|██████▍   | 53/83 [12:56<06:21, 12.71s/it]Running generate_until requests:  65%|██████▌   | 54/83 [13:14<06:51, 14.20s/it]Running generate_until requests:  66%|██████▋   | 55/83 [13:20<05:36, 12.02s/it]Running generate_until requests:  67%|██████▋   | 56/83 [13:38<06:10, 13.72s/it]Running generate_until requests:  69%|██████▊   | 57/83 [13:56<06:27, 14.91s/it]Running generate_until requests:  70%|██████▉   | 58/83 [14:13<06:33, 15.72s/it]Running generate_until requests:  71%|███████   | 59/83 [14:20<05:08, 12.87s/it]Running generate_until requests:  72%|███████▏  | 60/83 [14:37<05:29, 14.31s/it]Running generate_until requests:  73%|███████▎  | 61/83 [14:55<05:36, 15.32s/it]Running generate_until requests:  75%|███████▍  | 62/83 [15:01<04:22, 12.49s/it]Running generate_until requests:  76%|███████▌  | 63/83 [15:08<03:40, 11.00s/it]Running generate_until requests:  77%|███████▋  | 64/83 [15:26<04:07, 13.03s/it]Running generate_until requests:  78%|███████▊  | 65/83 [15:32<03:17, 10.97s/it]Running generate_until requests:  80%|███████▉  | 66/83 [15:50<03:40, 12.98s/it]Running generate_until requests:  81%|████████  | 67/83 [16:08<03:50, 14.41s/it]Running generate_until requests:  82%|████████▏ | 68/83 [16:25<03:50, 15.38s/it]Running generate_until requests:  83%|████████▎ | 69/83 [16:43<03:44, 16.06s/it]Running generate_until requests:  84%|████████▍ | 70/83 [17:01<03:34, 16.54s/it]Running generate_until requests:  86%|████████▌ | 71/83 [17:06<02:37, 13.16s/it]Running generate_until requests:  87%|████████▋ | 72/83 [17:24<02:39, 14.51s/it]Running generate_until requests:  88%|████████▊ | 73/83 [17:41<02:34, 15.44s/it]Running generate_until requests:  89%|████████▉ | 74/83 [17:48<01:56, 12.89s/it]Running generate_until requests:  90%|█████████ | 75/83 [18:06<01:54, 14.30s/it]Running generate_until requests:  92%|█████████▏| 76/83 [18:23<01:46, 15.28s/it]Running generate_until requests:  93%|█████████▎| 77/83 [18:41<01:35, 15.96s/it]Running generate_until requests:  94%|█████████▍| 78/83 [18:58<01:22, 16.43s/it]Running generate_until requests:  95%|█████████▌| 79/83 [19:16<01:07, 16.77s/it]Running generate_until requests:  96%|█████████▋| 80/83 [19:25<00:43, 14.45s/it]Running generate_until requests:  98%|█████████▊| 81/83 [19:30<00:23, 11.60s/it]Running generate_until requests:  99%|█████████▉| 82/83 [19:37<00:10, 10.13s/it]Running generate_until requests: 100%|██████████| 83/83 [19:54<00:00, 12.35s/it]Running generate_until requests: 100%|██████████| 83/83 [19:54<00:00, 14.39s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:02:51:28,105 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:28,192 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:28,233 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:28,254 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:28,469 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:28,649 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:28,662 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:28,807 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:51:33,077 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:33,078 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:33,083 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:33,083 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
2024-06-04:02:51:33,543 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:33,545 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:33,550 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:33,550 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
2024-06-04:02:51:33,684 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:33,685 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:33,689 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:33,689 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
2024-06-04:02:51:33,850 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:33,851 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:33,855 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:33,855 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:51:35,687 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:35,688 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:35,693 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:35,693 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
2024-06-04:02:51:35,846 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:35,847 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:35,852 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:35,852 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
2024-06-04:02:51:35,970 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:35,972 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:35,978 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:35,978 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
2024-06-04:02:51:36,097 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:51:36,098 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:51:36,102 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:51:36,102 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.4, 'check': False}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.26s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.12s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.85s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.83s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:01,  1.99s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.83s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.65s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.63s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.24s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.22s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.22s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.19s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.20s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.20s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:52:23,330 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:23,403 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:23,405 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:23,775 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 130.71it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 34%|███▎      | 28/83 [00:00<00:00, 131.85it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:52:24,081 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:52:24,083 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 51%|█████     | 42/83 [00:00<00:00, 133.30it/s] 67%|██████▋   | 56/83 [00:00<00:00, 133.03it/s]2024-06-04:02:52:24,250 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 84%|████████▍ | 70/83 [00:00<00:00, 132.80it/s] 24%|██▍       | 20/83 [00:00<00:00, 195.42it/s]100%|██████████| 83/83 [00:00<00:00, 132.70it/s]
 48%|████▊     | 40/83 [00:00<00:00, 179.56it/s] 71%|███████   | 59/83 [00:00<00:00, 145.50it/s] 90%|█████████ | 75/83 [00:00<00:00, 138.65it/s]100%|██████████| 83/83 [00:00<00:00, 141.60it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:53:01,771 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:53:01,773 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:53:01,865 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:53:01,867 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:53:02,026 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:53:02,116 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:53:02,139 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
  0%|          | 0/82 [00:00<?, ?it/s]Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:53:02,142 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 22%|██▏       | 18/82 [00:00<00:00, 179.17it/s] 17%|█▋        | 14/82 [00:00<00:00, 136.76it/s] 44%|████▍     | 36/82 [00:00<00:00, 149.32it/s] 35%|███▌      | 29/82 [00:00<00:00, 142.92it/s] 63%|██████▎   | 52/82 [00:00<00:00, 141.37it/s] 54%|█████▎    | 44/82 [00:00<00:00, 145.00it/s]2024-06-04:02:53:02,464 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/82 [00:00<?, ?it/s] 82%|████████▏ | 67/82 [00:00<00:00, 137.78it/s] 74%|███████▍  | 61/82 [00:00<00:00, 154.21it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:53:02,569 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:53:02,571 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 17%|█▋        | 14/82 [00:00<00:00, 132.10it/s] 99%|█████████▉| 81/82 [00:00<00:00, 135.95it/s]100%|██████████| 82/82 [00:00<00:00, 140.62it/s]
 94%|█████████▍| 77/82 [00:00<00:00, 148.54it/s] 34%|███▍      | 28/82 [00:00<00:00, 133.15it/s]100%|██████████| 82/82 [00:00<00:00, 146.51it/s]
 51%|█████     | 42/82 [00:00<00:00, 135.04it/s] 68%|██████▊   | 56/82 [00:00<00:00, 134.97it/s]2024-06-04:02:53:02,907 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 85%|████████▌ | 70/82 [00:00<00:00, 134.68it/s] 16%|█▌        | 13/83 [00:00<00:00, 124.90it/s]100%|██████████| 82/82 [00:00<00:00, 129.44it/s]
 31%|███▏      | 26/83 [00:00<00:00, 103.95it/s] 45%|████▍     | 37/83 [00:00<00:00, 104.71it/s] 58%|█████▊    | 48/83 [00:00<00:00, 105.21it/s] 71%|███████   | 59/83 [00:00<00:00, 105.51it/s] 84%|████████▍ | 70/83 [00:00<00:00, 105.64it/s] 98%|█████████▊| 81/83 [00:00<00:00, 105.26it/s]100%|██████████| 83/83 [00:00<00:00, 105.90it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:53:10,003 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:53:10,005 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:53:10,370 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 130.38it/s] 34%|███▍      | 28/82 [00:00<00:00, 131.38it/s] 51%|█████     | 42/82 [00:00<00:00, 131.61it/s] 68%|██████▊   | 56/82 [00:00<00:00, 131.54it/s] 85%|████████▌ | 70/82 [00:00<00:00, 126.49it/s]100%|██████████| 82/82 [00:00<00:00, 128.87it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:53:15,143 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:53:15,145 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:53:15,652 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 101.91it/s] 27%|██▋       | 22/83 [00:00<00:00, 102.04it/s] 40%|███▉      | 33/83 [00:00<00:00, 102.28it/s] 53%|█████▎    | 44/83 [00:00<00:00, 102.12it/s] 66%|██████▋   | 55/83 [00:00<00:00, 102.22it/s] 80%|███████▉  | 66/83 [00:00<00:00, 102.37it/s] 93%|█████████▎| 77/83 [00:00<00:00, 102.24it/s]100%|██████████| 83/83 [00:00<00:00, 102.19it/s]
2024-06-04:02:53:27,577 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:53:27,578 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:53:27,578 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:53:27,578 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:53:27,578 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:53:27,578 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:53:27,578 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:53:27,578 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:19<27:13, 19.92s/it]Running generate_until requests:   2%|▏         | 2/83 [00:31<20:28, 15.17s/it]Running generate_until requests:   4%|▎         | 3/83 [00:39<15:44, 11.81s/it]Running generate_until requests:   5%|▍         | 4/83 [00:44<12:05,  9.18s/it]Running generate_until requests:   6%|▌         | 5/83 [00:52<11:25,  8.79s/it]Running generate_until requests:   7%|▋         | 6/83 [01:00<10:40,  8.32s/it]Running generate_until requests:   8%|▊         | 7/83 [01:12<12:08,  9.58s/it]Running generate_until requests:  10%|▉         | 8/83 [01:19<11:08,  8.91s/it]Running generate_until requests:  11%|█         | 9/83 [01:28<10:51,  8.81s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:36<10:23,  8.54s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:44<10:14,  8.54s/it]Running generate_until requests:  14%|█▍        | 12/83 [02:00<12:47, 10.81s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:06<10:43,  9.19s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:13<09:42,  8.44s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:20<09:07,  8.05s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:38<12:24, 11.11s/it]Running generate_until requests:  20%|██        | 17/83 [02:46<11:15, 10.23s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:54<10:11,  9.41s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:03<10:03,  9.43s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:14<10:21,  9.86s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:19<08:47,  8.51s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:25<07:46,  7.64s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:34<08:11,  8.18s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:43<08:18,  8.44s/it]Running generate_until requests:  30%|███       | 25/83 [03:51<07:51,  8.13s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:57<07:14,  7.63s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:06<07:21,  7.88s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:11<06:33,  7.16s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:19<06:35,  7.32s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:28<06:54,  7.82s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:33<06:08,  7.08s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:41<06:05,  7.17s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:58<08:34, 10.29s/it]Running generate_until requests:  41%|████      | 34/83 [05:08<08:21, 10.23s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:15<07:22,  9.23s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:22<06:37,  8.45s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:31<06:37,  8.64s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:49<08:32, 11.40s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:54<07:00,  9.57s/it]Running generate_until requests:  48%|████▊     | 40/83 [06:00<05:58,  8.34s/it]Running generate_until requests:  49%|████▉     | 41/83 [06:05<05:13,  7.47s/it]Running generate_until requests:  51%|█████     | 42/83 [06:17<06:04,  8.89s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:26<05:48,  8.72s/it]Running generate_until requests:  53%|█████▎    | 44/83 [06:32<05:19,  8.18s/it]Running generate_until requests:  54%|█████▍    | 45/83 [06:38<04:38,  7.33s/it]Running generate_until requests:  55%|█████▌    | 46/83 [06:43<04:13,  6.84s/it]Running generate_until requests:  57%|█████▋    | 47/83 [06:49<03:53,  6.48s/it]Running generate_until requests:  58%|█████▊    | 48/83 [06:57<04:01,  6.91s/it]Running generate_until requests:  59%|█████▉    | 49/83 [07:01<03:25,  6.03s/it]Running generate_until requests:  60%|██████    | 50/83 [07:08<03:27,  6.30s/it]Running generate_until requests:  61%|██████▏   | 51/83 [07:20<04:13,  7.94s/it]Running generate_until requests:  63%|██████▎   | 52/83 [07:24<03:31,  6.81s/it]Running generate_until requests:  64%|██████▍   | 53/83 [07:31<03:22,  6.76s/it]Running generate_until requests:  65%|██████▌   | 54/83 [07:36<03:07,  6.45s/it]Running generate_until requests:  66%|██████▋   | 55/83 [07:41<02:45,  5.90s/it]Running generate_until requests:  67%|██████▋   | 56/83 [07:48<02:45,  6.13s/it]Running generate_until requests:  69%|██████▊   | 57/83 [08:05<04:11,  9.68s/it]Running generate_until requests:  70%|██████▉   | 58/83 [08:13<03:44,  8.97s/it]Running generate_until requests:  71%|███████   | 59/83 [08:18<03:09,  7.90s/it]Running generate_until requests:  72%|███████▏  | 60/83 [08:24<02:46,  7.23s/it]Running generate_until requests:  73%|███████▎  | 61/83 [08:30<02:34,  7.04s/it]Running generate_until requests:  75%|███████▍  | 62/83 [08:35<02:13,  6.35s/it]Running generate_until requests:  76%|███████▌  | 63/83 [08:53<03:14,  9.73s/it]Running generate_until requests:  77%|███████▋  | 64/83 [09:11<03:50, 12.15s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:16<03:02, 10.16s/it]Running generate_until requests:  80%|███████▉  | 66/83 [09:22<02:29,  8.81s/it]Running generate_until requests:  81%|████████  | 67/83 [09:25<01:54,  7.17s/it]Running generate_until requests:  82%|████████▏ | 68/83 [09:31<01:42,  6.80s/it]Running generate_until requests:  83%|████████▎ | 69/83 [09:43<01:55,  8.25s/it]Running generate_until requests:  84%|████████▍ | 70/83 [09:53<01:56,  8.95s/it]Running generate_until requests:  86%|████████▌ | 71/83 [09:58<01:32,  7.68s/it]Running generate_until requests:  87%|████████▋ | 72/83 [10:02<01:13,  6.71s/it]Running generate_until requests:  88%|████████▊ | 73/83 [10:08<01:03,  6.35s/it]Running generate_until requests:  89%|████████▉ | 74/83 [10:13<00:54,  6.10s/it]Running generate_until requests:  90%|█████████ | 75/83 [10:23<00:57,  7.16s/it]Running generate_until requests:  92%|█████████▏| 76/83 [10:29<00:47,  6.81s/it]Running generate_until requests:  93%|█████████▎| 77/83 [10:35<00:38,  6.40s/it]Running generate_until requests:  94%|█████████▍| 78/83 [10:42<00:32,  6.60s/it]Running generate_until requests:  95%|█████████▌| 79/83 [10:46<00:23,  5.92s/it]Running generate_until requests:  96%|█████████▋| 80/83 [10:50<00:16,  5.48s/it]Running generate_until requests:  98%|█████████▊| 81/83 [10:56<00:11,  5.58s/it]Running generate_until requests:  99%|█████████▉| 82/83 [11:01<00:05,  5.42s/it]Running generate_until requests: 100%|██████████| 83/83 [11:06<00:00,  5.08s/it]Running generate_until requests: 100%|██████████| 83/83 [11:06<00:00,  8.02s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:03:05:52,946 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:52,952 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:53,034 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:53,086 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:53,153 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:53,280 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:53,759 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:53,838 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:05:58,195 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:58,197 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:58,203 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:58,203 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
2024-06-04:03:05:58,553 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:58,555 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:58,560 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:58,561 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
2024-06-04:03:05:59,015 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:59,017 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:59,022 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:59,022 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:05:59,656 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:05:59,657 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:05:59,665 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:05:59,665 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:06:00,520 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:06:00,521 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:06:00,528 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:06:00,528 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
2024-06-04:03:06:00,715 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:06:00,717 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:06:00,737 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:06:00,737 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
2024-06-04:03:06:00,973 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:06:00,974 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:06:00,980 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:06:00,980 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:06:01,374 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:06:01,375 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:06:01,379 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:06:01,379 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': False}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.30s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.17s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.38s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.20s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.24s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.93s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.82s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.77s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.10s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.74s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.87s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.88s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.13s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:02,  2.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.74s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.48s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.25s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.38s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.39s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.48s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:53,213 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:53,215 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:53,395 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/83 [00:00<00:00, 197.06it/s] 49%|████▉     | 41/83 [00:00<00:00, 200.42it/s] 75%|███████▍  | 62/83 [00:00<00:00, 199.11it/s] 99%|█████████▉| 82/83 [00:00<00:00, 199.09it/s]100%|██████████| 83/83 [00:00<00:00, 199.05it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:06:54,002 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:54,004 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:06:54,166 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 203.08it/s] 51%|█████     | 42/82 [00:00<00:00, 204.21it/s] 77%|███████▋  | 63/82 [00:00<00:00, 204.49it/s]100%|██████████| 82/82 [00:00<00:00, 204.30it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:07:03,912 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:07:03,989 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:07:03,991 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:07:04,190 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 204.59it/s] 51%|█████     | 42/83 [00:00<00:00, 206.02it/s] 76%|███████▌  | 63/83 [00:00<00:00, 206.13it/s]100%|██████████| 83/83 [00:00<00:00, 205.94it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:07:28,813 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:07:28,815 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:07:28,968 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:07:28,970 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:07:29,101 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 139.03it/s]2024-06-04:03:07:29,223 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 35%|███▌      | 29/82 [00:00<00:00, 140.45it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 17%|█▋        | 14/82 [00:00<00:00, 137.30it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:07:29,417 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:07:29,419 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 54%|█████▎    | 44/82 [00:00<00:00, 139.61it/s] 39%|███▉      | 32/82 [00:00<00:00, 157.92it/s] 71%|███████   | 58/82 [00:00<00:00, 139.50it/s] 59%|█████▊    | 48/82 [00:00<00:00, 143.39it/s] 90%|█████████ | 74/82 [00:00<00:00, 144.95it/s] 77%|███████▋  | 63/82 [00:00<00:00, 137.53it/s]100%|██████████| 82/82 [00:00<00:00, 142.21it/s]
2024-06-04:03:07:29,731 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 94%|█████████▍| 77/82 [00:00<00:00, 135.56it/s]100%|██████████| 82/82 [00:00<00:00, 138.82it/s]
 17%|█▋        | 14/83 [00:00<00:00, 133.06it/s] 34%|███▎      | 28/83 [00:00<00:00, 123.40it/s] 49%|████▉     | 41/83 [00:00<00:00, 119.82it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 65%|██████▌   | 54/83 [00:00<00:00, 113.38it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:07:30,279 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:07:30,283 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 80%|███████▉  | 66/83 [00:00<00:00, 110.15it/s] 94%|█████████▍| 78/83 [00:00<00:00, 108.72it/s]100%|██████████| 83/83 [00:00<00:00, 112.68it/s]
2024-06-04:03:07:30,783 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 106.81it/s] 27%|██▋       | 22/83 [00:00<00:00, 106.84it/s] 40%|███▉      | 33/83 [00:00<00:00, 107.87it/s] 53%|█████▎    | 44/83 [00:00<00:00, 108.22it/s] 66%|██████▋   | 55/83 [00:00<00:00, 108.35it/s] 80%|███████▉  | 66/83 [00:00<00:00, 108.49it/s] 93%|█████████▎| 77/83 [00:00<00:00, 108.54it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
100%|██████████| 83/83 [00:00<00:00, 108.17it/s]
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:07:31,636 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:07:31,640 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:07:32,093 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 103.84it/s] 27%|██▋       | 22/82 [00:00<00:00, 104.82it/s] 40%|████      | 33/82 [00:00<00:00, 105.28it/s] 54%|█████▎    | 44/82 [00:00<00:00, 105.62it/s] 67%|██████▋   | 55/82 [00:00<00:00, 103.30it/s] 80%|████████  | 66/82 [00:00<00:00, 103.55it/s] 94%|█████████▍| 77/82 [00:00<00:00, 104.14it/s]100%|██████████| 82/82 [00:00<00:00, 104.26it/s]
2024-06-04:03:07:43,586 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:43,586 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:43,587 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:43,587 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:43,587 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:43,587 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:03:07:43,588 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:07:43,588 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:11<15:38, 11.45s/it]Running generate_until requests:   2%|▏         | 2/83 [00:33<23:34, 17.46s/it]Running generate_until requests:   4%|▎         | 3/83 [00:41<17:29, 13.12s/it]Running generate_until requests:   5%|▍         | 4/83 [00:47<13:50, 10.51s/it]Running generate_until requests:   6%|▌         | 5/83 [00:56<12:51,  9.89s/it]Running generate_until requests:   7%|▋         | 6/83 [01:03<11:26,  8.91s/it]Running generate_until requests:   8%|▊         | 7/83 [01:12<11:28,  9.06s/it]Running generate_until requests:  10%|▉         | 8/83 [01:20<10:54,  8.73s/it]Running generate_until requests:  11%|█         | 9/83 [01:31<11:29,  9.31s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:38<10:23,  8.55s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:45<09:48,  8.17s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:57<10:56,  9.25s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:03<09:45,  8.37s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:12<09:40,  8.41s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:26<11:42, 10.32s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:43<13:40, 12.24s/it]Running generate_until requests:  20%|██        | 17/83 [02:53<12:41, 11.54s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:01<11:30, 10.63s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:11<10:54, 10.22s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:24<11:33, 11.01s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:31<10:19, 10.00s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:39<09:24,  9.26s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:47<09:01,  9.03s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:55<08:28,  8.61s/it]Running generate_until requests:  30%|███       | 25/83 [04:02<07:55,  8.19s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:09<07:33,  7.96s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:18<07:28,  8.02s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:25<07:12,  7.87s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:34<07:15,  8.07s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:45<08:03,  9.12s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:53<07:37,  8.80s/it]Running generate_until requests:  39%|███▊      | 32/83 [05:02<07:25,  8.74s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:07<06:21,  7.63s/it]Running generate_until requests:  41%|████      | 34/83 [05:19<07:18,  8.95s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:27<06:51,  8.58s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:34<06:23,  8.16s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:42<06:17,  8.20s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:54<06:53,  9.19s/it]Running generate_until requests:  47%|████▋     | 39/83 [06:00<06:02,  8.23s/it]Running generate_until requests:  48%|████▊     | 40/83 [06:06<05:33,  7.75s/it]Running generate_until requests:  49%|████▉     | 41/83 [06:14<05:18,  7.59s/it]Running generate_until requests:  51%|█████     | 42/83 [06:22<05:25,  7.95s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:33<05:47,  8.69s/it]Running generate_until requests:  53%|█████▎    | 44/83 [06:40<05:23,  8.29s/it]Running generate_until requests:  54%|█████▍    | 45/83 [06:47<05:03,  8.00s/it]Running generate_until requests:  55%|█████▌    | 46/83 [06:54<04:44,  7.69s/it]Running generate_until requests:  57%|█████▋    | 47/83 [07:01<04:20,  7.25s/it]Running generate_until requests:  58%|█████▊    | 48/83 [07:10<04:39,  7.98s/it]Running generate_until requests:  59%|█████▉    | 49/83 [07:16<04:03,  7.16s/it]Running generate_until requests:  60%|██████    | 50/83 [07:24<04:05,  7.43s/it]Running generate_until requests:  61%|██████▏   | 51/83 [07:32<04:08,  7.76s/it]Running generate_until requests:  63%|██████▎   | 52/83 [07:39<03:50,  7.42s/it]Running generate_until requests:  64%|██████▍   | 53/83 [07:46<03:42,  7.41s/it]Running generate_until requests:  65%|██████▌   | 54/83 [07:53<03:29,  7.24s/it]Running generate_until requests:  66%|██████▋   | 55/83 [07:59<03:13,  6.92s/it]Running generate_until requests:  67%|██████▋   | 56/83 [08:13<04:03,  9.02s/it]Running generate_until requests:  69%|██████▊   | 57/83 [08:25<04:19,  9.98s/it]Running generate_until requests:  70%|██████▉   | 58/83 [08:33<03:55,  9.42s/it]Running generate_until requests:  71%|███████   | 59/83 [08:40<03:27,  8.65s/it]Running generate_until requests:  72%|███████▏  | 60/83 [08:46<03:01,  7.88s/it]Running generate_until requests:  73%|███████▎  | 61/83 [08:55<03:00,  8.23s/it]Running generate_until requests:  75%|███████▍  | 62/83 [09:00<02:32,  7.26s/it]Running generate_until requests:  76%|███████▌  | 63/83 [09:07<02:18,  6.93s/it]Running generate_until requests:  77%|███████▋  | 64/83 [09:14<02:13,  7.04s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:20<02:03,  6.87s/it]Running generate_until requests:  80%|███████▉  | 66/83 [09:27<01:58,  6.96s/it]Running generate_until requests:  81%|████████  | 67/83 [09:31<01:36,  6.03s/it]Running generate_until requests:  82%|████████▏ | 68/83 [09:35<01:22,  5.47s/it]Running generate_until requests:  83%|████████▎ | 69/83 [09:45<01:32,  6.58s/it]Running generate_until requests:  84%|████████▍ | 70/83 [09:58<01:52,  8.64s/it]Running generate_until requests:  86%|████████▌ | 71/83 [10:04<01:32,  7.69s/it]Running generate_until requests:  87%|████████▋ | 72/83 [10:10<01:20,  7.31s/it]Running generate_until requests:  88%|████████▊ | 73/83 [10:17<01:11,  7.15s/it]Running generate_until requests:  89%|████████▉ | 74/83 [10:23<01:01,  6.87s/it]Running generate_until requests:  90%|█████████ | 75/83 [10:34<01:04,  8.10s/it]Running generate_until requests:  92%|█████████▏| 76/83 [10:41<00:55,  7.89s/it]Running generate_until requests:  93%|█████████▎| 77/83 [10:48<00:45,  7.55s/it]Running generate_until requests:  94%|█████████▍| 78/83 [10:56<00:38,  7.63s/it]Running generate_until requests:  95%|█████████▌| 79/83 [11:04<00:31,  7.75s/it]Running generate_until requests:  96%|█████████▋| 80/83 [11:09<00:21,  7.01s/it]Running generate_until requests:  98%|█████████▊| 81/83 [11:16<00:13,  6.94s/it]Running generate_until requests:  99%|█████████▉| 82/83 [11:22<00:06,  6.61s/it]Running generate_until requests: 100%|██████████| 83/83 [11:27<00:00,  6.05s/it]Running generate_until requests: 100%|██████████| 83/83 [11:27<00:00,  8.28s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:03:19:58,502 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:19:58,502 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:19:58,525 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:19:58,603 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:19:58,664 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:19:58,723 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:19:58,785 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:19:58,832 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:20:03,649 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:20:03,650 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:20:03,650 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:20:03,652 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:20:03,656 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:20:03,656 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
2024-06-04:03:20:03,659 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:20:03,659 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:20:05,457 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:20:05,459 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:20:05,463 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:20:05,463 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
2024-06-04:03:20:05,572 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:20:05,573 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:20:05,578 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:20:05,578 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
2024-06-04:03:20:05,635 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:20:05,636 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:20:05,641 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:20:05,641 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
2024-06-04:03:20:05,809 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:20:05,809 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:20:05,811 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:20:05,811 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:20:05,814 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:20:05,815 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:20:05,815 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:20:05,816 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
2024-06-04:03:20:05,817 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:20:05,817 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
2024-06-04:03:20:05,819 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:20:05,819 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.6, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.33s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.27s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.14s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.11s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.84s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.79s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.81s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.87s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.71s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.70s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.74s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.25s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.28s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.21s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.20s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.25s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.29s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:20:52,033 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:20:52,104 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:20:52,106 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:20:52,280 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/83 [00:00<00:00, 192.60it/s] 48%|████▊     | 40/83 [00:00<00:00, 190.84it/s] 73%|███████▎  | 61/83 [00:00<00:00, 195.27it/s] 99%|█████████▉| 82/83 [00:00<00:00, 199.71it/s]100%|██████████| 83/83 [00:00<00:00, 197.35it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:20:54,687 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:20:54,689 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:20:54,863 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 20%|█▉        | 16/82 [00:00<00:00, 155.31it/s] 39%|███▉      | 32/82 [00:00<00:00, 147.33it/s] 57%|█████▋    | 47/82 [00:00<00:00, 145.32it/s] 76%|███████▌  | 62/82 [00:00<00:00, 144.00it/s] 94%|█████████▍| 77/82 [00:00<00:00, 143.43it/s]100%|██████████| 82/82 [00:00<00:00, 144.66it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:21:35,072 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:21:35,074 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:21:35,095 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:21:35,097 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:21:35,168 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:21:35,170 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:21:35,217 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:21:35,219 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:21:35,342 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:03:21:35,403 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 16%|█▌        | 13/82 [00:00<00:00, 126.83it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 16%|█▌        | 13/82 [00:00<00:00, 129.06it/s]2024-06-04:03:21:35,544 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
2024-06-04:03:21:35,549 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 32%|███▏      | 26/82 [00:00<00:00, 127.79it/s]  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:21:35,594 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:21:35,597 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 32%|███▏      | 26/82 [00:00<00:00, 129.59it/s] 16%|█▌        | 13/82 [00:00<00:00, 128.70it/s] 17%|█▋        | 14/83 [00:00<00:00, 134.40it/s] 49%|████▉     | 40/82 [00:00<00:00, 130.94it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 49%|████▉     | 40/82 [00:00<00:00, 129.99it/s] 34%|███▎      | 28/83 [00:00<00:00, 135.05it/s] 66%|██████▌   | 54/82 [00:00<00:00, 132.81it/s] 33%|███▎      | 27/82 [00:00<00:00, 129.89it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:21:35,810 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:21:35,812 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:21:35,830 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
 66%|██████▌   | 54/82 [00:00<00:00, 130.31it/s]  0%|          | 0/83 [00:00<?, ?it/s] 51%|█████     | 42/83 [00:00<00:00, 135.34it/s] 83%|████████▎ | 68/82 [00:00<00:00, 133.85it/s] 50%|█████     | 41/82 [00:00<00:00, 130.12it/s] 83%|████████▎ | 68/82 [00:00<00:00, 130.33it/s] 18%|█▊        | 15/83 [00:00<00:00, 139.96it/s] 67%|██████▋   | 56/83 [00:00<00:00, 135.45it/s]100%|██████████| 82/82 [00:00<00:00, 134.24it/s]100%|██████████| 82/82 [00:00<00:00, 132.69it/s]
 67%|██████▋   | 55/82 [00:00<00:00, 130.14it/s]100%|██████████| 82/82 [00:00<00:00, 130.28it/s]100%|██████████| 82/82 [00:00<00:00, 130.10it/s]
 35%|███▍      | 29/83 [00:00<00:00, 132.80it/s] 84%|████████▍ | 69/82 [00:00<00:00, 130.50it/s] 84%|████████▍ | 70/83 [00:00<00:00, 128.98it/s]2024-06-04:03:21:36,140 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 52%|█████▏    | 43/83 [00:00<00:00, 130.95it/s]100%|██████████| 82/82 [00:00<00:00, 130.94it/s]
100%|██████████| 83/83 [00:00<00:00, 124.65it/s]100%|██████████| 83/83 [00:00<00:00, 128.99it/s]
 17%|█▋        | 14/83 [00:00<00:00, 130.24it/s] 69%|██████▊   | 57/83 [00:00<00:00, 131.05it/s] 34%|███▎      | 28/83 [00:00<00:00, 130.96it/s] 86%|████████▌ | 71/83 [00:00<00:00, 131.16it/s] 51%|█████     | 42/83 [00:00<00:00, 131.63it/s]100%|██████████| 83/83 [00:00<00:00, 131.93it/s]
 67%|██████▋   | 56/83 [00:00<00:00, 126.57it/s] 83%|████████▎ | 69/83 [00:00<00:00, 121.62it/s] 99%|█████████▉| 82/83 [00:00<00:00, 119.24it/s]100%|██████████| 83/83 [00:00<00:00, 123.07it/s]
2024-06-04:03:21:48,126 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:21:48,126 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:21:48,126 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:21:48,126 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:21:48,126 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:21:48,126 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:21:48,126 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:03:21:48,129 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:12<17:32, 12.83s/it]Running generate_until requests:   2%|▏         | 2/83 [00:27<18:59, 14.06s/it]Running generate_until requests:   4%|▎         | 3/83 [00:35<14:54, 11.18s/it]Running generate_until requests:   5%|▍         | 4/83 [00:41<11:59,  9.10s/it]Running generate_until requests:   6%|▌         | 5/83 [00:49<11:22,  8.75s/it]Running generate_until requests:   7%|▋         | 6/83 [00:54<09:45,  7.61s/it]Running generate_until requests:   8%|▊         | 7/83 [01:03<09:52,  7.79s/it]Running generate_until requests:  10%|▉         | 8/83 [01:10<09:29,  7.59s/it]Running generate_until requests:  11%|█         | 9/83 [01:19<09:57,  8.08s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:24<08:51,  7.28s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:33<09:12,  7.68s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:43<09:55,  8.38s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:47<08:04,  6.92s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:54<08:08,  7.07s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:03<08:47,  7.76s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:15<10:04,  9.02s/it]Running generate_until requests:  20%|██        | 17/83 [02:22<09:15,  8.42s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:29<08:42,  8.03s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:37<08:22,  7.84s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:48<09:13,  8.78s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:53<08:05,  7.84s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:00<07:33,  7.43s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:07<07:15,  7.25s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:13<06:53,  7.01s/it]Running generate_until requests:  30%|███       | 25/83 [03:20<06:48,  7.05s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:27<06:30,  6.85s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:37<07:26,  7.98s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:44<06:53,  7.51s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:50<06:31,  7.26s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:01<07:11,  8.14s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:05<06:08,  7.09s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:12<05:52,  6.91s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:16<05:12,  6.26s/it]Running generate_until requests:  41%|████      | 34/83 [04:27<06:09,  7.55s/it]Running generate_until requests:  42%|████▏     | 35/83 [04:33<05:39,  7.07s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:40<05:25,  6.92s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:47<05:24,  7.06s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:59<06:25,  8.58s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:04<05:32,  7.56s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:10<05:04,  7.09s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:16<04:35,  6.57s/it]Running generate_until requests:  51%|█████     | 42/83 [05:21<04:20,  6.35s/it]Running generate_until requests:  52%|█████▏    | 43/83 [05:30<04:39,  6.99s/it]Running generate_until requests:  53%|█████▎    | 44/83 [05:36<04:25,  6.80s/it]Running generate_until requests:  54%|█████▍    | 45/83 [05:43<04:13,  6.67s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:49<04:05,  6.64s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:54<03:42,  6.18s/it]Running generate_until requests:  58%|█████▊    | 48/83 [06:02<03:48,  6.53s/it]Running generate_until requests:  59%|█████▉    | 49/83 [06:06<03:23,  6.00s/it]Running generate_until requests:  60%|██████    | 50/83 [06:14<03:36,  6.55s/it]Running generate_until requests:  61%|██████▏   | 51/83 [06:21<03:31,  6.62s/it]Running generate_until requests:  63%|██████▎   | 52/83 [06:27<03:20,  6.45s/it]Running generate_until requests:  64%|██████▍   | 53/83 [06:34<03:13,  6.46s/it]Running generate_until requests:  65%|██████▌   | 54/83 [06:39<02:56,  6.07s/it]Running generate_until requests:  66%|██████▋   | 55/83 [06:45<02:49,  6.05s/it]Running generate_until requests:  67%|██████▋   | 56/83 [06:59<03:45,  8.36s/it]Running generate_until requests:  69%|██████▊   | 57/83 [07:12<04:20, 10.02s/it]Running generate_until requests:  70%|██████▉   | 58/83 [07:19<03:48,  9.13s/it]Running generate_until requests:  71%|███████   | 59/83 [07:25<03:15,  8.16s/it]Running generate_until requests:  72%|███████▏  | 60/83 [07:30<02:45,  7.21s/it]Running generate_until requests:  73%|███████▎  | 61/83 [07:38<02:42,  7.39s/it]Running generate_until requests:  75%|███████▍  | 62/83 [07:44<02:24,  6.88s/it]Running generate_until requests:  76%|███████▌  | 63/83 [07:49<02:06,  6.34s/it]Running generate_until requests:  77%|███████▋  | 64/83 [07:57<02:08,  6.76s/it]Running generate_until requests:  78%|███████▊  | 65/83 [08:02<01:55,  6.39s/it]Running generate_until requests:  80%|███████▉  | 66/83 [08:11<01:59,  7.06s/it]Running generate_until requests:  81%|████████  | 67/83 [08:15<01:37,  6.09s/it]Running generate_until requests:  82%|████████▏ | 68/83 [08:20<01:29,  5.96s/it]Running generate_until requests:  83%|████████▎ | 69/83 [08:30<01:39,  7.11s/it]Running generate_until requests:  84%|████████▍ | 70/83 [08:38<01:37,  7.47s/it]Running generate_until requests:  86%|████████▌ | 71/83 [08:43<01:20,  6.68s/it]Running generate_until requests:  87%|████████▋ | 72/83 [08:49<01:10,  6.37s/it]Running generate_until requests:  88%|████████▊ | 73/83 [08:56<01:05,  6.57s/it]Running generate_until requests:  89%|████████▉ | 74/83 [09:02<00:56,  6.33s/it]Running generate_until requests:  90%|█████████ | 75/83 [09:10<00:55,  6.93s/it]Running generate_until requests:  92%|█████████▏| 76/83 [09:17<00:49,  7.02s/it]Running generate_until requests:  93%|█████████▎| 77/83 [09:22<00:38,  6.38s/it]Running generate_until requests:  94%|█████████▍| 78/83 [09:29<00:32,  6.43s/it]Running generate_until requests:  95%|█████████▌| 79/83 [09:37<00:28,  7.09s/it]Running generate_until requests:  96%|█████████▋| 80/83 [09:45<00:21,  7.13s/it]Running generate_until requests:  98%|█████████▊| 81/83 [09:51<00:13,  6.80s/it]Running generate_until requests:  99%|█████████▉| 82/83 [09:55<00:06,  6.18s/it]Running generate_until requests: 100%|██████████| 83/83 [09:59<00:00,  5.56s/it]Running generate_until requests: 100%|██████████| 83/83 [09:59<00:00,  7.23s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:03:34:48,007 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:34:48,024 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:34:48,226 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:34:48,348 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:34:48,707 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:34:49,017 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:34:49,130 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:34:49,140 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:34:52,627 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:52,629 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:52,633 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:52,633 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
2024-06-04:03:34:53,270 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:53,271 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:53,276 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:53,276 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
2024-06-04:03:34:53,347 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:53,348 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:53,352 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:53,352 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:34:54,609 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:54,611 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:54,617 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:54,617 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
2024-06-04:03:34:55,570 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:55,571 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:55,577 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:55,577 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
2024-06-04:03:34:55,741 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:55,742 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:55,747 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:55,747 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.09s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:34:56,285 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:56,286 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:56,291 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:56,291 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:34:56,425 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:56,427 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:56,432 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:56,432 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.7, 'check': False}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.01s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.22s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.00s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.03s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.57s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:01,  1.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.65s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.77s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.60s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.14s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.82s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.17s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.22s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.21s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.18s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:35:43,220 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:35:43,222 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:35:43,298 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:35:43,300 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:35:43,401 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:03:35:43,476 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 195.91it/s] 24%|██▍       | 20/83 [00:00<00:00, 198.19it/s] 49%|████▉     | 40/82 [00:00<00:00, 197.43it/s] 48%|████▊     | 40/83 [00:00<00:00, 198.77it/s] 73%|███████▎  | 60/82 [00:00<00:00, 197.92it/s] 72%|███████▏  | 60/83 [00:00<00:00, 199.23it/s] 98%|█████████▊| 80/82 [00:00<00:00, 198.50it/s]100%|██████████| 82/82 [00:00<00:00, 198.07it/s]
 96%|█████████▋| 80/83 [00:00<00:00, 199.50it/s]100%|██████████| 83/83 [00:00<00:00, 199.27it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:36:03,697 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:36:03,699 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:36:03,900 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 204.08it/s] 51%|█████     | 42/82 [00:00<00:00, 205.43it/s] 77%|███████▋  | 63/82 [00:00<00:00, 204.20it/s]100%|██████████| 82/82 [00:00<00:00, 204.57it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:36:04,707 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:36:04,711 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:36:04,923 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 206.04it/s] 51%|█████     | 42/83 [00:00<00:00, 207.12it/s] 76%|███████▌  | 63/83 [00:00<00:00, 207.46it/s]100%|██████████| 83/83 [00:00<00:00, 207.59it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:36:21,478 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:36:21,551 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:36:21,553 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:36:21,810 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 18%|█▊        | 15/83 [00:00<00:00, 142.99it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:36:21,943 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:36:21,945 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 36%|███▌      | 30/83 [00:00<00:00, 143.79it/s] 54%|█████▍    | 45/83 [00:00<00:00, 144.73it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:36:22,238 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:36:22,244 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
 76%|███████▌  | 63/83 [00:00<00:00, 157.26it/s]Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:36:22,246 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 134.16it/s] 95%|█████████▌| 79/83 [00:00<00:00, 145.67it/s]100%|██████████| 83/83 [00:00<00:00, 145.97it/s]
 37%|███▋      | 30/82 [00:00<00:00, 145.46it/s]2024-06-04:03:36:22,572 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 55%|█████▍    | 45/82 [00:00<00:00, 102.32it/s] 18%|█▊        | 15/82 [00:00<00:00, 141.71it/s] 72%|███████▏  | 59/82 [00:00<00:00, 111.41it/s] 37%|███▋      | 30/82 [00:00<00:00, 140.04it/s] 90%|█████████ | 74/82 [00:00<00:00, 121.41it/s] 55%|█████▍    | 45/82 [00:00<00:00, 140.44it/s]100%|██████████| 82/82 [00:00<00:00, 121.29it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 73%|███████▎  | 60/82 [00:00<00:00, 138.15it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:36:23,099 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:36:23,101 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 90%|█████████ | 74/82 [00:00<00:00, 137.57it/s]100%|██████████| 82/82 [00:00<00:00, 137.54it/s]
2024-06-04:03:36:23,496 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 13%|█▎        | 11/83 [00:00<00:00, 108.39it/s] 27%|██▋       | 22/83 [00:00<00:00, 109.20it/s] 40%|███▉      | 33/83 [00:00<00:00, 109.54it/s] 54%|█████▍    | 45/83 [00:00<00:00, 109.63it/s] 67%|██████▋   | 56/83 [00:00<00:00, 109.71it/s] 82%|████████▏ | 68/83 [00:00<00:00, 109.96it/s] 96%|█████████▋| 80/83 [00:00<00:00, 109.96it/s]100%|██████████| 83/83 [00:00<00:00, 109.70it/s]
2024-06-04:03:36:33,848 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:36:33,848 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:36:33,848 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:36:33,848 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:36:33,848 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:36:33,849 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:36:33,849 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:36:33,849 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:10<14:38, 10.71s/it]Running generate_until requests:   2%|▏         | 2/83 [00:23<16:13, 12.02s/it]Running generate_until requests:   4%|▎         | 3/83 [00:31<13:24, 10.05s/it]Running generate_until requests:   5%|▍         | 4/83 [00:36<10:38,  8.09s/it]Running generate_until requests:   6%|▌         | 5/83 [00:44<10:27,  8.05s/it]Running generate_until requests:   7%|▋         | 6/83 [00:50<09:24,  7.34s/it]Running generate_until requests:   8%|▊         | 7/83 [00:58<09:47,  7.73s/it]Running generate_until requests:  10%|▉         | 8/83 [01:05<09:23,  7.51s/it]Running generate_until requests:  11%|█         | 9/83 [01:14<09:51,  7.99s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:20<08:44,  7.19s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:27<08:28,  7.07s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:37<09:24,  7.95s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:41<07:58,  6.83s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:48<07:50,  6.82s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:03<10:41,  9.43s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:15<11:18, 10.13s/it]Running generate_until requests:  20%|██        | 17/83 [02:23<10:28,  9.53s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:30<09:31,  8.80s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:38<09:06,  8.54s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:49<09:45,  9.30s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:55<08:27,  8.19s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:01<07:47,  7.66s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:09<07:34,  7.57s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:15<07:05,  7.20s/it]Running generate_until requests:  30%|███       | 25/83 [03:21<06:33,  6.78s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:27<06:18,  6.64s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:34<06:19,  6.77s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:41<06:07,  6.68s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:47<05:49,  6.47s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:56<06:35,  7.45s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:04<06:24,  7.40s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:10<06:02,  7.11s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:15<05:17,  6.36s/it]Running generate_until requests:  41%|████      | 34/83 [04:25<06:13,  7.62s/it]Running generate_until requests:  42%|████▏     | 35/83 [04:31<05:40,  7.10s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:38<05:29,  7.01s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:45<05:19,  6.95s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:56<06:16,  8.38s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:01<05:25,  7.41s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:07<04:58,  6.93s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:12<04:25,  6.33s/it]Running generate_until requests:  51%|█████     | 42/83 [05:18<04:07,  6.03s/it]Running generate_until requests:  52%|█████▏    | 43/83 [05:25<04:22,  6.57s/it]Running generate_until requests:  53%|█████▎    | 44/83 [05:30<03:58,  6.10s/it]Running generate_until requests:  54%|█████▍    | 45/83 [05:37<03:54,  6.18s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:43<03:53,  6.31s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:49<03:43,  6.19s/it]Running generate_until requests:  58%|█████▊    | 48/83 [05:57<03:48,  6.54s/it]Running generate_until requests:  59%|█████▉    | 49/83 [06:01<03:18,  5.83s/it]Running generate_until requests:  60%|██████    | 50/83 [06:09<03:39,  6.66s/it]Running generate_until requests:  61%|██████▏   | 51/83 [06:16<03:36,  6.76s/it]Running generate_until requests:  63%|██████▎   | 52/83 [06:22<03:20,  6.48s/it]Running generate_until requests:  64%|██████▍   | 53/83 [06:30<03:28,  6.94s/it]Running generate_until requests:  65%|██████▌   | 54/83 [06:35<03:01,  6.25s/it]Running generate_until requests:  66%|██████▋   | 55/83 [06:40<02:45,  5.90s/it]Running generate_until requests:  67%|██████▋   | 56/83 [06:55<03:51,  8.58s/it]Running generate_until requests:  69%|██████▊   | 57/83 [07:04<03:47,  8.77s/it]Running generate_until requests:  70%|██████▉   | 58/83 [07:11<03:22,  8.11s/it]Running generate_until requests:  71%|███████   | 59/83 [07:16<02:53,  7.24s/it]Running generate_until requests:  72%|███████▏  | 60/83 [07:21<02:30,  6.54s/it]Running generate_until requests:  73%|███████▎  | 61/83 [07:28<02:31,  6.88s/it]Running generate_until requests:  75%|███████▍  | 62/83 [07:34<02:16,  6.49s/it]Running generate_until requests:  76%|███████▌  | 63/83 [07:39<02:03,  6.19s/it]Running generate_until requests:  77%|███████▋  | 64/83 [07:46<02:00,  6.36s/it]Running generate_until requests:  78%|███████▊  | 65/83 [07:52<01:50,  6.13s/it]Running generate_until requests:  80%|███████▉  | 66/83 [07:58<01:45,  6.19s/it]Running generate_until requests:  81%|████████  | 67/83 [08:02<01:29,  5.62s/it]Running generate_until requests:  82%|████████▏ | 68/83 [08:08<01:22,  5.50s/it]Running generate_until requests:  83%|████████▎ | 69/83 [08:16<01:29,  6.36s/it]Running generate_until requests:  84%|████████▍ | 70/83 [08:24<01:28,  6.83s/it]Running generate_until requests:  86%|████████▌ | 71/83 [08:28<01:11,  5.98s/it]Running generate_until requests:  87%|████████▋ | 72/83 [08:34<01:06,  6.06s/it]Running generate_until requests:  88%|████████▊ | 73/83 [08:40<00:59,  5.93s/it]Running generate_until requests:  89%|████████▉ | 74/83 [08:46<00:52,  5.88s/it]Running generate_until requests:  90%|█████████ | 75/83 [08:55<00:54,  6.84s/it]Running generate_until requests:  92%|█████████▏| 76/83 [09:03<00:50,  7.19s/it]Running generate_until requests:  93%|█████████▎| 77/83 [09:08<00:39,  6.52s/it]Running generate_until requests:  94%|█████████▍| 78/83 [09:13<00:30,  6.14s/it]Running generate_until requests:  95%|█████████▌| 79/83 [09:21<00:27,  6.88s/it]Running generate_until requests:  96%|█████████▋| 80/83 [09:29<00:20,  6.96s/it]Running generate_until requests:  98%|█████████▊| 81/83 [09:36<00:14,  7.03s/it]Running generate_until requests:  99%|█████████▉| 82/83 [09:40<00:06,  6.30s/it]Running generate_until requests: 100%|██████████| 83/83 [09:45<00:00,  5.67s/it]Running generate_until requests: 100%|██████████| 83/83 [09:45<00:00,  7.05s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:03:48:17,945 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:48:17,986 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:48:18,271 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:48:18,332 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:48:18,442 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:48:18,484 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:48:18,487 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:48:18,585 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:48:23,047 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:48:23,048 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:48:23,052 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:48:23,052 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
2024-06-04:03:48:23,362 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:48:23,363 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:48:23,367 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:48:23,367 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
2024-06-04:03:48:23,665 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:48:23,666 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:48:23,670 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:48:23,670 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:48:25,670 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:48:25,672 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:48:25,680 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:48:25,680 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
2024-06-04:03:48:25,888 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:48:25,890 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:48:25,896 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:48:25,896 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
2024-06-04:03:48:26,046 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:48:26,047 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:48:26,053 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:48:26,053 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
2024-06-04:03:48:26,136 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:48:26,138 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:48:26,144 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:48:26,144 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
2024-06-04:03:48:26,192 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:48:26,193 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:48:26,199 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:48:26,199 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.8, 'check': False}
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.13s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.27s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.15s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.11s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.03s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.82s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.91s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.15s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.33s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.67s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.86s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.87s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.26s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:05,  3.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:01,  1.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.80s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.32s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.32s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  1.99s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.34s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.38s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:49:18,148 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:49:18,221 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:49:18,222 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:49:18,437 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 23%|██▎       | 19/83 [00:00<00:00, 186.84it/s] 47%|████▋     | 39/83 [00:00<00:00, 191.21it/s] 71%|███████   | 59/83 [00:00<00:00, 192.94it/s] 95%|█████████▌| 79/83 [00:00<00:00, 194.15it/s]100%|██████████| 83/83 [00:00<00:00, 193.06it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:49:25,328 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:49:25,330 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:49:25,673 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 132.64it/s] 34%|███▎      | 28/83 [00:00<00:00, 135.14it/s] 51%|█████     | 42/83 [00:00<00:00, 136.09it/s] 67%|██████▋   | 56/83 [00:00<00:00, 136.48it/s] 84%|████████▍ | 70/83 [00:00<00:00, 136.61it/s]100%|██████████| 83/83 [00:00<00:00, 136.24it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:49:49,562 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:49:49,564 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:49:49,852 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 138.09it/s] 34%|███▍      | 28/82 [00:00<00:00, 138.70it/s] 51%|█████     | 42/82 [00:00<00:00, 138.89it/s] 68%|██████▊   | 56/82 [00:00<00:00, 138.87it/s] 85%|████████▌ | 70/82 [00:00<00:00, 138.95it/s]100%|██████████| 82/82 [00:00<00:00, 138.89it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:49:52,415 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:49:52,417 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:49:52,697 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 145.93it/s] 36%|███▌      | 30/83 [00:00<00:00, 146.68it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 54%|█████▍    | 45/83 [00:00<00:00, 147.70it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:49:53,066 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:49:53,068 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 73%|███████▎  | 61/83 [00:00<00:00, 151.14it/s] 93%|█████████▎| 77/83 [00:00<00:00, 142.40it/s]100%|██████████| 83/83 [00:00<00:00, 143.88it/s]
2024-06-04:03:49:53,392 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 132.87it/s] 34%|███▍      | 28/82 [00:00<00:00, 124.14it/s] 50%|█████     | 41/82 [00:00<00:00, 118.60it/s] 65%|██████▍   | 53/82 [00:00<00:00, 105.08it/s] 78%|███████▊  | 64/82 [00:00<00:00, 98.70it/s]  90%|█████████ | 74/82 [00:00<00:00, 95.13it/s]100%|██████████| 82/82 [00:00<00:00, 101.82it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:49:55,316 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:49:55,319 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:49:55,910 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 10%|▉         | 8/83 [00:00<00:01, 72.10it/s] 20%|██        | 17/83 [00:00<00:00, 77.95it/s] 31%|███▏      | 26/83 [00:00<00:00, 80.18it/s] 42%|████▏     | 35/83 [00:00<00:00, 83.05it/s] 53%|█████▎    | 44/83 [00:00<00:00, 76.25it/s] 63%|██████▎   | 52/83 [00:00<00:00, 70.75it/s] 72%|███████▏  | 60/83 [00:00<00:00, 69.17it/s] 81%|████████  | 67/83 [00:00<00:00, 65.23it/s] 89%|████████▉ | 74/83 [00:01<00:00, 61.95it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 99%|█████████▉| 82/83 [00:01<00:00, 66.64it/s]100%|██████████| 83/83 [00:01<00:00, 70.20it/s]
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:49:57,191 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:49:57,193 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:49:57,633 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 104.21it/s] 27%|██▋       | 22/82 [00:00<00:00, 104.59it/s] 40%|████      | 33/82 [00:00<00:00, 104.95it/s] 54%|█████▎    | 44/82 [00:00<00:00, 105.00it/s] 67%|██████▋   | 55/82 [00:00<00:00, 105.16it/s] 80%|████████  | 66/82 [00:00<00:00, 105.17it/s] 94%|█████████▍| 77/82 [00:00<00:00, 105.24it/s]100%|██████████| 82/82 [00:00<00:00, 104.97it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:49:59,486 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:49:59,489 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:49:59,934 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 102.26it/s] 27%|██▋       | 22/82 [00:00<00:00, 103.11it/s] 40%|████      | 33/82 [00:00<00:00, 103.93it/s] 54%|█████▎    | 44/82 [00:00<00:00, 104.04it/s] 67%|██████▋   | 55/82 [00:00<00:00, 104.06it/s] 80%|████████  | 66/82 [00:00<00:00, 104.31it/s] 94%|█████████▍| 77/82 [00:00<00:00, 104.48it/s]100%|██████████| 82/82 [00:00<00:00, 104.12it/s]
2024-06-04:03:50:11,841 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:50:11,841 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:50:11,841 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:50:11,841 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:50:11,841 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:50:11,841 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:50:11,841 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:50:11,848 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:13<17:53, 13.09s/it]Running generate_until requests:   2%|▏         | 2/83 [00:25<17:15, 12.78s/it]Running generate_until requests:   4%|▎         | 3/83 [00:34<14:44, 11.06s/it]Running generate_until requests:   5%|▍         | 4/83 [00:39<11:23,  8.65s/it]Running generate_until requests:   6%|▌         | 5/83 [00:46<10:22,  7.98s/it]Running generate_until requests:   7%|▋         | 6/83 [00:51<09:08,  7.13s/it]Running generate_until requests:   8%|▊         | 7/83 [01:00<09:27,  7.47s/it]Running generate_until requests:  10%|▉         | 8/83 [01:06<09:02,  7.23s/it]Running generate_until requests:  11%|█         | 9/83 [01:15<09:31,  7.72s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:20<08:27,  6.96s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:27<08:15,  6.89s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:37<09:07,  7.71s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:40<07:29,  6.42s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:47<07:26,  6.47s/it]Running generate_until requests:  18%|█▊        | 15/83 [01:58<08:52,  7.83s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:09<09:50,  8.81s/it]Running generate_until requests:  20%|██        | 17/83 [02:17<09:25,  8.56s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:24<08:49,  8.14s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:32<08:47,  8.25s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:43<09:17,  8.84s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:47<07:54,  7.65s/it]Running generate_until requests:  27%|██▋       | 22/83 [02:54<07:21,  7.24s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:00<06:53,  6.89s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:06<06:35,  6.70s/it]Running generate_until requests:  30%|███       | 25/83 [03:14<06:43,  6.96s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:20<06:26,  6.78s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:27<06:17,  6.75s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:33<06:03,  6.60s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:39<05:43,  6.37s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:48<06:26,  7.29s/it]Running generate_until requests:  37%|███▋      | 31/83 [03:55<06:12,  7.16s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:02<06:03,  7.12s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:07<05:15,  6.30s/it]Running generate_until requests:  41%|████      | 34/83 [04:16<05:58,  7.31s/it]Running generate_until requests:  42%|████▏     | 35/83 [04:22<05:29,  6.86s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:28<05:06,  6.52s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:34<05:02,  6.58s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:46<06:01,  8.03s/it]Running generate_until requests:  47%|████▋     | 39/83 [04:51<05:16,  7.20s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:01<05:38,  7.87s/it]Running generate_until requests:  49%|████▉     | 41/83 [05:05<04:50,  6.92s/it]Running generate_until requests:  51%|█████     | 42/83 [05:11<04:29,  6.56s/it]Running generate_until requests:  52%|█████▏    | 43/83 [05:18<04:33,  6.85s/it]Running generate_until requests:  53%|█████▎    | 44/83 [05:24<04:06,  6.33s/it]Running generate_until requests:  54%|█████▍    | 45/83 [05:30<04:01,  6.36s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:36<03:51,  6.26s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:41<03:27,  5.76s/it]Running generate_until requests:  58%|█████▊    | 48/83 [05:48<03:39,  6.27s/it]Running generate_until requests:  59%|█████▉    | 49/83 [05:52<03:11,  5.62s/it]Running generate_until requests:  60%|██████    | 50/83 [06:00<03:25,  6.24s/it]Running generate_until requests:  61%|██████▏   | 51/83 [06:06<03:22,  6.32s/it]Running generate_until requests:  63%|██████▎   | 52/83 [06:14<03:27,  6.69s/it]Running generate_until requests:  64%|██████▍   | 53/83 [06:21<03:25,  6.85s/it]Running generate_until requests:  65%|██████▌   | 54/83 [06:26<03:00,  6.24s/it]Running generate_until requests:  66%|██████▋   | 55/83 [06:32<02:52,  6.15s/it]Running generate_until requests:  67%|██████▋   | 56/83 [06:46<03:52,  8.60s/it]Running generate_until requests:  69%|██████▊   | 57/83 [06:57<03:59,  9.22s/it]Running generate_until requests:  70%|██████▉   | 58/83 [07:04<03:34,  8.57s/it]Running generate_until requests:  71%|███████   | 59/83 [07:09<03:01,  7.55s/it]Running generate_until requests:  72%|███████▏  | 60/83 [07:14<02:35,  6.77s/it]Running generate_until requests:  73%|███████▎  | 61/83 [07:22<02:34,  7.01s/it]Running generate_until requests:  75%|███████▍  | 62/83 [07:27<02:17,  6.55s/it]Running generate_until requests:  76%|███████▌  | 63/83 [07:33<02:03,  6.20s/it]Running generate_until requests:  77%|███████▋  | 64/83 [07:38<01:55,  6.10s/it]Running generate_until requests:  78%|███████▊  | 65/83 [07:44<01:46,  5.93s/it]Running generate_until requests:  80%|███████▉  | 66/83 [07:48<01:32,  5.44s/it]Running generate_until requests:  81%|████████  | 67/83 [07:53<01:22,  5.16s/it]Running generate_until requests:  82%|████████▏ | 68/83 [07:58<01:17,  5.15s/it]Running generate_until requests:  83%|████████▎ | 69/83 [08:08<01:34,  6.76s/it]Running generate_until requests:  84%|████████▍ | 70/83 [08:16<01:31,  7.02s/it]Running generate_until requests:  86%|████████▌ | 71/83 [08:20<01:13,  6.13s/it]Running generate_until requests:  87%|████████▋ | 72/83 [08:26<01:08,  6.21s/it]Running generate_until requests:  88%|████████▊ | 73/83 [08:32<00:59,  5.99s/it]Running generate_until requests:  89%|████████▉ | 74/83 [08:38<00:53,  5.92s/it]Running generate_until requests:  90%|█████████ | 75/83 [08:47<00:54,  6.85s/it]Running generate_until requests:  92%|█████████▏| 76/83 [08:54<00:48,  6.90s/it]Running generate_until requests:  93%|█████████▎| 77/83 [08:59<00:37,  6.30s/it]Running generate_until requests:  94%|█████████▍| 78/83 [09:03<00:28,  5.75s/it]Running generate_until requests:  95%|█████████▌| 79/83 [09:12<00:26,  6.60s/it]Running generate_until requests:  96%|█████████▋| 80/83 [09:17<00:18,  6.33s/it]Running generate_until requests:  98%|█████████▊| 81/83 [09:24<00:13,  6.56s/it]Running generate_until requests:  99%|█████████▉| 82/83 [09:30<00:06,  6.15s/it]Running generate_until requests: 100%|██████████| 83/83 [09:34<00:00,  5.69s/it]Running generate_until requests: 100%|██████████| 83/83 [09:34<00:00,  6.92s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:04:03:27,764 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:03:27,764 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:03:27,818 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:03:27,991 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:03:28,150 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:03:28,264 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:03:28,460 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:03:28,929 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:04:03:32,695 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:03:32,697 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:03:32,703 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:03:32,703 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': False}
2024-06-04:04:03:33,040 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:03:33,041 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:03:33,045 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:03:33,045 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': False}
2024-06-04:04:03:33,088 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:03:33,089 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:03:33,093 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:03:33,093 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:04:03:34,947 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:03:34,948 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:03:34,954 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:03:34,954 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': False}
2024-06-04:04:03:34,999 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:03:35,002 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:03:35,012 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:03:35,012 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': False}
2024-06-04:04:03:35,591 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:03:35,592 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:03:35,597 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:03:35,597 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': False}
2024-06-04:04:03:35,872 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:03:35,873 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:03:35,877 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:03:35,877 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': False}
2024-06-04:04:03:36,025 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:04:03:36,027 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:04:03:36,031 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:04:03:36,031 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.9, 'check': False}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.22s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.24s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.40s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.13s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:05,  2.55s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.87s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.40s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.01s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.86s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:01,  2.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.67s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.68s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.09s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.96s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.23s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.25s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.23s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.22s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.16s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:04:23,647 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:04:23,649 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:04:23,885 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:04:23,887 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:04:23,969 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 16%|█▌        | 13/83 [00:00<00:00, 125.43it/s] 31%|███▏      | 26/83 [00:00<00:00, 126.58it/s]2024-06-04:04:04:24,203 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 48%|████▊     | 40/83 [00:00<00:00, 128.91it/s] 17%|█▋        | 14/82 [00:00<00:00, 132.62it/s] 65%|██████▌   | 54/83 [00:00<00:00, 130.65it/s] 34%|███▍      | 28/82 [00:00<00:00, 132.70it/s] 82%|████████▏ | 68/83 [00:00<00:00, 131.51it/s] 51%|█████     | 42/82 [00:00<00:00, 133.01it/s] 99%|█████████▉| 82/83 [00:00<00:00, 132.32it/s]100%|██████████| 83/83 [00:00<00:00, 130.76it/s]
 70%|██████▉   | 57/82 [00:00<00:00, 137.36it/s] 94%|█████████▍| 77/82 [00:00<00:00, 158.60it/s]100%|██████████| 82/82 [00:00<00:00, 150.00it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:04:04:41,583 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:04:41,661 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:04:41,663 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:04:41,827 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 206.75it/s] 51%|█████     | 42/83 [00:00<00:00, 208.14it/s] 76%|███████▌  | 63/83 [00:00<00:00, 208.68it/s]100%|██████████| 83/83 [00:00<00:00, 208.83it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:04:44,748 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:04:44,751 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:04:45,144 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 13%|█▎        | 11/82 [00:00<00:00, 102.62it/s] 27%|██▋       | 22/82 [00:00<00:00, 103.04it/s] 40%|████      | 33/82 [00:00<00:00, 103.32it/s] 54%|█████▎    | 44/82 [00:00<00:00, 103.88it/s] 67%|██████▋   | 55/82 [00:00<00:00, 103.74it/s] 80%|████████  | 66/82 [00:00<00:00, 103.56it/s] 94%|█████████▍| 77/82 [00:00<00:00, 103.50it/s]100%|██████████| 82/82 [00:00<00:00, 103.45it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:05:01,351 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:05:01,353 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:05:01,634 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 16%|█▌        | 13/83 [00:00<00:00, 122.08it/s] 35%|███▍      | 29/83 [00:00<00:00, 140.20it/s] 53%|█████▎    | 44/83 [00:00<00:00, 137.68it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 70%|██████▉   | 58/83 [00:00<00:00, 135.62it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:05:02,084 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:05:02,086 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 88%|████████▊ | 73/83 [00:00<00:00, 138.70it/s]100%|██████████| 83/83 [00:00<00:00, 138.43it/s]
2024-06-04:04:05:02,358 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:05:02,485 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:05:02,488 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 17%|█▋        | 14/82 [00:00<00:00, 139.20it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:04:05:02,607 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:04:05:02,609 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 34%|███▍      | 28/82 [00:00<00:00, 134.37it/s] 51%|█████     | 42/82 [00:00<00:00, 83.57it/s] 2024-06-04:04:05:02,886 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 63%|██████▎   | 52/82 [00:00<00:00, 78.27it/s] 11%|█         | 9/83 [00:00<00:00, 87.07it/s] 22%|██▏       | 18/83 [00:00<00:00, 87.79it/s] 74%|███████▍  | 61/82 [00:00<00:00, 67.25it/s] 33%|███▎      | 27/83 [00:00<00:00, 88.68it/s]2024-06-04:04:05:03,253 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
 89%|████████▉ | 73/82 [00:00<00:00, 79.29it/s]  0%|          | 0/82 [00:00<?, ?it/s] 46%|████▌     | 38/83 [00:00<00:00, 94.73it/s]100%|██████████| 82/82 [00:00<00:00, 85.82it/s]
 15%|█▍        | 12/82 [00:00<00:00, 113.87it/s] 60%|██████    | 50/83 [00:00<00:00, 100.47it/s] 29%|██▉       | 24/82 [00:00<00:00, 103.03it/s] 75%|███████▍  | 62/83 [00:00<00:00, 104.18it/s] 43%|████▎     | 35/82 [00:00<00:00, 96.71it/s]  89%|████████▉ | 74/83 [00:00<00:00, 106.39it/s]100%|██████████| 83/83 [00:00<00:00, 101.50it/s]
 56%|█████▌    | 46/82 [00:00<00:00, 98.96it/s] 70%|██████▉   | 57/82 [00:00<00:00, 101.33it/s] 83%|████████▎ | 68/82 [00:00<00:00, 103.06it/s] 96%|█████████▋| 79/82 [00:00<00:00, 104.18it/s]100%|██████████| 82/82 [00:00<00:00, 102.74it/s]
2024-06-04:04:05:15,256 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:05:15,256 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:05:15,256 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:05:15,256 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:05:15,256 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:05:15,256 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:05:15,256 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:04:05:15,256 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:10<14:13, 10.41s/it]Running generate_until requests:   2%|▏         | 2/83 [00:23<16:15, 12.05s/it]Running generate_until requests:   4%|▎         | 3/83 [00:31<13:16,  9.96s/it]Running generate_until requests:   5%|▍         | 4/83 [00:35<10:22,  7.88s/it]Running generate_until requests:   6%|▌         | 5/83 [00:42<09:37,  7.41s/it]Running generate_until requests:   7%|▋         | 6/83 [00:47<08:39,  6.74s/it]Running generate_until requests:   8%|▊         | 7/83 [00:58<10:04,  7.95s/it]Running generate_until requests:  10%|▉         | 8/83 [01:04<09:25,  7.54s/it]Running generate_until requests:  11%|█         | 9/83 [01:13<09:45,  7.91s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:19<08:43,  7.17s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:25<08:22,  6.98s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:35<09:11,  7.77s/it]Running generate_until requests:  16%|█▌        | 13/83 [01:41<08:23,  7.20s/it]Running generate_until requests:  17%|█▋        | 14/83 [01:47<08:00,  6.96s/it]Running generate_until requests:  18%|█▊        | 15/83 [01:56<08:42,  7.68s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:08<09:47,  8.76s/it]Running generate_until requests:  20%|██        | 17/83 [02:16<09:19,  8.48s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:23<08:42,  8.04s/it]Running generate_until requests:  23%|██▎       | 19/83 [02:29<08:06,  7.60s/it]Running generate_until requests:  24%|██▍       | 20/83 [02:40<09:00,  8.58s/it]Running generate_until requests:  25%|██▌       | 21/83 [02:44<07:30,  7.27s/it]Running generate_until requests:  27%|██▋       | 22/83 [02:51<07:06,  7.00s/it]Running generate_until requests:  28%|██▊       | 23/83 [02:56<06:40,  6.67s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:03<06:24,  6.52s/it]Running generate_until requests:  30%|███       | 25/83 [03:11<06:55,  7.16s/it]Running generate_until requests:  31%|███▏      | 26/83 [03:18<06:33,  6.90s/it]Running generate_until requests:  33%|███▎      | 27/83 [03:25<06:31,  6.99s/it]Running generate_until requests:  34%|███▎      | 28/83 [03:31<06:12,  6.77s/it]Running generate_until requests:  35%|███▍      | 29/83 [03:38<06:10,  6.86s/it]Running generate_until requests:  36%|███▌      | 30/83 [03:46<06:24,  7.25s/it]Running generate_until requests:  37%|███▋      | 31/83 [03:52<05:57,  6.88s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:00<06:05,  7.16s/it]Running generate_until requests:  40%|███▉      | 33/83 [04:05<05:17,  6.35s/it]Running generate_until requests:  41%|████      | 34/83 [04:13<05:35,  6.85s/it]Running generate_until requests:  42%|████▏     | 35/83 [04:18<05:11,  6.50s/it]Running generate_until requests:  43%|████▎     | 36/83 [04:25<05:03,  6.46s/it]Running generate_until requests:  45%|████▍     | 37/83 [04:31<04:54,  6.40s/it]Running generate_until requests:  46%|████▌     | 38/83 [04:40<05:20,  7.13s/it]Running generate_until requests:  47%|████▋     | 39/83 [04:45<04:54,  6.69s/it]Running generate_until requests:  48%|████▊     | 40/83 [04:55<05:20,  7.45s/it]Running generate_until requests:  49%|████▉     | 41/83 [04:59<04:37,  6.61s/it]Running generate_until requests:  51%|█████     | 42/83 [05:06<04:29,  6.58s/it]Running generate_until requests:  52%|█████▏    | 43/83 [05:13<04:34,  6.87s/it]Running generate_until requests:  53%|█████▎    | 44/83 [05:18<04:05,  6.29s/it]Running generate_until requests:  54%|█████▍    | 45/83 [05:24<03:55,  6.20s/it]Running generate_until requests:  55%|█████▌    | 46/83 [05:30<03:47,  6.16s/it]Running generate_until requests:  57%|█████▋    | 47/83 [05:35<03:23,  5.64s/it]Running generate_until requests:  58%|█████▊    | 48/83 [05:42<03:32,  6.06s/it]Running generate_until requests:  59%|█████▉    | 49/83 [05:46<03:06,  5.48s/it]Running generate_until requests:  60%|██████    | 50/83 [05:54<03:29,  6.35s/it]Running generate_until requests:  61%|██████▏   | 51/83 [06:00<03:18,  6.21s/it]Running generate_until requests:  63%|██████▎   | 52/83 [06:05<02:55,  5.65s/it]Running generate_until requests:  64%|██████▍   | 53/83 [06:11<02:59,  5.97s/it]Running generate_until requests:  65%|██████▌   | 54/83 [06:16<02:42,  5.62s/it]Running generate_until requests:  66%|██████▋   | 55/83 [06:21<02:33,  5.49s/it]Running generate_until requests:  67%|██████▋   | 56/83 [06:35<03:36,  8.00s/it]Running generate_until requests:  69%|██████▊   | 57/83 [06:44<03:37,  8.37s/it]Running generate_until requests:  70%|██████▉   | 58/83 [06:51<03:19,  7.97s/it]Running generate_until requests:  71%|███████   | 59/83 [06:56<02:50,  7.11s/it]Running generate_until requests:  72%|███████▏  | 60/83 [07:01<02:28,  6.47s/it]Running generate_until requests:  73%|███████▎  | 61/83 [07:09<02:28,  6.74s/it]Running generate_until requests:  75%|███████▍  | 62/83 [07:13<02:07,  6.09s/it]Running generate_until requests:  76%|███████▌  | 63/83 [07:19<01:56,  5.83s/it]Running generate_until requests:  77%|███████▋  | 64/83 [07:27<02:04,  6.53s/it]Running generate_until requests:  78%|███████▊  | 65/83 [07:32<01:50,  6.14s/it]Running generate_until requests:  80%|███████▉  | 66/83 [07:36<01:34,  5.57s/it]Running generate_until requests:  81%|████████  | 67/83 [07:41<01:23,  5.22s/it]Running generate_until requests:  82%|████████▏ | 68/83 [07:46<01:17,  5.17s/it]Running generate_until requests:  83%|████████▎ | 69/83 [07:56<01:35,  6.84s/it]Running generate_until requests:  84%|████████▍ | 70/83 [08:04<01:31,  7.01s/it]Running generate_until requests:  86%|████████▌ | 71/83 [08:08<01:13,  6.09s/it]Running generate_until requests:  87%|████████▋ | 72/83 [08:12<01:00,  5.47s/it]Running generate_until requests:  88%|████████▊ | 73/83 [08:17<00:54,  5.45s/it]Running generate_until requests:  89%|████████▉ | 74/83 [08:22<00:48,  5.34s/it]Running generate_until requests:  90%|█████████ | 75/83 [08:31<00:50,  6.37s/it]Running generate_until requests:  92%|█████████▏| 76/83 [08:38<00:45,  6.55s/it]Running generate_until requests:  93%|█████████▎| 77/83 [08:43<00:36,  6.04s/it]Running generate_until requests:  94%|█████████▍| 78/83 [08:48<00:29,  5.80s/it]Running generate_until requests:  95%|█████████▌| 79/83 [08:57<00:26,  6.74s/it]Running generate_until requests:  96%|█████████▋| 80/83 [09:03<00:19,  6.43s/it]Running generate_until requests:  98%|█████████▊| 81/83 [09:10<00:13,  6.59s/it]Running generate_until requests:  99%|█████████▉| 82/83 [09:15<00:06,  6.12s/it]Running generate_until requests: 100%|██████████| 83/83 [09:20<00:00,  5.76s/it]Running generate_until requests: 100%|██████████| 83/83 [09:20<00:00,  6.75s/it]
