ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/data/home/beidic/anaconda3/envs/yangllm2/bin/transformers-cli'

wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.95s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.32s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.34s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.59s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:49<00:49, 49.20s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:48<00:48, 48.82s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:21<00:21, 21.71s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:22<00:22, 22.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 26.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.05s/it]
Some weights of LlamaWeirdLargeTest were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['addonsmallmodel.layers.1.self_attn.q_proj.weight', 'addonsmallmodel.layers.11.input_layernorm.weight', 'addonsmallmodel.layers.8.self_attn.o_proj.weight', 'addonsmallmodel.layers.11.mlp.down_proj.weight', 'addonsmallmodel.layers.1.mlp.down_proj.weight', 'addonsmallmodel.layers.2.self_attn.q_proj.weight', 'addonsmallmodel.layers.4.mlp.up_proj.weight', 'addonsmallmodel.layers.11.self_attn.v_proj.weight', 'addonsmallmodel.layers.10.self_attn.k_proj.weight', 'addonsmallmodel.layers.7.post_attention_layernorm.weight', 'addonsmallmodel.layers.1.self_attn.k_proj.weight', 'addonsmallmodel.layers.3.mlp.gate_proj.weight', 'addonsmallmodel.embed_projection.weight', 'addonsmallmodel.layers.8.post_attention_layernorm.weight', 'addonsmallmodel.layers.11.mlp.up_proj.weight', 'addonsmallmodel.layers.7.self_attn.v_proj.weight', 'addonsmallmodel.layers.11.mlp.gate_proj.weight', 'addonsmallmodel.layers.6.mlp.up_proj.weight', 'addonsmallmodel.layers.6.mlp.down_proj.weight', 'addonsmallmodel.layers.7.input_layernorm.weight', 'addonsmallmodel.layers.9.self_attn.o_proj.weight', 'addonsmallmodel.layers.0.self_attn.k_proj.weight', 'addonsmallmodel.layers.9.input_layernorm.weight', 'addonsmallmodel.layers.3.self_attn.v_proj.weight', 'addonsmallmodel.layers.10.self_attn.v_proj.weight', 'addonsmallmodel.layers.6.post_attention_layernorm.weight', 'addonsmallmodel.layers.2.post_attention_layernorm.weight', 'addonsmallmodel.layers.2.mlp.down_proj.weight', 'addonsmallmodel.layers.5.self_attn.k_proj.weight', 'addonsmallmodel.layers.7.mlp.down_proj.weight', 'addonsmallmodel.layers.2.self_attn.k_proj.weight', 'addonsmallmodel.layers.2.self_attn.o_proj.weight', 'addonsmallmodel.layers.3.input_layernorm.weight', 'addonsmallmodel.layers.7.mlp.up_proj.weight', 'addonsmallmodel.layers.3.mlp.down_proj.weight', 'addonsmallmodel.layers.7.self_attn.q_proj.weight', 'addonsmallmodel.layers.4.self_attn.k_proj.weight', 'addonsmallmodel.layers.0.mlp.down_proj.weight', 'addonsmallmodel.layers.8.self_attn.q_proj.weight', 'addonsmallmodel.layers.2.mlp.up_proj.weight', 'addonsmallmodel.layers.5.self_attn.v_proj.weight', 'addonsmallmodel.layers.4.self_attn.q_proj.weight', 'addonsmallmodel.layers.9.mlp.gate_proj.weight', 'addonsmallmodel.layers.9.post_attention_layernorm.weight', 'addonsmallmodel.layers.8.self_attn.v_proj.weight', 'addonsmallmodel.layers.3.self_attn.o_proj.weight', 'addonsmallmodel.layers.1.mlp.up_proj.weight', 'addonsmallmodel.embed_tokens.weight', 'addonsmallmodel.layers.9.mlp.up_proj.weight', 'addonsmallmodel.layers.7.mlp.gate_proj.weight', 'addonsmallmodel.layers.10.post_attention_layernorm.weight', 'addonsmallmodel.layers.4.mlp.down_proj.weight', 'addonsmallmodel.layers.8.self_attn.k_proj.weight', 'addonsmallmodel.layers.8.input_layernorm.weight', 'addonsmallmodel.layers.10.self_attn.q_proj.weight', 'addonsmallmodel.layers.0.self_attn.o_proj.weight', 'addonsmallmodel.layers.3.post_attention_layernorm.weight', 'addonsmallmodel.layers.6.self_attn.k_proj.weight', 'addonsmallmodel.layers.1.input_layernorm.weight', 'addonsmallmodel.layers.8.mlp.down_proj.weight', 'addonsmallmodel.layers.9.self_attn.k_proj.weight', 'addonsmallmodel.layers.11.self_attn.q_proj.weight', 'addonsmallmodel.layers.9.self_attn.q_proj.weight', 'addonsmallmodel.layers.0.self_attn.v_proj.weight', 'addonsmallmodel.layers.0.self_attn.q_proj.weight', 'addonsmallmodel.layers.8.mlp.up_proj.weight', 'addonsmallmodel.layers.1.post_attention_layernorm.weight', 'addonsmallmodel.layers.1.mlp.gate_proj.weight', 'addonsmallmodel.layers.9.self_attn.v_proj.weight', 'addonsmallmodel.layers.6.self_attn.q_proj.weight', 'addonsmallmodel.layers.10.mlp.gate_proj.weight', 'addonsmallmodel.layers.5.mlp.down_proj.weight', 'addonsmallmodel.layers.10.mlp.down_proj.weight', 'addonsmallmodel.lm_head.weight', 'addonsmallmodel.layers.10.input_layernorm.weight', 'addonsmallmodel.layers.2.mlp.gate_proj.weight', 'addonsmallmodel.layers.7.self_attn.k_proj.weight', 'addonsmallmodel.layers.5.input_layernorm.weight', 'addonsmallmodel.layers.11.self_attn.o_proj.weight', 'addonsmallmodel.norm.weight', 'addonsmallmodel.layers.6.self_attn.v_proj.weight', 'addonsmallmodel.layers.5.mlp.up_proj.weight', 'addonsmallmodel.layers.2.self_attn.v_proj.weight', 'addonsmallmodel.layers.11.self_attn.k_proj.weight', 'addonsmallmodel.layers.3.self_attn.q_proj.weight', 'addonsmallmodel.layers.0.mlp.gate_proj.weight', 'addonsmallmodel.layers.6.mlp.gate_proj.weight', 'addonsmallmodel.layers.0.post_attention_layernorm.weight', 'addonsmallmodel.layers.4.self_attn.v_proj.weight', 'addonsmallmodel.layers.8.mlp.gate_proj.weight', 'addonsmallmodel.layers.1.self_attn.o_proj.weight', 'addonsmallmodel.layers.11.post_attention_layernorm.weight', 'addonsmallmodel.layers.10.self_attn.o_proj.weight', 'addonsmallmodel.layers.9.mlp.down_proj.weight', 'addonsmallmodel.layers.5.self_attn.q_proj.weight', 'addonsmallmodel.layers.2.input_layernorm.weight', 'addonsmallmodel.layers.10.mlp.up_proj.weight', 'addonsmallmodel.layers.3.self_attn.k_proj.weight', 'addonsmallmodel.layers.0.input_layernorm.weight', 'addonsmallmodel.layers.4.mlp.gate_proj.weight', 'addonsmallmodel.layers.4.input_layernorm.weight', 'addonsmallmodel.layers.1.self_attn.v_proj.weight', 'addonsmallmodel.layers.5.mlp.gate_proj.weight', 'addonsmallmodel.layers.5.post_attention_layernorm.weight', 'addonsmallmodel.layers.4.self_attn.o_proj.weight', 'addonsmallmodel.layers.6.input_layernorm.weight', 'addonsmallmodel.layers.7.self_attn.o_proj.weight', 'addonsmallmodel.layers.0.mlp.up_proj.weight', 'addonsmallmodel.layers.6.self_attn.o_proj.weight', 'addonsmallmodel.layers.3.mlp.up_proj.weight', 'addonsmallmodel.layers.4.post_attention_layernorm.weight', 'addonsmallmodel.layers.5.self_attn.o_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 27.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.58s/it]
Some weights of LlamaWeirdLargeTest were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['addonsmallmodel.layers.0.input_layernorm.weight', 'addonsmallmodel.layers.4.mlp.down_proj.weight', 'addonsmallmodel.layers.11.mlp.down_proj.weight', 'addonsmallmodel.norm.weight', 'addonsmallmodel.layers.8.self_attn.v_proj.weight', 'addonsmallmodel.layers.5.self_attn.v_proj.weight', 'addonsmallmodel.layers.8.self_attn.o_proj.weight', 'addonsmallmodel.layers.9.self_attn.q_proj.weight', 'addonsmallmodel.layers.2.self_attn.o_proj.weight', 'addonsmallmodel.layers.5.mlp.up_proj.weight', 'addonsmallmodel.layers.4.input_layernorm.weight', 'addonsmallmodel.layers.9.post_attention_layernorm.weight', 'addonsmallmodel.layers.2.input_layernorm.weight', 'addonsmallmodel.layers.9.self_attn.o_proj.weight', 'addonsmallmodel.layers.10.input_layernorm.weight', 'addonsmallmodel.layers.1.self_attn.k_proj.weight', 'addonsmallmodel.layers.11.self_attn.k_proj.weight', 'addonsmallmodel.layers.8.input_layernorm.weight', 'addonsmallmodel.embed_tokens.weight', 'addonsmallmodel.layers.0.mlp.up_proj.weight', 'addonsmallmodel.layers.0.post_attention_layernorm.weight', 'addonsmallmodel.layers.5.mlp.down_proj.weight', 'addonsmallmodel.layers.6.self_attn.v_proj.weight', 'addonsmallmodel.layers.6.mlp.gate_proj.weight', 'addonsmallmodel.layers.1.self_attn.q_proj.weight', 'addonsmallmodel.layers.7.mlp.gate_proj.weight', 'addonsmallmodel.layers.6.mlp.down_proj.weight', 'addonsmallmodel.layers.3.mlp.gate_proj.weight', 'addonsmallmodel.layers.5.mlp.gate_proj.weight', 'addonsmallmodel.layers.10.mlp.down_proj.weight', 'addonsmallmodel.layers.0.self_attn.q_proj.weight', 'addonsmallmodel.layers.11.self_attn.v_proj.weight', 'addonsmallmodel.layers.5.self_attn.k_proj.weight', 'addonsmallmodel.layers.3.post_attention_layernorm.weight', 'addonsmallmodel.layers.10.post_attention_layernorm.weight', 'addonsmallmodel.layers.2.mlp.down_proj.weight', 'addonsmallmodel.layers.2.mlp.up_proj.weight', 'addonsmallmodel.layers.8.mlp.down_proj.weight', 'addonsmallmodel.layers.1.mlp.gate_proj.weight', 'addonsmallmodel.layers.6.self_attn.o_proj.weight', 'addonsmallmodel.layers.3.self_attn.v_proj.weight', 'addonsmallmodel.layers.1.mlp.down_proj.weight', 'addonsmallmodel.layers.6.self_attn.k_proj.weight', 'addonsmallmodel.layers.7.mlp.up_proj.weight', 'addonsmallmodel.layers.5.input_layernorm.weight', 'addonsmallmodel.layers.9.self_attn.v_proj.weight', 'addonsmallmodel.layers.3.input_layernorm.weight', 'addonsmallmodel.layers.8.self_attn.k_proj.weight', 'addonsmallmodel.layers.2.self_attn.v_proj.weight', 'addonsmallmodel.layers.7.input_layernorm.weight', 'addonsmallmodel.layers.11.mlp.up_proj.weight', 'addonsmallmodel.layers.7.mlp.down_proj.weight', 'addonsmallmodel.layers.3.self_attn.q_proj.weight', 'addonsmallmodel.layers.6.self_attn.q_proj.weight', 'addonsmallmodel.layers.2.self_attn.k_proj.weight', 'addonsmallmodel.layers.3.mlp.up_proj.weight', 'addonsmallmodel.layers.9.mlp.up_proj.weight', 'addonsmallmodel.layers.8.self_attn.q_proj.weight', 'addonsmallmodel.layers.4.post_attention_layernorm.weight', 'addonsmallmodel.layers.9.self_attn.k_proj.weight', 'addonsmallmodel.layers.4.mlp.gate_proj.weight', 'addonsmallmodel.layers.2.post_attention_layernorm.weight', 'addonsmallmodel.layers.2.mlp.gate_proj.weight', 'addonsmallmodel.layers.1.input_layernorm.weight', 'addonsmallmodel.layers.0.mlp.down_proj.weight', 'addonsmallmodel.layers.1.self_attn.o_proj.weight', 'addonsmallmodel.layers.5.self_attn.q_proj.weight', 'addonsmallmodel.layers.11.post_attention_layernorm.weight', 'addonsmallmodel.layers.7.self_attn.k_proj.weight', 'addonsmallmodel.layers.1.self_attn.v_proj.weight', 'addonsmallmodel.layers.2.self_attn.q_proj.weight', 'addonsmallmodel.layers.0.self_attn.o_proj.weight', 'addonsmallmodel.layers.10.mlp.gate_proj.weight', 'addonsmallmodel.layers.6.post_attention_layernorm.weight', 'addonsmallmodel.layers.1.mlp.up_proj.weight', 'addonsmallmodel.layers.11.input_layernorm.weight', 'addonsmallmodel.layers.7.self_attn.o_proj.weight', 'addonsmallmodel.layers.7.self_attn.q_proj.weight', 'addonsmallmodel.layers.9.mlp.down_proj.weight', 'addonsmallmodel.layers.8.mlp.up_proj.weight', 'addonsmallmodel.layers.7.post_attention_layernorm.weight', 'addonsmallmodel.layers.4.self_attn.v_proj.weight', 'addonsmallmodel.layers.8.post_attention_layernorm.weight', 'addonsmallmodel.layers.4.mlp.up_proj.weight', 'addonsmallmodel.layers.0.self_attn.v_proj.weight', 'addonsmallmodel.layers.3.self_attn.o_proj.weight', 'addonsmallmodel.lm_head.weight', 'addonsmallmodel.layers.3.self_attn.k_proj.weight', 'addonsmallmodel.layers.0.self_attn.k_proj.weight', 'addonsmallmodel.embed_projection.weight', 'addonsmallmodel.layers.10.mlp.up_proj.weight', 'addonsmallmodel.layers.10.self_attn.q_proj.weight', 'addonsmallmodel.layers.6.mlp.up_proj.weight', 'addonsmallmodel.layers.10.self_attn.o_proj.weight', 'addonsmallmodel.layers.3.mlp.down_proj.weight', 'addonsmallmodel.layers.6.input_layernorm.weight', 'addonsmallmodel.layers.11.self_attn.o_proj.weight', 'addonsmallmodel.layers.5.post_attention_layernorm.weight', 'addonsmallmodel.layers.7.self_attn.v_proj.weight', 'addonsmallmodel.layers.10.self_attn.k_proj.weight', 'addonsmallmodel.layers.10.self_attn.v_proj.weight', 'addonsmallmodel.layers.9.input_layernorm.weight', 'addonsmallmodel.layers.11.mlp.gate_proj.weight', 'addonsmallmodel.layers.0.mlp.gate_proj.weight', 'addonsmallmodel.layers.1.post_attention_layernorm.weight', 'addonsmallmodel.layers.8.mlp.gate_proj.weight', 'addonsmallmodel.layers.4.self_attn.q_proj.weight', 'addonsmallmodel.layers.5.self_attn.o_proj.weight', 'addonsmallmodel.layers.9.mlp.gate_proj.weight', 'addonsmallmodel.layers.4.self_attn.k_proj.weight', 'addonsmallmodel.layers.11.self_attn.q_proj.weight', 'addonsmallmodel.layers.4.self_attn.o_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 27.06s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.33s/it]
Some weights of LlamaWeirdLargeTest were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['addonsmallmodel.layers.10.self_attn.o_proj.weight', 'addonsmallmodel.layers.1.mlp.up_proj.weight', 'addonsmallmodel.layers.6.post_attention_layernorm.weight', 'addonsmallmodel.layers.4.mlp.gate_proj.weight', 'addonsmallmodel.embed_tokens.weight', 'addonsmallmodel.layers.9.input_layernorm.weight', 'addonsmallmodel.layers.11.self_attn.v_proj.weight', 'addonsmallmodel.layers.5.mlp.down_proj.weight', 'addonsmallmodel.layers.2.self_attn.o_proj.weight', 'addonsmallmodel.layers.0.self_attn.k_proj.weight', 'addonsmallmodel.layers.5.self_attn.q_proj.weight', 'addonsmallmodel.layers.2.self_attn.k_proj.weight', 'addonsmallmodel.layers.9.mlp.up_proj.weight', 'addonsmallmodel.layers.7.self_attn.k_proj.weight', 'addonsmallmodel.layers.2.input_layernorm.weight', 'addonsmallmodel.layers.3.mlp.down_proj.weight', 'addonsmallmodel.layers.7.mlp.down_proj.weight', 'addonsmallmodel.layers.8.self_attn.k_proj.weight', 'addonsmallmodel.layers.2.self_attn.v_proj.weight', 'addonsmallmodel.layers.1.self_attn.q_proj.weight', 'addonsmallmodel.lm_head.weight', 'addonsmallmodel.layers.3.post_attention_layernorm.weight', 'addonsmallmodel.layers.4.post_attention_layernorm.weight', 'addonsmallmodel.layers.3.self_attn.q_proj.weight', 'addonsmallmodel.layers.0.self_attn.o_proj.weight', 'addonsmallmodel.layers.4.self_attn.k_proj.weight', 'addonsmallmodel.layers.4.input_layernorm.weight', 'addonsmallmodel.layers.5.mlp.gate_proj.weight', 'addonsmallmodel.layers.6.mlp.down_proj.weight', 'addonsmallmodel.layers.2.mlp.up_proj.weight', 'addonsmallmodel.layers.7.input_layernorm.weight', 'addonsmallmodel.layers.1.input_layernorm.weight', 'addonsmallmodel.layers.6.self_attn.o_proj.weight', 'addonsmallmodel.layers.8.post_attention_layernorm.weight', 'addonsmallmodel.layers.9.mlp.down_proj.weight', 'addonsmallmodel.layers.1.mlp.down_proj.weight', 'addonsmallmodel.layers.0.self_attn.q_proj.weight', 'addonsmallmodel.layers.0.mlp.gate_proj.weight', 'addonsmallmodel.embed_projection.weight', 'addonsmallmodel.layers.3.self_attn.k_proj.weight', 'addonsmallmodel.layers.6.mlp.up_proj.weight', 'addonsmallmodel.layers.3.self_attn.v_proj.weight', 'addonsmallmodel.layers.8.mlp.down_proj.weight', 'addonsmallmodel.layers.9.mlp.gate_proj.weight', 'addonsmallmodel.layers.1.self_attn.v_proj.weight', 'addonsmallmodel.layers.1.self_attn.k_proj.weight', 'addonsmallmodel.layers.6.self_attn.v_proj.weight', 'addonsmallmodel.layers.0.mlp.up_proj.weight', 'addonsmallmodel.layers.4.self_attn.o_proj.weight', 'addonsmallmodel.layers.7.self_attn.v_proj.weight', 'addonsmallmodel.layers.8.self_attn.o_proj.weight', 'addonsmallmodel.layers.1.mlp.gate_proj.weight', 'addonsmallmodel.layers.9.self_attn.v_proj.weight', 'addonsmallmodel.layers.3.mlp.gate_proj.weight', 'addonsmallmodel.layers.6.mlp.gate_proj.weight', 'addonsmallmodel.layers.11.mlp.down_proj.weight', 'addonsmallmodel.layers.0.self_attn.v_proj.weight', 'addonsmallmodel.layers.8.mlp.gate_proj.weight', 'addonsmallmodel.layers.6.self_attn.k_proj.weight', 'addonsmallmodel.layers.10.post_attention_layernorm.weight', 'addonsmallmodel.layers.8.self_attn.v_proj.weight', 'addonsmallmodel.layers.11.input_layernorm.weight', 'addonsmallmodel.layers.5.self_attn.o_proj.weight', 'addonsmallmodel.layers.2.self_attn.q_proj.weight', 'addonsmallmodel.layers.0.input_layernorm.weight', 'addonsmallmodel.layers.10.mlp.down_proj.weight', 'addonsmallmodel.layers.11.mlp.up_proj.weight', 'addonsmallmodel.layers.5.input_layernorm.weight', 'addonsmallmodel.layers.2.post_attention_layernorm.weight', 'addonsmallmodel.layers.7.self_attn.q_proj.weight', 'addonsmallmodel.layers.9.post_attention_layernorm.weight', 'addonsmallmodel.layers.7.mlp.up_proj.weight', 'addonsmallmodel.layers.11.self_attn.k_proj.weight', 'addonsmallmodel.layers.4.self_attn.q_proj.weight', 'addonsmallmodel.layers.9.self_attn.o_proj.weight', 'addonsmallmodel.norm.weight', 'addonsmallmodel.layers.10.input_layernorm.weight', 'addonsmallmodel.layers.11.self_attn.q_proj.weight', 'addonsmallmodel.layers.2.mlp.gate_proj.weight', 'addonsmallmodel.layers.5.self_attn.v_proj.weight', 'addonsmallmodel.layers.2.mlp.down_proj.weight', 'addonsmallmodel.layers.8.self_attn.q_proj.weight', 'addonsmallmodel.layers.11.post_attention_layernorm.weight', 'addonsmallmodel.layers.3.self_attn.o_proj.weight', 'addonsmallmodel.layers.7.self_attn.o_proj.weight', 'addonsmallmodel.layers.7.post_attention_layernorm.weight', 'addonsmallmodel.layers.4.mlp.up_proj.weight', 'addonsmallmodel.layers.11.mlp.gate_proj.weight', 'addonsmallmodel.layers.3.input_layernorm.weight', 'addonsmallmodel.layers.10.mlp.up_proj.weight', 'addonsmallmodel.layers.3.mlp.up_proj.weight', 'addonsmallmodel.layers.4.mlp.down_proj.weight', 'addonsmallmodel.layers.8.input_layernorm.weight', 'addonsmallmodel.layers.9.self_attn.q_proj.weight', 'addonsmallmodel.layers.8.mlp.up_proj.weight', 'addonsmallmodel.layers.5.mlp.up_proj.weight', 'addonsmallmodel.layers.0.post_attention_layernorm.weight', 'addonsmallmodel.layers.1.self_attn.o_proj.weight', 'addonsmallmodel.layers.0.mlp.down_proj.weight', 'addonsmallmodel.layers.10.mlp.gate_proj.weight', 'addonsmallmodel.layers.5.post_attention_layernorm.weight', 'addonsmallmodel.layers.10.self_attn.k_proj.weight', 'addonsmallmodel.layers.10.self_attn.q_proj.weight', 'addonsmallmodel.layers.6.input_layernorm.weight', 'addonsmallmodel.layers.1.post_attention_layernorm.weight', 'addonsmallmodel.layers.11.self_attn.o_proj.weight', 'addonsmallmodel.layers.9.self_attn.k_proj.weight', 'addonsmallmodel.layers.4.self_attn.v_proj.weight', 'addonsmallmodel.layers.10.self_attn.v_proj.weight', 'addonsmallmodel.layers.7.mlp.gate_proj.weight', 'addonsmallmodel.layers.5.self_attn.k_proj.weight', 'addonsmallmodel.layers.6.self_attn.q_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 27.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.72s/it]
Some weights of LlamaWeirdLargeTest were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['addonsmallmodel.layers.11.mlp.gate_proj.weight', 'addonsmallmodel.layers.11.post_attention_layernorm.weight', 'addonsmallmodel.layers.9.self_attn.o_proj.weight', 'addonsmallmodel.layers.7.mlp.down_proj.weight', 'addonsmallmodel.layers.8.post_attention_layernorm.weight', 'addonsmallmodel.layers.0.mlp.down_proj.weight', 'addonsmallmodel.layers.7.input_layernorm.weight', 'addonsmallmodel.layers.2.self_attn.k_proj.weight', 'addonsmallmodel.layers.2.post_attention_layernorm.weight', 'addonsmallmodel.layers.4.self_attn.v_proj.weight', 'addonsmallmodel.layers.8.mlp.gate_proj.weight', 'addonsmallmodel.layers.3.input_layernorm.weight', 'addonsmallmodel.layers.5.post_attention_layernorm.weight', 'addonsmallmodel.layers.0.mlp.up_proj.weight', 'addonsmallmodel.layers.4.self_attn.k_proj.weight', 'addonsmallmodel.layers.2.mlp.gate_proj.weight', 'addonsmallmodel.layers.7.self_attn.o_proj.weight', 'addonsmallmodel.layers.4.self_attn.q_proj.weight', 'addonsmallmodel.layers.6.mlp.gate_proj.weight', 'addonsmallmodel.layers.5.self_attn.o_proj.weight', 'addonsmallmodel.layers.3.mlp.gate_proj.weight', 'addonsmallmodel.layers.6.input_layernorm.weight', 'addonsmallmodel.layers.1.mlp.down_proj.weight', 'addonsmallmodel.layers.1.mlp.gate_proj.weight', 'addonsmallmodel.layers.8.self_attn.v_proj.weight', 'addonsmallmodel.embed_tokens.weight', 'addonsmallmodel.layers.5.self_attn.v_proj.weight', 'addonsmallmodel.layers.1.input_layernorm.weight', 'addonsmallmodel.layers.3.mlp.down_proj.weight', 'addonsmallmodel.layers.6.post_attention_layernorm.weight', 'addonsmallmodel.layers.3.self_attn.k_proj.weight', 'addonsmallmodel.layers.9.input_layernorm.weight', 'addonsmallmodel.layers.0.post_attention_layernorm.weight', 'addonsmallmodel.layers.6.self_attn.v_proj.weight', 'addonsmallmodel.layers.1.self_attn.q_proj.weight', 'addonsmallmodel.layers.10.self_attn.v_proj.weight', 'addonsmallmodel.layers.5.self_attn.q_proj.weight', 'addonsmallmodel.layers.7.mlp.up_proj.weight', 'addonsmallmodel.layers.7.mlp.gate_proj.weight', 'addonsmallmodel.layers.5.self_attn.k_proj.weight', 'addonsmallmodel.layers.9.self_attn.q_proj.weight', 'addonsmallmodel.layers.1.self_attn.o_proj.weight', 'addonsmallmodel.layers.7.self_attn.q_proj.weight', 'addonsmallmodel.layers.7.self_attn.k_proj.weight', 'addonsmallmodel.layers.8.input_layernorm.weight', 'addonsmallmodel.layers.8.self_attn.q_proj.weight', 'addonsmallmodel.layers.2.self_attn.q_proj.weight', 'addonsmallmodel.layers.2.mlp.down_proj.weight', 'addonsmallmodel.layers.11.self_attn.q_proj.weight', 'addonsmallmodel.norm.weight', 'addonsmallmodel.layers.11.input_layernorm.weight', 'addonsmallmodel.layers.10.self_attn.o_proj.weight', 'addonsmallmodel.layers.5.input_layernorm.weight', 'addonsmallmodel.layers.9.self_attn.v_proj.weight', 'addonsmallmodel.embed_projection.weight', 'addonsmallmodel.layers.11.self_attn.k_proj.weight', 'addonsmallmodel.layers.3.post_attention_layernorm.weight', 'addonsmallmodel.layers.4.mlp.up_proj.weight', 'addonsmallmodel.layers.10.self_attn.k_proj.weight', 'addonsmallmodel.layers.0.self_attn.q_proj.weight', 'addonsmallmodel.layers.3.self_attn.o_proj.weight', 'addonsmallmodel.layers.1.mlp.up_proj.weight', 'addonsmallmodel.layers.8.mlp.down_proj.weight', 'addonsmallmodel.layers.1.post_attention_layernorm.weight', 'addonsmallmodel.layers.11.mlp.down_proj.weight', 'addonsmallmodel.layers.3.self_attn.q_proj.weight', 'addonsmallmodel.layers.11.self_attn.v_proj.weight', 'addonsmallmodel.layers.0.self_attn.v_proj.weight', 'addonsmallmodel.layers.4.mlp.down_proj.weight', 'addonsmallmodel.layers.7.post_attention_layernorm.weight', 'addonsmallmodel.layers.10.input_layernorm.weight', 'addonsmallmodel.layers.7.self_attn.v_proj.weight', 'addonsmallmodel.layers.4.self_attn.o_proj.weight', 'addonsmallmodel.layers.8.self_attn.o_proj.weight', 'addonsmallmodel.layers.6.self_attn.k_proj.weight', 'addonsmallmodel.layers.2.mlp.up_proj.weight', 'addonsmallmodel.layers.4.mlp.gate_proj.weight', 'addonsmallmodel.layers.10.mlp.down_proj.weight', 'addonsmallmodel.layers.2.self_attn.v_proj.weight', 'addonsmallmodel.layers.0.self_attn.k_proj.weight', 'addonsmallmodel.layers.10.post_attention_layernorm.weight', 'addonsmallmodel.layers.9.mlp.gate_proj.weight', 'addonsmallmodel.layers.8.self_attn.k_proj.weight', 'addonsmallmodel.layers.9.self_attn.k_proj.weight', 'addonsmallmodel.layers.6.self_attn.o_proj.weight', 'addonsmallmodel.layers.3.mlp.up_proj.weight', 'addonsmallmodel.layers.9.post_attention_layernorm.weight', 'addonsmallmodel.layers.10.self_attn.q_proj.weight', 'addonsmallmodel.layers.5.mlp.gate_proj.weight', 'addonsmallmodel.layers.0.mlp.gate_proj.weight', 'addonsmallmodel.layers.9.mlp.up_proj.weight', 'addonsmallmodel.layers.6.self_attn.q_proj.weight', 'addonsmallmodel.layers.9.mlp.down_proj.weight', 'addonsmallmodel.layers.4.input_layernorm.weight', 'addonsmallmodel.layers.2.input_layernorm.weight', 'addonsmallmodel.layers.8.mlp.up_proj.weight', 'addonsmallmodel.layers.6.mlp.up_proj.weight', 'addonsmallmodel.layers.5.mlp.up_proj.weight', 'addonsmallmodel.lm_head.weight', 'addonsmallmodel.layers.3.self_attn.v_proj.weight', 'addonsmallmodel.layers.0.input_layernorm.weight', 'addonsmallmodel.layers.4.post_attention_layernorm.weight', 'addonsmallmodel.layers.10.mlp.up_proj.weight', 'addonsmallmodel.layers.5.mlp.down_proj.weight', 'addonsmallmodel.layers.1.self_attn.v_proj.weight', 'addonsmallmodel.layers.0.self_attn.o_proj.weight', 'addonsmallmodel.layers.1.self_attn.k_proj.weight', 'addonsmallmodel.layers.2.self_attn.o_proj.weight', 'addonsmallmodel.layers.11.self_attn.o_proj.weight', 'addonsmallmodel.layers.11.mlp.up_proj.weight', 'addonsmallmodel.layers.6.mlp.down_proj.weight', 'addonsmallmodel.layers.10.mlp.gate_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 27.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.55s/it]
Some weights of LlamaWeirdLargeTest were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['addonsmallmodel.layers.4.mlp.down_proj.weight', 'addonsmallmodel.layers.7.mlp.down_proj.weight', 'addonsmallmodel.layers.4.post_attention_layernorm.weight', 'addonsmallmodel.layers.2.self_attn.q_proj.weight', 'addonsmallmodel.layers.4.input_layernorm.weight', 'addonsmallmodel.layers.8.self_attn.o_proj.weight', 'addonsmallmodel.layers.10.self_attn.q_proj.weight', 'addonsmallmodel.layers.5.self_attn.v_proj.weight', 'addonsmallmodel.embed_projection.weight', 'addonsmallmodel.layers.1.mlp.up_proj.weight', 'addonsmallmodel.layers.11.mlp.gate_proj.weight', 'addonsmallmodel.layers.11.mlp.up_proj.weight', 'addonsmallmodel.embed_tokens.weight', 'addonsmallmodel.layers.5.mlp.down_proj.weight', 'addonsmallmodel.layers.6.self_attn.o_proj.weight', 'addonsmallmodel.layers.4.self_attn.k_proj.weight', 'addonsmallmodel.layers.0.self_attn.o_proj.weight', 'addonsmallmodel.layers.10.mlp.up_proj.weight', 'addonsmallmodel.layers.6.mlp.up_proj.weight', 'addonsmallmodel.layers.4.mlp.up_proj.weight', 'addonsmallmodel.layers.6.mlp.down_proj.weight', 'addonsmallmodel.layers.9.mlp.up_proj.weight', 'addonsmallmodel.layers.9.mlp.down_proj.weight', 'addonsmallmodel.layers.10.input_layernorm.weight', 'addonsmallmodel.layers.4.mlp.gate_proj.weight', 'addonsmallmodel.layers.5.self_attn.q_proj.weight', 'addonsmallmodel.layers.0.mlp.gate_proj.weight', 'addonsmallmodel.layers.5.mlp.up_proj.weight', 'addonsmallmodel.layers.10.self_attn.v_proj.weight', 'addonsmallmodel.layers.9.post_attention_layernorm.weight', 'addonsmallmodel.layers.4.self_attn.o_proj.weight', 'addonsmallmodel.layers.1.mlp.down_proj.weight', 'addonsmallmodel.layers.10.mlp.gate_proj.weight', 'addonsmallmodel.layers.5.post_attention_layernorm.weight', 'addonsmallmodel.layers.3.post_attention_layernorm.weight', 'addonsmallmodel.layers.0.post_attention_layernorm.weight', 'addonsmallmodel.layers.0.self_attn.q_proj.weight', 'addonsmallmodel.layers.11.self_attn.k_proj.weight', 'addonsmallmodel.layers.4.self_attn.v_proj.weight', 'addonsmallmodel.layers.5.self_attn.o_proj.weight', 'addonsmallmodel.layers.1.self_attn.k_proj.weight', 'addonsmallmodel.layers.11.self_attn.q_proj.weight', 'addonsmallmodel.layers.10.post_attention_layernorm.weight', 'addonsmallmodel.norm.weight', 'addonsmallmodel.layers.0.self_attn.v_proj.weight', 'addonsmallmodel.layers.0.mlp.up_proj.weight', 'addonsmallmodel.layers.3.self_attn.o_proj.weight', 'addonsmallmodel.layers.0.input_layernorm.weight', 'addonsmallmodel.layers.7.self_attn.k_proj.weight', 'addonsmallmodel.layers.6.self_attn.q_proj.weight', 'addonsmallmodel.layers.8.self_attn.q_proj.weight', 'addonsmallmodel.layers.9.self_attn.v_proj.weight', 'addonsmallmodel.layers.2.self_attn.k_proj.weight', 'addonsmallmodel.layers.3.self_attn.k_proj.weight', 'addonsmallmodel.layers.6.self_attn.v_proj.weight', 'addonsmallmodel.layers.1.self_attn.o_proj.weight', 'addonsmallmodel.layers.4.self_attn.q_proj.weight', 'addonsmallmodel.layers.6.mlp.gate_proj.weight', 'addonsmallmodel.layers.11.self_attn.v_proj.weight', 'addonsmallmodel.layers.10.self_attn.k_proj.weight', 'addonsmallmodel.layers.8.self_attn.k_proj.weight', 'addonsmallmodel.layers.9.self_attn.q_proj.weight', 'addonsmallmodel.layers.2.input_layernorm.weight', 'addonsmallmodel.layers.1.input_layernorm.weight', 'addonsmallmodel.layers.1.self_attn.q_proj.weight', 'addonsmallmodel.layers.2.mlp.up_proj.weight', 'addonsmallmodel.layers.7.self_attn.q_proj.weight', 'addonsmallmodel.layers.5.mlp.gate_proj.weight', 'addonsmallmodel.layers.8.mlp.up_proj.weight', 'addonsmallmodel.layers.1.post_attention_layernorm.weight', 'addonsmallmodel.lm_head.weight', 'addonsmallmodel.layers.11.post_attention_layernorm.weight', 'addonsmallmodel.layers.2.mlp.gate_proj.weight', 'addonsmallmodel.layers.8.self_attn.v_proj.weight', 'addonsmallmodel.layers.7.mlp.gate_proj.weight', 'addonsmallmodel.layers.11.mlp.down_proj.weight', 'addonsmallmodel.layers.2.self_attn.v_proj.weight', 'addonsmallmodel.layers.8.input_layernorm.weight', 'addonsmallmodel.layers.6.post_attention_layernorm.weight', 'addonsmallmodel.layers.7.post_attention_layernorm.weight', 'addonsmallmodel.layers.9.mlp.gate_proj.weight', 'addonsmallmodel.layers.7.input_layernorm.weight', 'addonsmallmodel.layers.10.self_attn.o_proj.weight', 'addonsmallmodel.layers.6.self_attn.k_proj.weight', 'addonsmallmodel.layers.7.mlp.up_proj.weight', 'addonsmallmodel.layers.2.post_attention_layernorm.weight', 'addonsmallmodel.layers.8.post_attention_layernorm.weight', 'addonsmallmodel.layers.1.mlp.gate_proj.weight', 'addonsmallmodel.layers.8.mlp.down_proj.weight', 'addonsmallmodel.layers.2.mlp.down_proj.weight', 'addonsmallmodel.layers.1.self_attn.v_proj.weight', 'addonsmallmodel.layers.3.self_attn.v_proj.weight', 'addonsmallmodel.layers.3.mlp.up_proj.weight', 'addonsmallmodel.layers.11.input_layernorm.weight', 'addonsmallmodel.layers.11.self_attn.o_proj.weight', 'addonsmallmodel.layers.3.mlp.down_proj.weight', 'addonsmallmodel.layers.0.self_attn.k_proj.weight', 'addonsmallmodel.layers.2.self_attn.o_proj.weight', 'addonsmallmodel.layers.9.input_layernorm.weight', 'addonsmallmodel.layers.3.mlp.gate_proj.weight', 'addonsmallmodel.layers.9.self_attn.k_proj.weight', 'addonsmallmodel.layers.7.self_attn.o_proj.weight', 'addonsmallmodel.layers.10.mlp.down_proj.weight', 'addonsmallmodel.layers.0.mlp.down_proj.weight', 'addonsmallmodel.layers.5.self_attn.k_proj.weight', 'addonsmallmodel.layers.8.mlp.gate_proj.weight', 'addonsmallmodel.layers.5.input_layernorm.weight', 'addonsmallmodel.layers.6.input_layernorm.weight', 'addonsmallmodel.layers.3.self_attn.q_proj.weight', 'addonsmallmodel.layers.7.self_attn.v_proj.weight', 'addonsmallmodel.layers.3.input_layernorm.weight', 'addonsmallmodel.layers.9.self_attn.o_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 27.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:01<00:00, 30.60s/it]
Some weights of LlamaWeirdLargeTest were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['addonsmallmodel.layers.11.input_layernorm.weight', 'addonsmallmodel.layers.6.self_attn.k_proj.weight', 'addonsmallmodel.layers.11.mlp.gate_proj.weight', 'addonsmallmodel.layers.11.self_attn.v_proj.weight', 'addonsmallmodel.lm_head.weight', 'addonsmallmodel.layers.11.post_attention_layernorm.weight', 'addonsmallmodel.layers.11.self_attn.k_proj.weight', 'addonsmallmodel.layers.3.input_layernorm.weight', 'addonsmallmodel.layers.9.self_attn.o_proj.weight', 'addonsmallmodel.layers.6.self_attn.o_proj.weight', 'addonsmallmodel.layers.4.mlp.down_proj.weight', 'addonsmallmodel.layers.4.self_attn.o_proj.weight', 'addonsmallmodel.layers.8.input_layernorm.weight', 'addonsmallmodel.layers.1.mlp.down_proj.weight', 'addonsmallmodel.layers.0.self_attn.q_proj.weight', 'addonsmallmodel.layers.4.mlp.gate_proj.weight', 'addonsmallmodel.layers.6.self_attn.q_proj.weight', 'addonsmallmodel.layers.5.input_layernorm.weight', 'addonsmallmodel.layers.0.self_attn.v_proj.weight', 'addonsmallmodel.layers.6.self_attn.v_proj.weight', 'addonsmallmodel.layers.10.mlp.down_proj.weight', 'addonsmallmodel.layers.10.self_attn.q_proj.weight', 'addonsmallmodel.layers.9.mlp.up_proj.weight', 'addonsmallmodel.layers.4.self_attn.k_proj.weight', 'addonsmallmodel.layers.5.self_attn.k_proj.weight', 'addonsmallmodel.layers.2.mlp.down_proj.weight', 'addonsmallmodel.layers.9.post_attention_layernorm.weight', 'addonsmallmodel.layers.0.mlp.down_proj.weight', 'addonsmallmodel.layers.3.self_attn.q_proj.weight', 'addonsmallmodel.layers.5.post_attention_layernorm.weight', 'addonsmallmodel.layers.7.self_attn.k_proj.weight', 'addonsmallmodel.layers.7.mlp.down_proj.weight', 'addonsmallmodel.layers.1.self_attn.o_proj.weight', 'addonsmallmodel.layers.11.self_attn.o_proj.weight', 'addonsmallmodel.layers.8.post_attention_layernorm.weight', 'addonsmallmodel.layers.3.post_attention_layernorm.weight', 'addonsmallmodel.embed_tokens.weight', 'addonsmallmodel.layers.9.mlp.gate_proj.weight', 'addonsmallmodel.layers.8.self_attn.o_proj.weight', 'addonsmallmodel.layers.1.post_attention_layernorm.weight', 'addonsmallmodel.layers.1.self_attn.q_proj.weight', 'addonsmallmodel.layers.6.post_attention_layernorm.weight', 'addonsmallmodel.layers.2.post_attention_layernorm.weight', 'addonsmallmodel.layers.7.self_attn.q_proj.weight', 'addonsmallmodel.layers.8.mlp.gate_proj.weight', 'addonsmallmodel.layers.3.mlp.gate_proj.weight', 'addonsmallmodel.layers.1.self_attn.k_proj.weight', 'addonsmallmodel.layers.2.self_attn.q_proj.weight', 'addonsmallmodel.layers.7.mlp.up_proj.weight', 'addonsmallmodel.layers.4.self_attn.q_proj.weight', 'addonsmallmodel.layers.2.mlp.gate_proj.weight', 'addonsmallmodel.layers.3.self_attn.k_proj.weight', 'addonsmallmodel.layers.2.self_attn.o_proj.weight', 'addonsmallmodel.layers.10.mlp.gate_proj.weight', 'addonsmallmodel.layers.7.mlp.gate_proj.weight', 'addonsmallmodel.layers.10.post_attention_layernorm.weight', 'addonsmallmodel.layers.10.self_attn.o_proj.weight', 'addonsmallmodel.layers.0.post_attention_layernorm.weight', 'addonsmallmodel.layers.11.mlp.up_proj.weight', 'addonsmallmodel.layers.5.self_attn.q_proj.weight', 'addonsmallmodel.layers.9.self_attn.k_proj.weight', 'addonsmallmodel.layers.0.self_attn.k_proj.weight', 'addonsmallmodel.layers.0.mlp.up_proj.weight', 'addonsmallmodel.layers.9.self_attn.q_proj.weight', 'addonsmallmodel.embed_projection.weight', 'addonsmallmodel.layers.7.post_attention_layernorm.weight', 'addonsmallmodel.layers.7.self_attn.v_proj.weight', 'addonsmallmodel.layers.2.mlp.up_proj.weight', 'addonsmallmodel.layers.2.input_layernorm.weight', 'addonsmallmodel.layers.7.self_attn.o_proj.weight', 'addonsmallmodel.layers.8.mlp.down_proj.weight', 'addonsmallmodel.layers.7.input_layernorm.weight', 'addonsmallmodel.layers.3.mlp.down_proj.weight', 'addonsmallmodel.layers.5.mlp.down_proj.weight', 'addonsmallmodel.layers.9.input_layernorm.weight', 'addonsmallmodel.layers.4.mlp.up_proj.weight', 'addonsmallmodel.layers.11.self_attn.q_proj.weight', 'addonsmallmodel.layers.1.self_attn.v_proj.weight', 'addonsmallmodel.layers.2.self_attn.v_proj.weight', 'addonsmallmodel.layers.8.mlp.up_proj.weight', 'addonsmallmodel.layers.11.mlp.down_proj.weight', 'addonsmallmodel.layers.5.self_attn.o_proj.weight', 'addonsmallmodel.layers.6.mlp.up_proj.weight', 'addonsmallmodel.layers.4.input_layernorm.weight', 'addonsmallmodel.layers.5.mlp.gate_proj.weight', 'addonsmallmodel.layers.1.mlp.up_proj.weight', 'addonsmallmodel.layers.1.mlp.gate_proj.weight', 'addonsmallmodel.layers.0.mlp.gate_proj.weight', 'addonsmallmodel.layers.4.post_attention_layernorm.weight', 'addonsmallmodel.layers.10.self_attn.k_proj.weight', 'addonsmallmodel.layers.0.self_attn.o_proj.weight', 'addonsmallmodel.norm.weight', 'addonsmallmodel.layers.10.self_attn.v_proj.weight', 'addonsmallmodel.layers.6.input_layernorm.weight', 'addonsmallmodel.layers.9.mlp.down_proj.weight', 'addonsmallmodel.layers.3.mlp.up_proj.weight', 'addonsmallmodel.layers.8.self_attn.q_proj.weight', 'addonsmallmodel.layers.8.self_attn.k_proj.weight', 'addonsmallmodel.layers.10.input_layernorm.weight', 'addonsmallmodel.layers.0.input_layernorm.weight', 'addonsmallmodel.layers.6.mlp.down_proj.weight', 'addonsmallmodel.layers.3.self_attn.v_proj.weight', 'addonsmallmodel.layers.2.self_attn.k_proj.weight', 'addonsmallmodel.layers.8.self_attn.v_proj.weight', 'addonsmallmodel.layers.10.mlp.up_proj.weight', 'addonsmallmodel.layers.3.self_attn.o_proj.weight', 'addonsmallmodel.layers.5.mlp.up_proj.weight', 'addonsmallmodel.layers.5.self_attn.v_proj.weight', 'addonsmallmodel.layers.1.input_layernorm.weight', 'addonsmallmodel.layers.4.self_attn.v_proj.weight', 'addonsmallmodel.layers.6.mlp.gate_proj.weight', 'addonsmallmodel.layers.9.self_attn.v_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 13.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 13.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 15.11s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.86s/it]
Some weights of LlamaWeirdLargeTest were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['addonsmallmodel.layers.3.self_attn.o_proj.weight', 'addonsmallmodel.layers.6.mlp.up_proj.weight', 'addonsmallmodel.layers.9.self_attn.k_proj.weight', 'addonsmallmodel.layers.5.self_attn.k_proj.weight', 'addonsmallmodel.layers.3.mlp.gate_proj.weight', 'addonsmallmodel.layers.3.post_attention_layernorm.weight', 'addonsmallmodel.layers.5.post_attention_layernorm.weight', 'addonsmallmodel.layers.10.input_layernorm.weight', 'addonsmallmodel.layers.6.mlp.down_proj.weight', 'addonsmallmodel.layers.0.mlp.down_proj.weight', 'addonsmallmodel.layers.9.mlp.up_proj.weight', 'addonsmallmodel.layers.10.self_attn.k_proj.weight', 'addonsmallmodel.layers.6.post_attention_layernorm.weight', 'addonsmallmodel.layers.2.self_attn.o_proj.weight', 'addonsmallmodel.layers.4.mlp.gate_proj.weight', 'addonsmallmodel.layers.0.self_attn.k_proj.weight', 'addonsmallmodel.layers.7.input_layernorm.weight', 'addonsmallmodel.layers.4.mlp.up_proj.weight', 'addonsmallmodel.layers.10.mlp.down_proj.weight', 'addonsmallmodel.layers.8.self_attn.k_proj.weight', 'addonsmallmodel.embed_tokens.weight', 'addonsmallmodel.layers.4.self_attn.q_proj.weight', 'addonsmallmodel.layers.7.mlp.gate_proj.weight', 'addonsmallmodel.layers.6.self_attn.k_proj.weight', 'addonsmallmodel.layers.0.self_attn.q_proj.weight', 'addonsmallmodel.layers.5.input_layernorm.weight', 'addonsmallmodel.layers.6.self_attn.o_proj.weight', 'addonsmallmodel.layers.2.self_attn.v_proj.weight', 'addonsmallmodel.layers.2.mlp.gate_proj.weight', 'addonsmallmodel.layers.2.mlp.down_proj.weight', 'addonsmallmodel.layers.9.self_attn.q_proj.weight', 'addonsmallmodel.layers.1.input_layernorm.weight', 'addonsmallmodel.layers.8.post_attention_layernorm.weight', 'addonsmallmodel.layers.8.self_attn.v_proj.weight', 'addonsmallmodel.layers.10.mlp.up_proj.weight', 'addonsmallmodel.layers.9.post_attention_layernorm.weight', 'addonsmallmodel.layers.6.mlp.gate_proj.weight', 'addonsmallmodel.layers.6.input_layernorm.weight', 'addonsmallmodel.layers.7.mlp.up_proj.weight', 'addonsmallmodel.layers.0.post_attention_layernorm.weight', 'addonsmallmodel.layers.8.input_layernorm.weight', 'addonsmallmodel.layers.3.mlp.down_proj.weight', 'addonsmallmodel.layers.11.post_attention_layernorm.weight', 'addonsmallmodel.layers.5.mlp.down_proj.weight', 'addonsmallmodel.layers.9.self_attn.v_proj.weight', 'addonsmallmodel.layers.11.self_attn.o_proj.weight', 'addonsmallmodel.layers.5.mlp.up_proj.weight', 'addonsmallmodel.layers.7.post_attention_layernorm.weight', 'addonsmallmodel.layers.8.mlp.down_proj.weight', 'addonsmallmodel.layers.8.mlp.gate_proj.weight', 'addonsmallmodel.lm_head.weight', 'addonsmallmodel.layers.9.mlp.gate_proj.weight', 'addonsmallmodel.layers.5.self_attn.q_proj.weight', 'addonsmallmodel.layers.3.input_layernorm.weight', 'addonsmallmodel.layers.3.self_attn.k_proj.weight', 'addonsmallmodel.layers.4.self_attn.k_proj.weight', 'addonsmallmodel.layers.7.mlp.down_proj.weight', 'addonsmallmodel.layers.4.input_layernorm.weight', 'addonsmallmodel.layers.11.mlp.gate_proj.weight', 'addonsmallmodel.norm.weight', 'addonsmallmodel.layers.4.self_attn.v_proj.weight', 'addonsmallmodel.layers.0.mlp.gate_proj.weight', 'addonsmallmodel.layers.0.self_attn.o_proj.weight', 'addonsmallmodel.layers.1.self_attn.q_proj.weight', 'addonsmallmodel.layers.1.post_attention_layernorm.weight', 'addonsmallmodel.layers.9.mlp.down_proj.weight', 'addonsmallmodel.layers.10.self_attn.q_proj.weight', 'addonsmallmodel.layers.2.self_attn.k_proj.weight', 'addonsmallmodel.layers.5.mlp.gate_proj.weight', 'addonsmallmodel.layers.8.self_attn.o_proj.weight', 'addonsmallmodel.layers.7.self_attn.v_proj.weight', 'addonsmallmodel.layers.11.self_attn.q_proj.weight', 'addonsmallmodel.layers.10.mlp.gate_proj.weight', 'addonsmallmodel.layers.2.input_layernorm.weight', 'addonsmallmodel.layers.5.self_attn.v_proj.weight', 'addonsmallmodel.layers.2.self_attn.q_proj.weight', 'addonsmallmodel.layers.2.post_attention_layernorm.weight', 'addonsmallmodel.layers.1.mlp.gate_proj.weight', 'addonsmallmodel.layers.3.self_attn.q_proj.weight', 'addonsmallmodel.layers.1.self_attn.v_proj.weight', 'addonsmallmodel.layers.7.self_attn.o_proj.weight', 'addonsmallmodel.layers.3.mlp.up_proj.weight', 'addonsmallmodel.layers.7.self_attn.k_proj.weight', 'addonsmallmodel.layers.9.input_layernorm.weight', 'addonsmallmodel.layers.11.self_attn.k_proj.weight', 'addonsmallmodel.layers.7.self_attn.q_proj.weight', 'addonsmallmodel.layers.6.self_attn.v_proj.weight', 'addonsmallmodel.layers.6.self_attn.q_proj.weight', 'addonsmallmodel.layers.4.post_attention_layernorm.weight', 'addonsmallmodel.layers.11.mlp.up_proj.weight', 'addonsmallmodel.embed_projection.weight', 'addonsmallmodel.layers.1.self_attn.o_proj.weight', 'addonsmallmodel.layers.2.mlp.up_proj.weight', 'addonsmallmodel.layers.0.self_attn.v_proj.weight', 'addonsmallmodel.layers.3.self_attn.v_proj.weight', 'addonsmallmodel.layers.11.input_layernorm.weight', 'addonsmallmodel.layers.8.mlp.up_proj.weight', 'addonsmallmodel.layers.1.mlp.up_proj.weight', 'addonsmallmodel.layers.8.self_attn.q_proj.weight', 'addonsmallmodel.layers.11.self_attn.v_proj.weight', 'addonsmallmodel.layers.0.input_layernorm.weight', 'addonsmallmodel.layers.10.self_attn.v_proj.weight', 'addonsmallmodel.layers.5.self_attn.o_proj.weight', 'addonsmallmodel.layers.0.mlp.up_proj.weight', 'addonsmallmodel.layers.10.self_attn.o_proj.weight', 'addonsmallmodel.layers.1.mlp.down_proj.weight', 'addonsmallmodel.layers.9.self_attn.o_proj.weight', 'addonsmallmodel.layers.4.self_attn.o_proj.weight', 'addonsmallmodel.layers.10.post_attention_layernorm.weight', 'addonsmallmodel.layers.1.self_attn.k_proj.weight', 'addonsmallmodel.layers.4.mlp.down_proj.weight', 'addonsmallmodel.layers.11.mlp.down_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of LlamaWeirdLargeTest were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['addonsmallmodel.layers.7.post_attention_layernorm.weight', 'addonsmallmodel.layers.4.mlp.down_proj.weight', 'addonsmallmodel.layers.7.self_attn.q_proj.weight', 'addonsmallmodel.layers.7.self_attn.o_proj.weight', 'addonsmallmodel.layers.0.self_attn.o_proj.weight', 'addonsmallmodel.layers.6.self_attn.v_proj.weight', 'addonsmallmodel.layers.2.self_attn.v_proj.weight', 'addonsmallmodel.layers.11.input_layernorm.weight', 'addonsmallmodel.layers.1.mlp.gate_proj.weight', 'addonsmallmodel.layers.11.mlp.gate_proj.weight', 'addonsmallmodel.layers.3.mlp.gate_proj.weight', 'addonsmallmodel.layers.4.self_attn.v_proj.weight', 'addonsmallmodel.layers.6.self_attn.o_proj.weight', 'addonsmallmodel.layers.0.self_attn.q_proj.weight', 'addonsmallmodel.layers.4.self_attn.o_proj.weight', 'addonsmallmodel.layers.11.self_attn.v_proj.weight', 'addonsmallmodel.layers.7.self_attn.k_proj.weight', 'addonsmallmodel.layers.9.mlp.down_proj.weight', 'addonsmallmodel.layers.9.self_attn.o_proj.weight', 'addonsmallmodel.layers.9.input_layernorm.weight', 'addonsmallmodel.layers.11.mlp.up_proj.weight', 'addonsmallmodel.layers.10.self_attn.v_proj.weight', 'addonsmallmodel.layers.7.mlp.gate_proj.weight', 'addonsmallmodel.layers.10.mlp.down_proj.weight', 'addonsmallmodel.layers.0.mlp.up_proj.weight', 'addonsmallmodel.layers.1.self_attn.o_proj.weight', 'addonsmallmodel.layers.9.self_attn.v_proj.weight', 'addonsmallmodel.layers.2.input_layernorm.weight', 'addonsmallmodel.layers.4.self_attn.q_proj.weight', 'addonsmallmodel.norm.weight', 'addonsmallmodel.layers.3.post_attention_layernorm.weight', 'addonsmallmodel.layers.8.self_attn.q_proj.weight', 'addonsmallmodel.layers.3.input_layernorm.weight', 'addonsmallmodel.layers.8.self_attn.k_proj.weight', 'addonsmallmodel.layers.8.mlp.down_proj.weight', 'addonsmallmodel.layers.0.mlp.gate_proj.weight', 'addonsmallmodel.layers.0.post_attention_layernorm.weight', 'addonsmallmodel.layers.8.self_attn.o_proj.weight', 'addonsmallmodel.layers.3.mlp.down_proj.weight', 'addonsmallmodel.layers.9.self_attn.q_proj.weight', 'addonsmallmodel.layers.5.self_attn.k_proj.weight', 'addonsmallmodel.layers.5.mlp.gate_proj.weight', 'addonsmallmodel.layers.4.self_attn.k_proj.weight', 'addonsmallmodel.layers.4.input_layernorm.weight', 'addonsmallmodel.layers.7.mlp.up_proj.weight', 'addonsmallmodel.layers.4.post_attention_layernorm.weight', 'addonsmallmodel.layers.11.mlp.down_proj.weight', 'addonsmallmodel.layers.5.self_attn.q_proj.weight', 'addonsmallmodel.layers.7.input_layernorm.weight', 'addonsmallmodel.layers.2.self_attn.k_proj.weight', 'addonsmallmodel.layers.11.post_attention_layernorm.weight', 'addonsmallmodel.layers.7.self_attn.v_proj.weight', 'addonsmallmodel.layers.2.self_attn.o_proj.weight', 'addonsmallmodel.layers.10.input_layernorm.weight', 'addonsmallmodel.layers.5.self_attn.v_proj.weight', 'addonsmallmodel.layers.8.mlp.gate_proj.weight', 'addonsmallmodel.layers.10.post_attention_layernorm.weight', 'addonsmallmodel.embed_tokens.weight', 'addonsmallmodel.layers.5.input_layernorm.weight', 'addonsmallmodel.layers.3.mlp.up_proj.weight', 'addonsmallmodel.layers.6.mlp.up_proj.weight', 'addonsmallmodel.layers.7.mlp.down_proj.weight', 'addonsmallmodel.layers.1.self_attn.q_proj.weight', 'addonsmallmodel.layers.10.mlp.gate_proj.weight', 'addonsmallmodel.layers.1.mlp.up_proj.weight', 'addonsmallmodel.layers.0.self_attn.k_proj.weight', 'addonsmallmodel.layers.1.self_attn.k_proj.weight', 'addonsmallmodel.layers.2.mlp.gate_proj.weight', 'addonsmallmodel.layers.2.mlp.down_proj.weight', 'addonsmallmodel.layers.9.mlp.up_proj.weight', 'addonsmallmodel.layers.11.self_attn.k_proj.weight', 'addonsmallmodel.layers.1.self_attn.v_proj.weight', 'addonsmallmodel.layers.3.self_attn.k_proj.weight', 'addonsmallmodel.layers.6.self_attn.k_proj.weight', 'addonsmallmodel.layers.6.post_attention_layernorm.weight', 'addonsmallmodel.layers.5.mlp.up_proj.weight', 'addonsmallmodel.layers.10.self_attn.o_proj.weight', 'addonsmallmodel.layers.8.post_attention_layernorm.weight', 'addonsmallmodel.layers.0.self_attn.v_proj.weight', 'addonsmallmodel.layers.1.mlp.down_proj.weight', 'addonsmallmodel.layers.8.input_layernorm.weight', 'addonsmallmodel.layers.9.self_attn.k_proj.weight', 'addonsmallmodel.layers.1.input_layernorm.weight', 'addonsmallmodel.layers.5.self_attn.o_proj.weight', 'addonsmallmodel.layers.5.post_attention_layernorm.weight', 'addonsmallmodel.layers.2.mlp.up_proj.weight', 'addonsmallmodel.layers.6.mlp.down_proj.weight', 'addonsmallmodel.layers.3.self_attn.o_proj.weight', 'addonsmallmodel.layers.4.mlp.up_proj.weight', 'addonsmallmodel.layers.3.self_attn.v_proj.weight', 'addonsmallmodel.layers.0.mlp.down_proj.weight', 'addonsmallmodel.layers.8.self_attn.v_proj.weight', 'addonsmallmodel.layers.9.post_attention_layernorm.weight', 'addonsmallmodel.layers.11.self_attn.o_proj.weight', 'addonsmallmodel.layers.4.mlp.gate_proj.weight', 'addonsmallmodel.layers.9.mlp.gate_proj.weight', 'addonsmallmodel.layers.10.self_attn.q_proj.weight', 'addonsmallmodel.layers.11.self_attn.q_proj.weight', 'addonsmallmodel.lm_head.weight', 'addonsmallmodel.layers.2.self_attn.q_proj.weight', 'addonsmallmodel.layers.0.input_layernorm.weight', 'addonsmallmodel.layers.2.post_attention_layernorm.weight', 'addonsmallmodel.layers.6.self_attn.q_proj.weight', 'addonsmallmodel.layers.10.mlp.up_proj.weight', 'addonsmallmodel.layers.6.mlp.gate_proj.weight', 'addonsmallmodel.layers.6.input_layernorm.weight', 'addonsmallmodel.layers.1.post_attention_layernorm.weight', 'addonsmallmodel.layers.3.self_attn.q_proj.weight', 'addonsmallmodel.layers.5.mlp.down_proj.weight', 'addonsmallmodel.layers.8.mlp.up_proj.weight', 'addonsmallmodel.embed_projection.weight', 'addonsmallmodel.layers.10.self_attn.k_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/weird_training_addingextralarge.py", line 1241, in <module>
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/weird_training_addingextralarge.py", line 1241, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/weird_training_addingextralarge.py", line 1241, in <module>
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/weird_training_addingextralarge.py", line 1241, in <module>
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/weird_training_addingextralarge.py", line 1241, in <module>
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/weird_training_addingextralarge.py", line 1241, in <module>
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/weird_training_addingextralarge.py", line 1241, in <module>
    large_model = LlamaWeirdLargeTest.from_pretrained("meta-llama/Llama-2-7b-hf", cache_dir = dir_models).to(torch_device).to(torch.bfloat16) 
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/modeling_utils.py", line 2303, in to
            large_model = LlamaWeirdLargeTest.from_pretrained("meta-llama/Llama-2-7b-hf", cache_dir = dir_models).to(torch_device).to(torch.bfloat16)     large_model = LlamaWeirdLargeTest.from_pretrained("meta-llama/Llama-2-7b-hf", cache_dir = dir_models).to(torch_device).to(torch.bfloat16)     large_model = LlamaWeirdLargeTest.from_pretrained("meta-llama/Llama-2-7b-hf", cache_dir = dir_models).to(torch_device).to(torch.bfloat16) 
large_model = LlamaWeirdLargeTest.from_pretrained("meta-llama/Llama-2-7b-hf", cache_dir = dir_models).to(torch_device).to(torch.bfloat16) 
large_model = LlamaWeirdLargeTest.from_pretrained("meta-llama/Llama-2-7b-hf", cache_dir = dir_models).to(torch_device).to(torch.bfloat16) 
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/modeling_utils.py", line 2303, in to

  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/modeling_utils.py", line 2303, in to
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/modeling_utils.py", line 2303, in to

  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/modeling_utils.py", line 2303, in to
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/modeling_utils.py", line 2303, in to
    large_model = LlamaWeirdLargeTest.from_pretrained("meta-llama/Llama-2-7b-hf", cache_dir = dir_models).to(torch_device).to(torch.bfloat16) 
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/modeling_utils.py", line 2303, in to
Traceback (most recent call last):
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/weird_training_addingextralarge.py", line 1241, in <module>
    large_model = LlamaWeirdLargeTest.from_pretrained("meta-llama/Llama-2-7b-hf", cache_dir = dir_models).to(torch_device).to(torch.bfloat16) 
  File "/opt/hpcaas/.mounts/fs-03efe25c053395d1f/beidic/yang/transformersprofiling/src/transformers/modeling_utils.py", line 2303, in to
                    return super().to(*args, **kwargs)return super().to(*args, **kwargs)return super().to(*args, **kwargs)    return super().to(*args, **kwargs)return super().to(*args, **kwargs)


return super().to(*args, **kwargs)
    
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in to
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in to
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in to

  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in to
return super().to(*args, **kwargs)  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in to
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in to

  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in to
    return super().to(*args, **kwargs)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in to
                        return self._apply(convert)    return self._apply(convert)return self._apply(convert)return self._apply(convert)return self._apply(convert)return self._apply(convert)
return self._apply(convert)




  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply

  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
    return self._apply(convert)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
                    module._apply(fn)    module._apply(fn)module._apply(fn)module._apply(fn)module._apply(fn)
module._apply(fn)


    
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply

  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
module._apply(fn)  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply

  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
            module._apply(fn)        module._apply(fn)    module._apply(fn)
module._apply(fn)module._apply(fn)
module._apply(fn)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply

    
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply

  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply
module._apply(fn)  [Previous line repeated 2 more times]
      File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply

  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 833, in _apply
module._apply(fn)  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 810, in _apply

    module._apply(fn)  [Previous line repeated 2 more times]

      File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 833, in _apply
      [Previous line repeated 2 more times]
module._apply(fn)module._apply(fn)  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 833, in _apply

    
    module._apply(fn)  [Previous line repeated 2 more times]
        param_applied = fn(param)  [Previous line repeated 2 more times]

  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 833, in _apply
module._apply(fn)module._apply(fn)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 833, in _apply
  [Previous line repeated 2 more times]


  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1158, in convert
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 833, in _apply
      [Previous line repeated 2 more times]
      [Previous line repeated 2 more times]
param_applied = fn(param)  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 833, in _apply
param_applied = fn(param)  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 833, in _apply

    
      File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1158, in convert
param_applied = fn(param)      File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1158, in convert
param_applied = fn(param)    
param_applied = fn(param)    
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1158, in convert

param_applied = fn(param)  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1158, in convert

  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1158, in convert

torch.cuda  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1158, in convert
.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 79.33 GiB of which 118.19 MiB is free. Including non-PyTorch memory, this process has 9.50 GiB memory in use. Process 2868796 has 13.29 GiB memory in use. Process 2868794 has 8.91 GiB memory in use. Process 2868797 has 9.66 GiB memory in use. Process 2868800 has 8.74 GiB memory in use. Process 2868798 has 9.66 GiB memory in use. Process 2868799 has 9.83 GiB memory in use. Process 2868801 has 9.66 GiB memory in use. Of the allocated memory 9.08 GiB is allocated by PyTorch, and 13.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF    
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)    
param_applied = fn(param)
torch.cuda  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1158, in convert
.    OutOfMemoryError    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking):     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)    
CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 79.33 GiB of which 54.19 MiB is free. Process 2868795 has 9.50 GiB memory in use. Process 2868796 has 13.29 GiB memory in use. Process 2868794 has 8.91 GiB memory in use. Process 2868797 has 9.66 GiB memory in use. Process 2868800 has 8.74 GiB memory in use. Including non-PyTorch memory, this process has 9.66 GiB memory in use. Process 2868799 has 9.83 GiB memory in use. Process 2868801 has 9.66 GiB memory in use. Of the allocated memory 9.25 GiB is allocated by PyTorch, and 13.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cudareturn t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)

.torch.cuda
OutOfMemoryError.torch.cudatorch.cuda: OutOfMemoryError..CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 79.33 GiB of which 54.19 MiB is free. Process 2868795 has 9.50 GiB memory in use. Process 2868796 has 13.29 GiB memory in use. Process 2868794 has 8.91 GiB memory in use. Including non-PyTorch memory, this process has 9.66 GiB memory in use. Process 2868800 has 8.74 GiB memory in use. Process 2868798 has 9.66 GiB memory in use. Process 2868799 has 9.83 GiB memory in use. Process 2868801 has 9.66 GiB memory in use. Of the allocated memory 9.25 GiB is allocated by PyTorch, and 13.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF: OutOfMemoryErrortorch.cudaOutOfMemoryError
CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 79.33 GiB of which 54.19 MiB is free. Process 2868795 has 9.50 GiB memory in use. Process 2868796 has 13.29 GiB memory in use. Process 2868794 has 8.91 GiB memory in use. Process 2868797 has 9.66 GiB memory in use. Process 2868800 has 8.74 GiB memory in use. Process 2868798 has 9.66 GiB memory in use. Including non-PyTorch memory, this process has 9.83 GiB memory in use. Process 2868801 has 9.66 GiB memory in use. Of the allocated memory 9.41 GiB is allocated by PyTorch, and 13.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF: .: 
CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 79.33 GiB of which 118.19 MiB is free. Process 2868795 has 9.50 GiB memory in use. Including non-PyTorch memory, this process has 13.29 GiB memory in use. Process 2868794 has 8.91 GiB memory in use. Process 2868797 has 9.66 GiB memory in use. Process 2868800 has 8.74 GiB memory in use. Process 2868798 has 9.66 GiB memory in use. Process 2868799 has 9.83 GiB memory in use. Process 2868801 has 9.66 GiB memory in use. Of the allocated memory 12.87 GiB is allocated by PyTorch, and 13.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONFOutOfMemoryErrorCUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 79.33 GiB of which 54.19 MiB is free. Process 2868795 has 9.50 GiB memory in use. Process 2868796 has 13.29 GiB memory in use. Process 2868794 has 8.91 GiB memory in use. Process 2868797 has 9.66 GiB memory in use. Process 2868800 has 8.74 GiB memory in use. Process 2868798 has 9.66 GiB memory in use. Process 2868799 has 9.83 GiB memory in use. Including non-PyTorch memory, this process has 9.66 GiB memory in use. Of the allocated memory 9.25 GiB is allocated by PyTorch, and 13.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
: 
CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 79.33 GiB of which 34.19 MiB is free. Process 2868795 has 9.50 GiB memory in use. Process 2868796 has 13.29 GiB memory in use. Process 2868794 has 8.91 GiB memory in use. Process 2868797 has 9.66 GiB memory in use. Including non-PyTorch memory, this process has 8.74 GiB memory in use. Process 2868798 has 9.66 GiB memory in use. Process 2868799 has 9.83 GiB memory in use. Process 2868801 has 9.66 GiB memory in use. Of the allocated memory 8.32 GiB is allocated by PyTorch, and 17.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacty of 79.33 GiB of which 118.19 MiB is free. Process 2868795 has 9.50 GiB memory in use. Process 2868796 has 13.29 GiB memory in use. Including non-PyTorch memory, this process has 8.91 GiB memory in use. Process 2868797 has 9.66 GiB memory in use. Process 2868800 has 8.74 GiB memory in use. Process 2868798 has 9.66 GiB memory in use. Process 2868799 has 9.83 GiB memory in use. Process 2868801 has 9.66 GiB memory in use. Of the allocated memory 8.49 GiB is allocated by PyTorch, and 17.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-04-06 03:42:51,100] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2868794) of binary: /data/home/beidic/anaconda3/envs/yangllm2/bin/python3.9
Traceback (most recent call last):
  File "/data/home/beidic/anaconda3/envs/yangllm2/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/accelerate/commands/launch.py", line 1014, in launch_command
    multi_gpu_launcher(args)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/accelerate/commands/launch.py", line 672, in multi_gpu_launcher
    distrib_run.run(args)
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/data/home/beidic/anaconda3/envs/yangllm2/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
weird_training_addingextralarge.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-04-06_03:42:51
  host      : a100-st-p4de24xlarge-1029.fair-a100.hpcaas
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2868795)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-04-06_03:42:51
  host      : a100-st-p4de24xlarge-1029.fair-a100.hpcaas
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 2868796)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-04-06_03:42:51
  host      : a100-st-p4de24xlarge-1029.fair-a100.hpcaas
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2868797)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-04-06_03:42:51
  host      : a100-st-p4de24xlarge-1029.fair-a100.hpcaas
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 2868798)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-04-06_03:42:51
  host      : a100-st-p4de24xlarge-1029.fair-a100.hpcaas
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 2868799)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-04-06_03:42:51
  host      : a100-st-p4de24xlarge-1029.fair-a100.hpcaas
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 2868800)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-04-06_03:42:51
  host      : a100-st-p4de24xlarge-1029.fair-a100.hpcaas
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 2868801)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-06_03:42:51
  host      : a100-st-p4de24xlarge-1029.fair-a100.hpcaas
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2868794)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
