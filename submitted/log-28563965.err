Already on 'yangexp2'
wandb: Currently logged in as: stevenzhou0816100. Use `wandb login --relogin` to force relogin
Traceback (most recent call last):
  File "/private/home/beidic/.conda/envs/griffin/bin/huggingface-cli", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py", line 51, in main
    service.run()
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/commands/user.py", line 98, in run
    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 111, in login
    _login(token, add_to_git_credential=add_to_git_credential, write_permission=write_permission)
  File "/private/home/beidic/.conda/envs/griffin/lib/python3.12/site-packages/huggingface_hub/_login.py", line 307, in _login
    raise ValueError("Invalid token passed!")
ValueError: Invalid token passed!
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:02:10:28,864 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:10:28,864 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:10:28,865 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:10:28,873 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:10:28,904 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:10:28,916 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:10:28,935 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:10:28,939 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:10:35,912 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:10:35,912 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:10:35,912 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:10:35,912 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:10:35,912 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:10:35,912 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:10:35,912 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:10:35,912 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:10:35,913 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:10:35,913 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:10:35,914 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:10:35,914 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:10:35,914 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:10:35,914 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:10:35,914 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:10:35,914 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:10:35,924 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:10:35,924 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:10:35,924 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-04:02:10:35,924 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-04:02:10:35,924 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:10:35,924 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:10:35,924 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:10:35,924 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:10:35,924 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:10:35,924 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:10:35,924 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-04:02:10:35,924 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-04:02:10:35,924 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-04:02:10:35,924 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-04:02:10:35,924 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
2024-06-04:02:10:35,924 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.05}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:35<01:45, 35.23s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:35<01:47, 35.75s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:35<01:47, 35.79s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:36<01:48, 36.07s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:35<01:47, 35.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:36<01:48, 36.15s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:35<01:47, 35.88s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:35<01:47, 35.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:09<01:09, 34.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:10<01:10, 35.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:10<01:10, 35.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:10<01:10, 35.25s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:10<01:10, 35.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:11<01:10, 35.39s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:10<01:10, 35.25s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:10<01:10, 35.24s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:44<00:34, 34.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:45<00:35, 35.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:45<00:35, 35.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:45<00:34, 34.94s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:45<00:35, 35.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:45<00:34, 34.96s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:45<00:34, 34.95s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:45<00:35, 35.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 23.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 28.03s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 24.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 28.16s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 23.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 28.10s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 24.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 28.14s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 24.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 24.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 28.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 28.10s/it]

Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 23.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 28.10s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 23.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:52<00:00, 28.11s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:13:06,251 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:13:06,254 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:13:06,563 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 19%|█▉        | 16/83 [00:00<00:00, 153.53it/s] 39%|███▊      | 32/83 [00:00<00:00, 154.45it/s] 58%|█████▊    | 48/83 [00:00<00:00, 154.62it/s] 77%|███████▋  | 64/83 [00:00<00:00, 150.50it/s] 96%|█████████▋| 80/83 [00:00<00:00, 149.76it/s]100%|██████████| 83/83 [00:00<00:00, 151.17it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:13:08,068 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:13:08,070 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:13:08,260 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 206.88it/s] 51%|█████     | 42/82 [00:00<00:00, 207.36it/s] 77%|███████▋  | 63/82 [00:00<00:00, 207.03it/s]100%|██████████| 82/82 [00:00<00:00, 207.35it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:13:21,138 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:13:21,139 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:13:21,367 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 27%|██▋       | 22/82 [00:00<00:00, 210.74it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:13:21,558 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:13:21,559 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 54%|█████▎    | 44/82 [00:00<00:00, 211.34it/s] 80%|████████  | 66/82 [00:00<00:00, 213.09it/s]2024-06-04:02:13:21,711 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s]100%|██████████| 82/82 [00:00<00:00, 213.02it/s]
 27%|██▋       | 22/83 [00:00<00:00, 215.50it/s] 53%|█████▎    | 44/83 [00:00<00:00, 217.24it/s] 80%|███████▉  | 66/83 [00:00<00:00, 216.93it/s]100%|██████████| 83/83 [00:00<00:00, 216.94it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:13:23,754 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:13:23,755 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:13:23,838 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:13:23,840 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:13:23,950 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:02:13:24,000 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 205.00it/s] 26%|██▌       | 21/82 [00:00<00:00, 207.51it/s] 51%|█████     | 42/82 [00:00<00:00, 205.35it/s] 51%|█████     | 42/82 [00:00<00:00, 208.20it/s] 77%|███████▋  | 63/82 [00:00<00:00, 205.64it/s] 77%|███████▋  | 63/82 [00:00<00:00, 205.89it/s]100%|██████████| 82/82 [00:00<00:00, 205.71it/s]
100%|██████████| 82/82 [00:00<00:00, 207.52it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:13:27,607 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:13:27,683 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:13:27,684 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:13:27,843 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 27%|██▋       | 22/83 [00:00<00:00, 212.32it/s] 53%|█████▎    | 44/83 [00:00<00:00, 215.05it/s] 80%|███████▉  | 66/83 [00:00<00:00, 215.69it/s]100%|██████████| 83/83 [00:00<00:00, 215.77it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:13:30,935 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:13:30,937 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:13:31,180 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 18%|█▊        | 15/83 [00:00<00:00, 146.19it/s] 36%|███▌      | 30/83 [00:00<00:00, 146.37it/s] 54%|█████▍    | 45/83 [00:00<00:00, 146.73it/s] 72%|███████▏  | 60/83 [00:00<00:00, 146.89it/s] 90%|█████████ | 75/83 [00:00<00:00, 146.98it/s]100%|██████████| 83/83 [00:00<00:00, 146.83it/s]
2024-06-04:02:13:36,030 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:13:36,030 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:13:36,030 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:13:36,030 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:13:36,030 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:13:36,030 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:13:36,031 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:13:36,031 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:17<24:30, 17.93s/it]Running generate_until requests:   2%|▏         | 2/83 [00:36<24:57, 18.49s/it]Running generate_until requests:   4%|▎         | 3/83 [00:44<18:17, 13.72s/it]Running generate_until requests:   5%|▍         | 4/83 [00:49<13:35, 10.32s/it]Running generate_until requests:   6%|▌         | 5/83 [00:58<12:40,  9.76s/it]Running generate_until requests:   7%|▋         | 6/83 [01:04<10:45,  8.38s/it]Running generate_until requests:   8%|▊         | 7/83 [01:14<11:14,  8.87s/it]Running generate_until requests:  10%|▉         | 8/83 [01:21<10:29,  8.39s/it]Running generate_until requests:  11%|█         | 9/83 [01:31<10:50,  8.80s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:38<09:54,  8.15s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:44<09:18,  7.76s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:56<10:34,  8.94s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:01<08:57,  7.68s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:09<08:50,  7.68s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:26<12:12, 10.77s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:44<14:16, 12.78s/it]Running generate_until requests:  20%|██        | 17/83 [02:53<12:50, 11.67s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:01<11:19, 10.46s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:08<10:00,  9.39s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:19<10:38, 10.13s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:28<09:52,  9.55s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:34<08:52,  8.73s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:46<09:39,  9.66s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:53<08:39,  8.81s/it]Running generate_until requests:  30%|███       | 25/83 [04:02<08:34,  8.87s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:09<07:49,  8.23s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:20<08:31,  9.13s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:27<07:43,  8.42s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:34<07:20,  8.16s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:45<07:51,  8.89s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:52<07:16,  8.39s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:59<06:42,  7.89s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:04<05:49,  6.98s/it]Running generate_until requests:  41%|████      | 34/83 [05:15<06:49,  8.36s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:22<06:16,  7.85s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:29<05:53,  7.53s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:36<05:46,  7.53s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:45<05:57,  7.94s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:51<05:19,  7.26s/it]Running generate_until requests:  48%|████▊     | 40/83 [05:57<05:00,  7.00s/it]Running generate_until requests:  49%|████▉     | 41/83 [06:03<04:42,  6.73s/it]Running generate_until requests:  51%|█████     | 42/83 [06:10<04:31,  6.63s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:18<04:45,  7.14s/it]Running generate_until requests:  53%|█████▎    | 44/83 [06:24<04:18,  6.62s/it]Running generate_until requests:  54%|█████▍    | 45/83 [06:31<04:17,  6.79s/it]Running generate_until requests:  55%|█████▌    | 46/83 [06:37<04:10,  6.77s/it]Running generate_until requests:  57%|█████▋    | 47/83 [06:44<04:03,  6.78s/it]Running generate_until requests:  58%|█████▊    | 48/83 [06:54<04:27,  7.63s/it]Running generate_until requests:  59%|█████▉    | 49/83 [06:58<03:47,  6.70s/it]Running generate_until requests:  60%|██████    | 50/83 [07:09<04:17,  7.80s/it]Running generate_until requests:  61%|██████▏   | 51/83 [07:18<04:23,  8.24s/it]Running generate_until requests:  63%|██████▎   | 52/83 [07:23<03:45,  7.27s/it]Running generate_until requests:  64%|██████▍   | 53/83 [07:31<03:43,  7.44s/it]Running generate_until requests:  65%|██████▌   | 54/83 [07:36<03:16,  6.79s/it]Running generate_until requests:  66%|██████▋   | 55/83 [07:42<02:59,  6.41s/it]Running generate_until requests:  67%|██████▋   | 56/83 [07:55<03:46,  8.39s/it]Running generate_until requests:  69%|██████▊   | 57/83 [08:09<04:23, 10.15s/it]Running generate_until requests:  70%|██████▉   | 58/83 [08:16<03:51,  9.28s/it]Running generate_until requests:  71%|███████   | 59/83 [08:22<03:15,  8.14s/it]Running generate_until requests:  72%|███████▏  | 60/83 [08:27<02:47,  7.28s/it]Running generate_until requests:  73%|███████▎  | 61/83 [08:35<02:46,  7.57s/it]Running generate_until requests:  75%|███████▍  | 62/83 [08:40<02:20,  6.69s/it]Running generate_until requests:  76%|███████▌  | 63/83 [08:46<02:11,  6.59s/it]Running generate_until requests:  77%|███████▋  | 64/83 [08:55<02:17,  7.23s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:01<02:03,  6.86s/it]Running generate_until requests:  80%|███████▉  | 66/83 [09:07<01:54,  6.72s/it]Running generate_until requests:  81%|████████  | 67/83 [09:11<01:32,  5.80s/it]Running generate_until requests:  82%|████████▏ | 68/83 [09:17<01:30,  6.02s/it]Running generate_until requests:  83%|████████▎ | 69/83 [09:27<01:40,  7.19s/it]Running generate_until requests:  84%|████████▍ | 70/83 [09:37<01:43,  7.95s/it]Running generate_until requests:  86%|████████▌ | 71/83 [09:42<01:25,  7.09s/it]Running generate_until requests:  87%|████████▋ | 72/83 [09:48<01:15,  6.82s/it]Running generate_until requests:  88%|████████▊ | 73/83 [09:55<01:06,  6.66s/it]Running generate_until requests:  89%|████████▉ | 74/83 [10:00<00:57,  6.39s/it]Running generate_until requests:  90%|█████████ | 75/83 [10:11<01:01,  7.74s/it]Running generate_until requests:  92%|█████████▏| 76/83 [10:17<00:50,  7.26s/it]Running generate_until requests:  93%|█████████▎| 77/83 [10:23<00:41,  6.88s/it]Running generate_until requests:  94%|█████████▍| 78/83 [10:32<00:36,  7.27s/it]Running generate_until requests:  95%|█████████▌| 79/83 [10:41<00:31,  7.93s/it]Running generate_until requests:  96%|█████████▋| 80/83 [10:46<00:21,  7.05s/it]Running generate_until requests:  98%|█████████▊| 81/83 [10:53<00:13,  6.88s/it]Running generate_until requests:  99%|█████████▉| 82/83 [10:58<00:06,  6.42s/it]Running generate_until requests: 100%|██████████| 83/83 [11:02<00:00,  5.78s/it]Running generate_until requests: 100%|██████████| 83/83 [11:02<00:00,  7.98s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:02:26:38,156 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:38,156 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:38,160 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:38,230 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:38,262 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:38,424 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:38,437 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:38,556 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:26:42,547 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:42,548 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:26:42,551 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:42,551 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:02:26:43,419 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:43,420 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:26:43,424 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:43,424 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:26:43,778 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:43,779 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:26:43,783 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:43,783 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:02:26:44,209 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:44,211 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:26:44,218 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:44,218 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:26:44,684 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:44,686 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:26:44,692 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:44,692 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:26:44,928 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:44,930 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:26:44,934 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:44,934 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:02:26:44,952 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:44,953 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:26:44,957 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:44,957 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
2024-06-04:02:26:45,040 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:26:45,041 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:26:45,045 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:26:45,045 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.1}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.91s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.93s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.54s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.03s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.37s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.60s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.93s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.56s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.28s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.55s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.84s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.54s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:01,  1.95s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.63s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.00s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.54s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.50s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.10s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.08s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.03s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.05s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:27:29,089 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:29,091 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:29,259 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 207.68it/s] 51%|█████     | 42/82 [00:00<00:00, 205.84it/s] 77%|███████▋  | 63/82 [00:00<00:00, 206.78it/s]100%|██████████| 82/82 [00:00<00:00, 207.86it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:27:30,062 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:30,064 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:30,221 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 209.42it/s] 52%|█████▏    | 43/83 [00:00<00:00, 210.62it/s] 78%|███████▊  | 65/83 [00:00<00:00, 210.76it/s]100%|██████████| 83/83 [00:00<00:00, 210.64it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:27:43,955 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:43,956 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:44,143 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 27%|██▋       | 22/82 [00:00<00:00, 216.46it/s] 54%|█████▎    | 44/82 [00:00<00:00, 217.29it/s] 80%|████████  | 66/82 [00:00<00:00, 217.25it/s]100%|██████████| 82/82 [00:00<00:00, 217.00it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:27:44,716 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:44,718 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:44,873 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 27%|██▋       | 22/82 [00:00<00:00, 219.09it/s] 55%|█████▍    | 45/82 [00:00<00:00, 219.77it/s] 83%|████████▎ | 68/82 [00:00<00:00, 220.02it/s]100%|██████████| 82/82 [00:00<00:00, 219.92it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:27:55,640 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:27:55,759 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:55,761 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:27:55,845 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:27:55,845 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:55,847 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:55,847 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:27:55,969 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:27:56,065 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:27:56,067 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 25%|██▌       | 21/83 [00:00<00:00, 196.75it/s]2024-06-04:02:27:56,118 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:02:27:56,205 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 49%|████▉     | 41/83 [00:00<00:00, 159.83it/s] 17%|█▋        | 14/82 [00:00<00:00, 138.06it/s] 18%|█▊        | 15/83 [00:00<00:00, 141.25it/s] 35%|███▌      | 29/82 [00:00<00:00, 140.53it/s]2024-06-04:02:27:56,351 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
 70%|██████▉   | 58/83 [00:00<00:00, 149.50it/s]  0%|          | 0/83 [00:00<?, ?it/s] 36%|███▌      | 30/83 [00:00<00:00, 141.89it/s] 54%|█████▎    | 44/82 [00:00<00:00, 141.25it/s] 89%|████████▉ | 74/83 [00:00<00:00, 146.14it/s] 18%|█▊        | 15/83 [00:00<00:00, 141.25it/s]100%|██████████| 83/83 [00:00<00:00, 150.43it/s]
 54%|█████▍    | 45/83 [00:00<00:00, 141.48it/s] 72%|███████▏  | 59/82 [00:00<00:00, 141.14it/s] 37%|███▋      | 31/83 [00:00<00:00, 150.65it/s] 72%|███████▏  | 60/83 [00:00<00:00, 141.01it/s] 90%|█████████ | 74/82 [00:00<00:00, 140.90it/s] 60%|██████    | 50/83 [00:00<00:00, 165.65it/s]100%|██████████| 82/82 [00:00<00:00, 140.83it/s]
 90%|█████████ | 75/83 [00:00<00:00, 143.78it/s] 81%|████████  | 67/83 [00:00<00:00, 152.65it/s]100%|██████████| 83/83 [00:00<00:00, 141.92it/s]
100%|██████████| 83/83 [00:00<00:00, 124.88it/s]100%|██████████| 83/83 [00:00<00:00, 136.36it/s]
2024-06-04:02:28:00,526 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:28:00,526 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:28:00,526 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:28:00,526 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:28:00,526 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:28:00,526 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:28:00,526 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:28:00,526 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:17<24:03, 17.60s/it]Running generate_until requests:   2%|▏         | 2/83 [00:36<24:34, 18.21s/it]Running generate_until requests:   4%|▎         | 3/83 [00:44<18:16, 13.70s/it]Running generate_until requests:   5%|▍         | 4/83 [00:49<13:38, 10.36s/it]Running generate_until requests:   6%|▌         | 5/83 [00:58<12:38,  9.72s/it]Running generate_until requests:   7%|▋         | 6/83 [01:03<10:38,  8.29s/it]Running generate_until requests:   8%|▊         | 7/83 [01:13<11:08,  8.80s/it]Running generate_until requests:  10%|▉         | 8/83 [01:21<10:28,  8.39s/it]Running generate_until requests:  11%|█         | 9/83 [01:31<10:54,  8.84s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:37<10:00,  8.23s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:44<09:25,  7.86s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:56<10:37,  8.98s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:02<09:20,  8.00s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:11<09:35,  8.34s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:23<10:45,  9.49s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:42<13:41, 12.27s/it]Running generate_until requests:  20%|██        | 17/83 [02:51<12:22, 11.24s/it]Running generate_until requests:  22%|██▏       | 18/83 [02:58<11:02, 10.20s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:05<09:49,  9.22s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:17<10:28,  9.98s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:25<09:47,  9.48s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:32<08:51,  8.72s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:44<09:35,  9.59s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:51<08:49,  8.97s/it]Running generate_until requests:  30%|███       | 25/83 [04:02<09:02,  9.35s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:08<08:06,  8.53s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:20<08:42,  9.34s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:26<07:50,  8.55s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:34<07:26,  8.27s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:45<07:57,  9.01s/it]Running generate_until requests:  37%|███▋      | 31/83 [04:52<07:20,  8.47s/it]Running generate_until requests:  39%|███▊      | 32/83 [04:59<06:56,  8.17s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:04<06:00,  7.21s/it]Running generate_until requests:  41%|████      | 34/83 [05:16<06:55,  8.48s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:22<06:22,  7.97s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:29<05:57,  7.61s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:37<05:53,  7.68s/it]Running generate_until requests:  46%|████▌     | 38/83 [05:52<07:16,  9.71s/it]Running generate_until requests:  47%|████▋     | 39/83 [05:57<06:09,  8.41s/it]Running generate_until requests:  48%|████▊     | 40/83 [06:04<05:50,  8.16s/it]Running generate_until requests:  49%|████▉     | 41/83 [06:11<05:16,  7.53s/it]Running generate_until requests:  51%|█████     | 42/83 [06:17<04:55,  7.20s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:25<05:01,  7.53s/it]Running generate_until requests:  53%|█████▎    | 44/83 [06:31<04:29,  6.91s/it]Running generate_until requests:  54%|█████▍    | 45/83 [06:38<04:25,  6.98s/it]Running generate_until requests:  55%|█████▌    | 46/83 [06:44<04:13,  6.86s/it]Running generate_until requests:  57%|█████▋    | 47/83 [06:51<04:05,  6.81s/it]Running generate_until requests:  58%|█████▊    | 48/83 [07:01<04:31,  7.77s/it]Running generate_until requests:  59%|█████▉    | 49/83 [07:06<03:49,  6.76s/it]Running generate_until requests:  60%|██████    | 50/83 [07:16<04:19,  7.87s/it]Running generate_until requests:  61%|██████▏   | 51/83 [07:25<04:25,  8.28s/it]Running generate_until requests:  63%|██████▎   | 52/83 [07:30<03:45,  7.27s/it]Running generate_until requests:  64%|██████▍   | 53/83 [07:38<03:42,  7.41s/it]Running generate_until requests:  65%|██████▌   | 54/83 [07:43<03:18,  6.85s/it]Running generate_until requests:  66%|██████▋   | 55/83 [07:50<03:05,  6.63s/it]Running generate_until requests:  67%|██████▋   | 56/83 [08:05<04:11,  9.31s/it]Running generate_until requests:  69%|██████▊   | 57/83 [08:19<04:41, 10.82s/it]Running generate_until requests:  70%|██████▉   | 58/83 [08:27<04:03,  9.74s/it]Running generate_until requests:  71%|███████   | 59/83 [08:32<03:23,  8.48s/it]Running generate_until requests:  72%|███████▏  | 60/83 [08:38<02:53,  7.56s/it]Running generate_until requests:  73%|███████▎  | 61/83 [08:46<02:52,  7.86s/it]Running generate_until requests:  75%|███████▍  | 62/83 [08:53<02:36,  7.46s/it]Running generate_until requests:  76%|███████▌  | 63/83 [08:59<02:24,  7.20s/it]Running generate_until requests:  77%|███████▋  | 64/83 [09:08<02:26,  7.69s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:14<02:09,  7.19s/it]Running generate_until requests:  80%|███████▉  | 66/83 [09:20<01:52,  6.64s/it]Running generate_until requests:  81%|████████  | 67/83 [09:23<01:32,  5.76s/it]Running generate_until requests:  82%|████████▏ | 68/83 [09:30<01:30,  6.02s/it]Running generate_until requests:  83%|████████▎ | 69/83 [09:40<01:43,  7.37s/it]Running generate_until requests:  84%|████████▍ | 70/83 [09:50<01:44,  8.03s/it]Running generate_until requests:  86%|████████▌ | 71/83 [09:55<01:25,  7.12s/it]Running generate_until requests:  87%|████████▋ | 72/83 [10:01<01:15,  6.82s/it]Running generate_until requests:  88%|████████▊ | 73/83 [10:07<01:06,  6.66s/it]Running generate_until requests:  89%|████████▉ | 74/83 [10:13<00:57,  6.38s/it]Running generate_until requests:  90%|█████████ | 75/83 [10:25<01:04,  8.11s/it]Running generate_until requests:  92%|█████████▏| 76/83 [10:30<00:50,  7.20s/it]Running generate_until requests:  93%|█████████▎| 77/83 [10:38<00:43,  7.28s/it]Running generate_until requests:  94%|█████████▍| 78/83 [10:46<00:37,  7.57s/it]Running generate_until requests:  95%|█████████▌| 79/83 [10:55<00:32,  8.11s/it]Running generate_until requests:  96%|█████████▋| 80/83 [11:00<00:21,  7.13s/it]Running generate_until requests:  98%|█████████▊| 81/83 [11:07<00:13,  6.88s/it]Running generate_until requests:  99%|█████████▉| 82/83 [11:12<00:06,  6.46s/it]Running generate_until requests: 100%|██████████| 83/83 [11:16<00:00,  5.77s/it]Running generate_until requests: 100%|██████████| 83/83 [11:16<00:00,  8.15s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:02:41:12,959 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:41:12,960 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:41:12,961 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:41:13,016 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:41:13,018 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:41:13,020 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:41:13,052 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:41:13,064 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:41:17,317 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:41:17,318 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:41:17,322 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:41:17,322 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-04:02:41:17,346 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:41:17,347 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:41:17,351 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:41:17,352 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-04:02:41:18,059 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:41:18,060 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:41:18,065 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:41:18,065 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-04:02:41:18,479 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:41:18,480 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:41:18,484 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:41:18,484 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:41:19,600 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:41:19,602 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:41:19,609 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:41:19,609 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:41:19,656 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:41:19,657 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:41:19,661 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:41:19,661 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-04:02:41:19,685 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:41:19,686 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:41:19,689 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:41:19,690 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:41:19,690 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
2024-06-04:02:41:19,690 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:41:19,694 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:41:19,694 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.15}
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.07s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.50s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.10s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.95s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.70s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.53s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.52s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.91s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.58s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.56s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.53s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.08s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.03s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.11s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.16s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.10s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:42:02,718 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:42:02,720 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:42:02,858 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:42:02,930 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:42:02,932 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:42:02,937 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 206.67it/s] 51%|█████     | 42/83 [00:00<00:00, 207.61it/s]2024-06-04:02:42:03,237 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
 76%|███████▌  | 63/83 [00:00<00:00, 207.87it/s]  0%|          | 0/83 [00:00<?, ?it/s]100%|██████████| 83/83 [00:00<00:00, 207.88it/s]
 18%|█▊        | 15/83 [00:00<00:00, 146.22it/s] 37%|███▋      | 31/83 [00:00<00:00, 150.99it/s] 57%|█████▋    | 47/83 [00:00<00:00, 152.45it/s] 76%|███████▌  | 63/83 [00:00<00:00, 153.14it/s] 95%|█████████▌| 79/83 [00:00<00:00, 153.57it/s]100%|██████████| 83/83 [00:00<00:00, 152.66it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:42:20,717 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:42:20,719 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:42:20,900 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 24%|██▍       | 20/82 [00:00<00:00, 191.27it/s] 49%|████▉     | 40/82 [00:00<00:00, 191.42it/s] 73%|███████▎  | 60/82 [00:00<00:00, 191.52it/s] 98%|█████████▊| 80/82 [00:00<00:00, 191.71it/s]100%|██████████| 82/82 [00:00<00:00, 191.56it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:42:28,518 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:42:28,520 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:42:28,760 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 18%|█▊        | 15/82 [00:00<00:00, 144.69it/s] 37%|███▋      | 30/82 [00:00<00:00, 145.15it/s] 55%|█████▍    | 45/82 [00:00<00:00, 145.13it/s] 73%|███████▎  | 60/82 [00:00<00:00, 145.34it/s] 91%|█████████▏| 75/82 [00:00<00:00, 145.32it/s]100%|██████████| 82/82 [00:00<00:00, 145.21it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:42:31,168 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:42:31,168 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:42:31,171 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:42:31,171 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:42:31,185 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:42:31,187 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:42:31,240 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:42:31,242 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:42:31,422 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:02:42:31,465 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:02:42:31,526 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 17%|█▋        | 14/82 [00:00<00:00, 134.95it/s]2024-06-04:02:42:31,550 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 136.90it/s] 17%|█▋        | 14/82 [00:00<00:00, 138.32it/s] 35%|███▌      | 29/82 [00:00<00:00, 138.82it/s] 18%|█▊        | 15/83 [00:00<00:00, 141.37it/s] 34%|███▎      | 28/83 [00:00<00:00, 137.91it/s] 34%|███▍      | 28/82 [00:00<00:00, 138.33it/s] 54%|█████▎    | 44/82 [00:00<00:00, 140.11it/s] 36%|███▌      | 30/83 [00:00<00:00, 141.64it/s] 51%|█████     | 42/83 [00:00<00:00, 138.12it/s] 51%|█████     | 42/82 [00:00<00:00, 136.92it/s] 72%|███████▏  | 59/82 [00:00<00:00, 140.78it/s] 54%|█████▍    | 45/83 [00:00<00:00, 141.80it/s] 69%|██████▊   | 57/83 [00:00<00:00, 139.40it/s] 70%|██████▉   | 57/82 [00:00<00:00, 138.52it/s] 90%|█████████ | 74/82 [00:00<00:00, 141.29it/s] 72%|███████▏  | 60/83 [00:00<00:00, 141.78it/s] 87%|████████▋ | 72/83 [00:00<00:00, 140.05it/s]100%|██████████| 82/82 [00:00<00:00, 140.13it/s]
 88%|████████▊ | 72/82 [00:00<00:00, 139.32it/s]100%|██████████| 83/83 [00:00<00:00, 139.57it/s]
 93%|█████████▎| 77/83 [00:00<00:00, 150.43it/s]100%|██████████| 82/82 [00:00<00:00, 141.51it/s]
100%|██████████| 83/83 [00:00<00:00, 145.69it/s]
2024-06-04:02:42:35,478 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:42:35,478 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:42:35,478 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:42:35,478 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:42:35,478 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:42:35,478 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:42:35,478 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:02:42:35,479 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:19<27:14, 19.93s/it]Running generate_until requests:   2%|▏         | 2/83 [00:42<29:06, 21.57s/it]Running generate_until requests:   4%|▎         | 3/83 [00:51<21:16, 15.95s/it]Running generate_until requests:   5%|▍         | 4/83 [00:57<15:40, 11.90s/it]Running generate_until requests:   6%|▌         | 5/83 [01:07<14:30, 11.16s/it]Running generate_until requests:   7%|▋         | 6/83 [01:13<12:13,  9.52s/it]Running generate_until requests:   8%|▊         | 7/83 [01:24<12:45, 10.07s/it]Running generate_until requests:  10%|▉         | 8/83 [01:33<11:57,  9.57s/it]Running generate_until requests:  11%|█         | 9/83 [01:44<12:22, 10.03s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:52<11:17,  9.28s/it]Running generate_until requests:  13%|█▎        | 11/83 [02:03<11:57,  9.97s/it]Running generate_until requests:  14%|█▍        | 12/83 [02:17<13:00, 11.00s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:23<11:12,  9.61s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:33<11:16,  9.80s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:57<16:00, 14.13s/it]Running generate_until requests:  19%|█▉        | 16/83 [03:18<17:49, 15.96s/it]Running generate_until requests:  20%|██        | 17/83 [03:28<15:40, 14.25s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:37<13:38, 12.59s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:45<12:14, 11.47s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:59<12:43, 12.12s/it]Running generate_until requests:  25%|██▌       | 21/83 [04:08<11:33, 11.19s/it]Running generate_until requests:  27%|██▋       | 22/83 [04:16<10:20, 10.18s/it]Running generate_until requests:  28%|██▊       | 23/83 [04:25<09:51,  9.86s/it]Running generate_until requests:  29%|██▉       | 24/83 [04:34<09:19,  9.48s/it]Running generate_until requests:  30%|███       | 25/83 [04:45<09:48, 10.14s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:53<08:53,  9.36s/it]Running generate_until requests:  33%|███▎      | 27/83 [05:06<09:42, 10.40s/it]Running generate_until requests:  34%|███▎      | 28/83 [05:13<08:47,  9.58s/it]Running generate_until requests:  35%|███▍      | 29/83 [05:22<08:25,  9.36s/it]Running generate_until requests:  36%|███▌      | 30/83 [05:34<09:01, 10.22s/it]Running generate_until requests:  37%|███▋      | 31/83 [05:43<08:29,  9.79s/it]Running generate_until requests:  39%|███▊      | 32/83 [05:52<08:08,  9.58s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:58<06:56,  8.34s/it]Running generate_until requests:  41%|████      | 34/83 [06:11<07:59,  9.79s/it]Running generate_until requests:  42%|████▏     | 35/83 [06:19<07:18,  9.14s/it]Running generate_until requests:  43%|████▎     | 36/83 [06:26<06:46,  8.64s/it]Running generate_until requests:  45%|████▍     | 37/83 [06:36<06:54,  9.01s/it]Running generate_until requests:  46%|████▌     | 38/83 [06:49<07:44, 10.32s/it]Running generate_until requests:  47%|████▋     | 39/83 [06:55<06:39,  9.09s/it]Running generate_until requests:  48%|████▊     | 40/83 [07:04<06:23,  8.92s/it]Running generate_until requests:  49%|████▉     | 41/83 [07:11<05:54,  8.45s/it]Running generate_until requests:  51%|█████     | 42/83 [07:21<06:06,  8.94s/it]Running generate_until requests:  52%|█████▏    | 43/83 [07:31<06:06,  9.16s/it]Running generate_until requests:  53%|█████▎    | 44/83 [07:37<05:20,  8.23s/it]Running generate_until requests:  54%|█████▍    | 45/83 [07:45<05:11,  8.19s/it]Running generate_until requests:  55%|█████▌    | 46/83 [07:55<05:23,  8.74s/it]Running generate_until requests:  57%|█████▋    | 47/83 [08:03<05:02,  8.39s/it]Running generate_until requests:  58%|█████▊    | 48/83 [08:14<05:24,  9.26s/it]Running generate_until requests:  59%|█████▉    | 49/83 [08:19<04:30,  7.96s/it]Running generate_until requests:  60%|██████    | 50/83 [08:31<05:02,  9.18s/it]Running generate_until requests:  61%|██████▏   | 51/83 [08:43<05:19,  9.99s/it]Running generate_until requests:  63%|██████▎   | 52/83 [08:49<04:29,  8.69s/it]Running generate_until requests:  64%|██████▍   | 53/83 [08:57<04:22,  8.75s/it]Running generate_until requests:  65%|██████▌   | 54/83 [09:04<03:50,  7.95s/it]Running generate_until requests:  66%|██████▋   | 55/83 [09:10<03:33,  7.63s/it]Running generate_until requests:  67%|██████▋   | 56/83 [09:30<05:05, 11.31s/it]Running generate_until requests:  69%|██████▊   | 57/83 [09:43<05:02, 11.62s/it]Running generate_until requests:  70%|██████▉   | 58/83 [09:51<04:25, 10.61s/it]Running generate_until requests:  71%|███████   | 59/83 [09:57<03:43,  9.32s/it]Running generate_until requests:  72%|███████▏  | 60/83 [10:04<03:13,  8.42s/it]Running generate_until requests:  73%|███████▎  | 61/83 [10:13<03:14,  8.84s/it]Running generate_until requests:  75%|███████▍  | 62/83 [10:21<02:55,  8.36s/it]Running generate_until requests:  76%|███████▌  | 63/83 [10:28<02:42,  8.14s/it]Running generate_until requests:  77%|███████▋  | 64/83 [10:43<03:13, 10.18s/it]Running generate_until requests:  78%|███████▊  | 65/83 [10:50<02:44,  9.15s/it]Running generate_until requests:  80%|███████▉  | 66/83 [10:56<02:20,  8.24s/it]Running generate_until requests:  81%|████████  | 67/83 [11:00<01:51,  6.98s/it]Running generate_until requests:  82%|████████▏ | 68/83 [11:08<01:47,  7.15s/it]Running generate_until requests:  83%|████████▎ | 69/83 [11:20<02:00,  8.60s/it]Running generate_until requests:  84%|████████▍ | 70/83 [11:29<01:56,  8.95s/it]Running generate_until requests:  86%|████████▌ | 71/83 [11:35<01:35,  8.00s/it]Running generate_until requests:  87%|████████▋ | 72/83 [11:42<01:24,  7.69s/it]Running generate_until requests:  88%|████████▊ | 73/83 [11:49<01:15,  7.52s/it]Running generate_until requests:  89%|████████▉ | 74/83 [11:55<01:03,  7.10s/it]Running generate_until requests:  90%|█████████ | 75/83 [12:10<01:13,  9.20s/it]Running generate_until requests:  92%|█████████▏| 76/83 [12:15<00:57,  8.16s/it]Running generate_until requests:  93%|█████████▎| 77/83 [12:23<00:48,  8.12s/it]Running generate_until requests:  94%|█████████▍| 78/83 [12:33<00:42,  8.50s/it]Running generate_until requests:  95%|█████████▌| 79/83 [12:43<00:36,  9.17s/it]Running generate_until requests:  96%|█████████▋| 80/83 [12:49<00:24,  8.08s/it]Running generate_until requests:  98%|█████████▊| 81/83 [12:56<00:15,  7.82s/it]Running generate_until requests:  99%|█████████▉| 82/83 [13:02<00:07,  7.29s/it]Running generate_until requests: 100%|██████████| 83/83 [13:07<00:00,  6.51s/it]Running generate_until requests: 100%|██████████| 83/83 [13:07<00:00,  9.49s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:02:58:37,344 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:58:37,345 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:58:37,346 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:58:37,347 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:58:37,356 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:58:37,365 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:58:37,441 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:58:37,530 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:02:58:41,865 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:58:41,867 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:58:41,872 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:58:41,872 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-04:02:58:41,888 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:58:41,889 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:58:41,892 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:58:41,892 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-04:02:58:42,606 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:58:42,608 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:58:42,614 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:58:42,614 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:58:43,328 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:58:43,329 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:58:43,334 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:58:43,334 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:02:58:44,040 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:58:44,041 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:58:44,047 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:58:44,047 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-04:02:58:44,120 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:58:44,121 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:58:44,125 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:58:44,125 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-04:02:58:44,204 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:58:44,205 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:58:44,209 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:58:44,209 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
2024-06-04:02:58:44,229 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:02:58:44,230 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:02:58:44,234 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:02:58:44,234 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.2}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.02s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.92s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.38s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.97s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.87s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.61s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.46s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.57s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:05,  2.50s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.91s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.57s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.51s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.63s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.07s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.04s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.52s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.07s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.06s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.12s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:59:27,285 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:59:27,287 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:59:27,397 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:59:27,399 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:59:27,486 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:02:59:27,556 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 205.62it/s] 26%|██▌       | 21/82 [00:00<00:00, 208.70it/s] 51%|█████     | 42/82 [00:00<00:00, 206.09it/s] 52%|█████▏    | 43/82 [00:00<00:00, 209.64it/s] 77%|███████▋  | 63/82 [00:00<00:00, 206.29it/s] 78%|███████▊  | 64/82 [00:00<00:00, 209.79it/s]100%|██████████| 82/82 [00:00<00:00, 206.44it/s]
100%|██████████| 82/82 [00:00<00:00, 209.76it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:59:43,393 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:59:43,395 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:59:43,585 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 27%|██▋       | 22/82 [00:00<00:00, 214.04it/s] 54%|█████▎    | 44/82 [00:00<00:00, 215.09it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 80%|████████  | 66/82 [00:00<00:00, 214.70it/s]100%|██████████| 82/82 [00:00<00:00, 215.10it/s]
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:59:43,982 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:59:43,984 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:59:44,144 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 27%|██▋       | 22/82 [00:00<00:00, 218.81it/s] 54%|█████▎    | 44/82 [00:00<00:00, 219.37it/s] 80%|████████  | 66/82 [00:00<00:00, 219.50it/s]100%|██████████| 82/82 [00:00<00:00, 219.49it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:02:59:55,141 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:59:55,212 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:59:55,214 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:59:55,290 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:59:55,291 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:59:55,304 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:59:55,306 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:02:59:55,328 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:59:55,330 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:02:59:55,449 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 137.92it/s]2024-06-04:02:59:55,582 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
2024-06-04:02:59:55,600 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s]  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:02:59:55,636 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 35%|███▍      | 29/83 [00:00<00:00, 139.77it/s] 18%|█▊        | 15/83 [00:00<00:00, 141.09it/s] 18%|█▊        | 15/83 [00:00<00:00, 141.08it/s] 18%|█▊        | 15/83 [00:00<00:00, 142.44it/s] 53%|█████▎    | 44/83 [00:00<00:00, 140.62it/s] 36%|███▌      | 30/83 [00:00<00:00, 141.94it/s] 36%|███▌      | 30/83 [00:00<00:00, 140.84it/s] 36%|███▌      | 30/83 [00:00<00:00, 142.50it/s] 71%|███████   | 59/83 [00:00<00:00, 140.41it/s] 54%|█████▍    | 45/83 [00:00<00:00, 142.08it/s] 54%|█████▍    | 45/83 [00:00<00:00, 140.44it/s] 54%|█████▍    | 45/83 [00:00<00:00, 142.34it/s] 89%|████████▉ | 74/83 [00:00<00:00, 140.34it/s] 72%|███████▏  | 60/83 [00:00<00:00, 142.38it/s] 72%|███████▏  | 60/83 [00:00<00:00, 140.69it/s]100%|██████████| 83/83 [00:00<00:00, 140.32it/s]
 72%|███████▏  | 60/83 [00:00<00:00, 141.69it/s] 90%|█████████ | 75/83 [00:00<00:00, 141.54it/s] 92%|█████████▏| 76/83 [00:00<00:00, 144.68it/s] 90%|█████████ | 75/83 [00:00<00:00, 141.38it/s]100%|██████████| 83/83 [00:00<00:00, 141.70it/s]
100%|██████████| 83/83 [00:00<00:00, 142.16it/s]
100%|██████████| 83/83 [00:00<00:00, 143.39it/s]
2024-06-04:03:00:00,361 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:00:00,362 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:00:00,362 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:00:00,362 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:00:00,362 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:00:00,362 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:03:00:00,363 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:00:00,364 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   1%|          | 1/83 [00:17<24:08, 17.66s/it]Running generate_until requests:   2%|▏         | 2/83 [00:33<22:19, 16.54s/it]Running generate_until requests:   4%|▎         | 3/83 [00:41<16:54, 12.68s/it]Running generate_until requests:   5%|▍         | 4/83 [00:46<12:50,  9.76s/it]Running generate_until requests:   6%|▌         | 5/83 [00:55<12:07,  9.32s/it]Running generate_until requests:   7%|▋         | 6/83 [01:00<10:18,  8.03s/it]Running generate_until requests:   8%|▊         | 7/83 [01:10<10:58,  8.66s/it]Running generate_until requests:  10%|▉         | 8/83 [01:18<10:20,  8.27s/it]Running generate_until requests:  11%|█         | 9/83 [01:27<10:42,  8.68s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:34<09:52,  8.12s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:44<10:29,  8.74s/it]Running generate_until requests:  14%|█▍        | 12/83 [01:56<11:21,  9.60s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:02<09:47,  8.40s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:11<09:54,  8.62s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:34<14:54, 13.15s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:50<15:34, 13.94s/it]Running generate_until requests:  20%|██        | 17/83 [02:59<13:41, 12.45s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:07<11:56, 11.02s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:15<11:00, 10.32s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:27<11:17, 10.76s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:35<10:20, 10.00s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:42<09:13,  9.07s/it]Running generate_until requests:  28%|██▊       | 23/83 [03:51<08:52,  8.88s/it]Running generate_until requests:  29%|██▉       | 24/83 [03:58<08:19,  8.47s/it]Running generate_until requests:  30%|███       | 25/83 [04:08<08:39,  8.95s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:15<07:49,  8.23s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:26<08:27,  9.07s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:33<07:39,  8.36s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:40<07:16,  8.09s/it]Running generate_until requests:  36%|███▌      | 30/83 [04:53<08:24,  9.52s/it]Running generate_until requests:  37%|███▋      | 31/83 [05:01<07:47,  9.00s/it]Running generate_until requests:  39%|███▊      | 32/83 [05:09<07:23,  8.70s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:15<06:40,  8.00s/it]Running generate_until requests:  41%|████      | 34/83 [05:27<07:23,  9.04s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:36<07:18,  9.14s/it]Running generate_until requests:  43%|████▎     | 36/83 [05:44<07:00,  8.94s/it]Running generate_until requests:  45%|████▍     | 37/83 [05:53<06:51,  8.95s/it]Running generate_until requests:  46%|████▌     | 38/83 [06:09<08:10, 10.89s/it]Running generate_until requests:  47%|████▋     | 39/83 [06:14<06:49,  9.30s/it]Running generate_until requests:  48%|████▊     | 40/83 [06:22<06:16,  8.75s/it]Running generate_until requests:  49%|████▉     | 41/83 [06:28<05:37,  8.04s/it]Running generate_until requests:  51%|█████     | 42/83 [06:37<05:39,  8.29s/it]Running generate_until requests:  52%|█████▏    | 43/83 [06:47<05:50,  8.76s/it]Running generate_until requests:  53%|█████▎    | 44/83 [06:52<05:01,  7.72s/it]Running generate_until requests:  54%|█████▍    | 45/83 [06:59<04:46,  7.55s/it]Running generate_until requests:  55%|█████▌    | 46/83 [07:07<04:42,  7.63s/it]Running generate_until requests:  57%|█████▋    | 47/83 [07:14<04:24,  7.34s/it]Running generate_until requests:  58%|█████▊    | 48/83 [07:24<04:44,  8.13s/it]Running generate_until requests:  59%|█████▉    | 49/83 [07:28<03:57,  6.99s/it]Running generate_until requests:  60%|██████    | 50/83 [07:39<04:23,  8.00s/it]Running generate_until requests:  61%|██████▏   | 51/83 [07:50<04:46,  8.95s/it]Running generate_until requests:  63%|██████▎   | 52/83 [07:59<04:36,  8.93s/it]Running generate_until requests:  64%|██████▍   | 53/83 [08:06<04:16,  8.55s/it]Running generate_until requests:  65%|██████▌   | 54/83 [08:16<04:15,  8.81s/it]Running generate_until requests:  66%|██████▋   | 55/83 [08:22<03:43,  7.97s/it]Running generate_until requests:  67%|██████▋   | 56/83 [08:38<04:39, 10.35s/it]Running generate_until requests:  69%|██████▊   | 57/83 [08:51<04:52, 11.27s/it]Running generate_until requests:  70%|██████▉   | 58/83 [08:58<04:11, 10.06s/it]Running generate_until requests:  71%|███████   | 59/83 [09:04<03:28,  8.68s/it]Running generate_until requests:  72%|███████▏  | 60/83 [09:09<02:58,  7.77s/it]Running generate_until requests:  73%|███████▎  | 61/83 [09:19<03:00,  8.20s/it]Running generate_until requests:  75%|███████▍  | 62/83 [09:25<02:42,  7.72s/it]Running generate_until requests:  76%|███████▌  | 63/83 [09:32<02:27,  7.38s/it]Running generate_until requests:  77%|███████▋  | 64/83 [09:41<02:28,  7.83s/it]Running generate_until requests:  78%|███████▊  | 65/83 [09:47<02:10,  7.23s/it]Running generate_until requests:  80%|███████▉  | 66/83 [09:52<01:54,  6.71s/it]Running generate_until requests:  81%|████████  | 67/83 [09:57<01:40,  6.29s/it]Running generate_until requests:  82%|████████▏ | 68/83 [10:04<01:35,  6.39s/it]Running generate_until requests:  83%|████████▎ | 69/83 [10:20<02:11,  9.40s/it]Running generate_until requests:  84%|████████▍ | 70/83 [10:29<01:59,  9.16s/it]Running generate_until requests:  86%|████████▌ | 71/83 [10:34<01:34,  7.90s/it]Running generate_until requests:  87%|████████▋ | 72/83 [10:43<01:30,  8.20s/it]Running generate_until requests:  88%|████████▊ | 73/83 [10:49<01:15,  7.59s/it]Running generate_until requests:  89%|████████▉ | 74/83 [10:56<01:06,  7.44s/it]Running generate_until requests:  90%|█████████ | 75/83 [11:08<01:10,  8.83s/it]Running generate_until requests:  92%|█████████▏| 76/83 [11:21<01:10, 10.10s/it]Running generate_until requests:  93%|█████████▎| 77/83 [11:28<00:55,  9.19s/it]Running generate_until requests:  94%|█████████▍| 78/83 [11:34<00:40,  8.16s/it]Running generate_until requests:  95%|█████████▌| 79/83 [11:43<00:34,  8.53s/it]Running generate_until requests:  96%|█████████▋| 80/83 [11:49<00:22,  7.48s/it]Running generate_until requests:  98%|█████████▊| 81/83 [11:57<00:15,  7.80s/it]Running generate_until requests:  99%|█████████▉| 82/83 [12:02<00:07,  7.07s/it]Running generate_until requests: 100%|██████████| 83/83 [12:07<00:00,  6.22s/it]Running generate_until requests: 100%|██████████| 83/83 [12:07<00:00,  8.76s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:03:15:56,840 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:15:56,840 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:15:56,854 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:15:56,890 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:15:56,911 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:15:56,938 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:15:56,974 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:15:57,060 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:16:01,326 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:16:01,327 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:16:01,330 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:16:01,330 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-04:03:16:01,600 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:16:01,601 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:16:01,605 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:16:01,605 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-04:03:16:01,849 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:16:01,851 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:16:01,855 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:16:01,855 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-04:03:16:01,895 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:16:01,896 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:16:01,900 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:16:01,900 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]2024-06-04:03:16:03,542 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:16:03,544 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:16:03,548 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:16:03,548 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-04:03:16:03,563 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:16:03,564 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:16:03,567 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:16:03,567 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-04:03:16:03,571 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:16:03,573 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:16:03,577 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:16:03,577 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
2024-06-04:03:16:03,641 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:16:03,642 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:16:03,645 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:16:03,646 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.3}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.98s/it]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.63s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.84s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.53s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.81s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.55s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.50s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.59s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.58s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.08s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.11s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.20s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:16:46,561 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:16:46,563 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:16:46,727 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:16:46,744 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:16:46,745 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 26%|██▌       | 21/82 [00:00<00:00, 207.79it/s]2024-06-04:03:16:46,940 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
 51%|█████     | 42/82 [00:00<00:00, 207.81it/s]  0%|          | 0/82 [00:00<?, ?it/s] 77%|███████▋  | 63/82 [00:00<00:00, 208.04it/s] 26%|██▌       | 21/82 [00:00<00:00, 203.82it/s]100%|██████████| 82/82 [00:00<00:00, 208.23it/s]
 51%|█████     | 42/82 [00:00<00:00, 204.14it/s] 77%|███████▋  | 63/82 [00:00<00:00, 203.89it/s]100%|██████████| 82/82 [00:00<00:00, 203.95it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:17:02,962 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:17:02,964 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:17:03,054 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:17:03,055 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:17:03,142 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:03:17:03,214 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s] 25%|██▌       | 21/83 [00:00<00:00, 203.50it/s] 27%|██▋       | 22/82 [00:00<00:00, 212.52it/s] 52%|█████▏    | 43/83 [00:00<00:00, 209.73it/s] 54%|█████▎    | 44/82 [00:00<00:00, 215.67it/s] 78%|███████▊  | 65/83 [00:00<00:00, 212.82it/s] 80%|████████  | 66/82 [00:00<00:00, 217.33it/s]100%|██████████| 83/83 [00:00<00:00, 212.96it/s]
100%|██████████| 82/82 [00:00<00:00, 217.36it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:17:14,827 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:17:14,903 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:17:14,904 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:17:14,926 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:17:14,927 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:17:14,975 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:17:14,977 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:17:14,978 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:17:14,979 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:17:15,203 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]2024-06-04:03:17:15,237 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:03:17:15,279 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
2024-06-04:03:17:15,283 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s]  0%|          | 0/83 [00:00<?, ?it/s] 17%|█▋        | 14/83 [00:00<00:00, 136.60it/s] 17%|█▋        | 14/82 [00:00<00:00, 139.75it/s] 18%|█▊        | 15/83 [00:00<00:00, 140.76it/s] 18%|█▊        | 15/83 [00:00<00:00, 140.59it/s] 35%|███▍      | 29/83 [00:00<00:00, 139.51it/s] 35%|███▌      | 29/82 [00:00<00:00, 141.07it/s] 36%|███▌      | 30/83 [00:00<00:00, 141.13it/s] 36%|███▌      | 30/83 [00:00<00:00, 141.07it/s] 53%|█████▎    | 44/83 [00:00<00:00, 142.97it/s] 56%|█████▌    | 46/82 [00:00<00:00, 152.15it/s] 54%|█████▍    | 45/83 [00:00<00:00, 141.24it/s] 54%|█████▍    | 45/83 [00:00<00:00, 141.18it/s] 80%|███████▉  | 66/83 [00:00<00:00, 170.59it/s] 83%|████████▎ | 68/82 [00:00<00:00, 177.53it/s]100%|██████████| 83/83 [00:00<00:00, 168.53it/s]
 72%|███████▏  | 60/83 [00:00<00:00, 141.68it/s] 72%|███████▏  | 60/83 [00:00<00:00, 141.54it/s]100%|██████████| 82/82 [00:00<00:00, 172.36it/s]
 90%|█████████ | 75/83 [00:00<00:00, 141.70it/s] 90%|█████████ | 75/83 [00:00<00:00, 141.45it/s]100%|██████████| 83/83 [00:00<00:00, 141.59it/s]
100%|██████████| 83/83 [00:00<00:00, 141.46it/s]
2024-06-04:03:17:19,512 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:17:19,512 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:17:19,512 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:17:19,512 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:17:19,512 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:17:19,512 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:17:19,512 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:17:19,512 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:16<22:17, 16.31s/it]Running generate_until requests:   2%|▏         | 2/83 [00:32<21:36, 16.00s/it]Running generate_until requests:   4%|▎         | 3/83 [00:40<16:41, 12.51s/it]Running generate_until requests:   5%|▍         | 4/83 [00:45<12:42,  9.65s/it]Running generate_until requests:   6%|▌         | 5/83 [00:54<12:00,  9.24s/it]Running generate_until requests:   7%|▋         | 6/83 [01:01<10:51,  8.46s/it]Running generate_until requests:   8%|▊         | 7/83 [01:17<13:49, 10.91s/it]Running generate_until requests:  10%|▉         | 8/83 [01:24<12:15,  9.81s/it]Running generate_until requests:  11%|█         | 9/83 [01:34<12:00,  9.74s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:41<10:45,  8.85s/it]Running generate_until requests:  13%|█▎        | 11/83 [01:52<11:26,  9.53s/it]Running generate_until requests:  14%|█▍        | 12/83 [02:04<12:17, 10.39s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:09<10:21,  8.88s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:19<10:31,  9.16s/it]Running generate_until requests:  18%|█▊        | 15/83 [02:39<13:59, 12.35s/it]Running generate_until requests:  19%|█▉        | 16/83 [02:53<14:30, 12.99s/it]Running generate_until requests:  20%|██        | 17/83 [03:04<13:35, 12.35s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:12<11:55, 11.00s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:22<11:28, 10.75s/it]Running generate_until requests:  24%|██▍       | 20/83 [03:36<12:22, 11.79s/it]Running generate_until requests:  25%|██▌       | 21/83 [03:44<10:47, 10.45s/it]Running generate_until requests:  27%|██▋       | 22/83 [03:51<09:34,  9.42s/it]Running generate_until requests:  28%|██▊       | 23/83 [04:02<09:48,  9.80s/it]Running generate_until requests:  29%|██▉       | 24/83 [04:09<08:56,  9.09s/it]Running generate_until requests:  30%|███       | 25/83 [04:19<09:01,  9.34s/it]Running generate_until requests:  31%|███▏      | 26/83 [04:28<08:42,  9.17s/it]Running generate_until requests:  33%|███▎      | 27/83 [04:38<08:45,  9.39s/it]Running generate_until requests:  34%|███▎      | 28/83 [04:45<07:56,  8.66s/it]Running generate_until requests:  35%|███▍      | 29/83 [04:59<09:19, 10.36s/it]Running generate_until requests:  36%|███▌      | 30/83 [05:12<09:48, 11.11s/it]Running generate_until requests:  37%|███▋      | 31/83 [05:20<08:46, 10.13s/it]Running generate_until requests:  39%|███▊      | 32/83 [05:28<08:13,  9.67s/it]Running generate_until requests:  40%|███▉      | 33/83 [05:36<07:38,  9.17s/it]Running generate_until requests:  41%|████      | 34/83 [05:48<08:01,  9.83s/it]Running generate_until requests:  42%|████▏     | 35/83 [05:56<07:27,  9.32s/it]Running generate_until requests:  43%|████▎     | 36/83 [06:05<07:25,  9.47s/it]Running generate_until requests:  45%|████▍     | 37/83 [06:14<07:06,  9.28s/it]Running generate_until requests:  46%|████▌     | 38/83 [06:28<07:53, 10.52s/it]Running generate_until requests:  47%|████▋     | 39/83 [06:33<06:38,  9.05s/it]Running generate_until requests:  48%|████▊     | 40/83 [06:47<07:22, 10.30s/it]Running generate_until requests:  49%|████▉     | 41/83 [06:53<06:25,  9.17s/it]Running generate_until requests:  51%|█████     | 42/83 [07:04<06:34,  9.62s/it]Running generate_until requests:  52%|█████▏    | 43/83 [07:13<06:23,  9.59s/it]Running generate_until requests:  53%|█████▎    | 44/83 [07:19<05:23,  8.29s/it]Running generate_until requests:  54%|█████▍    | 45/83 [07:26<05:02,  7.96s/it]Running generate_until requests:  55%|█████▌    | 46/83 [07:36<05:15,  8.52s/it]Running generate_until requests:  57%|█████▋    | 47/83 [07:42<04:46,  7.95s/it]Running generate_until requests:  58%|█████▊    | 48/83 [07:52<04:57,  8.50s/it]Running generate_until requests:  59%|█████▉    | 49/83 [07:56<04:07,  7.27s/it]Running generate_until requests:  60%|██████    | 50/83 [08:07<04:38,  8.43s/it]Running generate_until requests:  61%|██████▏   | 51/83 [08:21<05:18,  9.94s/it]Running generate_until requests:  63%|██████▎   | 52/83 [08:31<05:07,  9.91s/it]Running generate_until requests:  64%|██████▍   | 53/83 [08:41<05:01, 10.06s/it]Running generate_until requests:  65%|██████▌   | 54/83 [08:49<04:30,  9.33s/it]Running generate_until requests:  66%|██████▋   | 55/83 [08:59<04:32,  9.72s/it]Running generate_until requests:  67%|██████▋   | 56/83 [09:19<05:41, 12.65s/it]Running generate_until requests:  69%|██████▊   | 57/83 [09:38<06:22, 14.70s/it]Running generate_until requests:  70%|██████▉   | 58/83 [09:47<05:21, 12.88s/it]Running generate_until requests:  71%|███████   | 59/83 [09:52<04:15, 10.64s/it]Running generate_until requests:  72%|███████▏  | 60/83 [09:59<03:39,  9.54s/it]Running generate_until requests:  73%|███████▎  | 61/83 [10:09<03:32,  9.65s/it]Running generate_until requests:  75%|███████▍  | 62/83 [10:15<02:58,  8.52s/it]Running generate_until requests:  76%|███████▌  | 63/83 [10:22<02:38,  7.94s/it]Running generate_until requests:  77%|███████▋  | 64/83 [10:30<02:33,  8.10s/it]Running generate_until requests:  78%|███████▊  | 65/83 [10:36<02:13,  7.42s/it]Running generate_until requests:  80%|███████▉  | 66/83 [10:42<01:56,  6.84s/it]Running generate_until requests:  81%|████████  | 67/83 [10:47<01:42,  6.41s/it]Running generate_until requests:  82%|████████▏ | 68/83 [10:54<01:36,  6.45s/it]Running generate_until requests:  83%|████████▎ | 69/83 [11:08<02:01,  8.70s/it]Running generate_until requests:  84%|████████▍ | 70/83 [11:19<02:05,  9.64s/it]Running generate_until requests:  86%|████████▌ | 71/83 [11:24<01:39,  8.27s/it]Running generate_until requests:  87%|████████▋ | 72/83 [11:35<01:39,  9.08s/it]Running generate_until requests:  88%|████████▊ | 73/83 [11:42<01:22,  8.20s/it]Running generate_until requests:  89%|████████▉ | 74/83 [11:50<01:13,  8.15s/it]Running generate_until requests:  90%|█████████ | 75/83 [12:02<01:15,  9.40s/it]Running generate_until requests:  92%|█████████▏| 76/83 [12:15<01:13, 10.49s/it]Running generate_until requests:  93%|█████████▎| 77/83 [12:22<00:57,  9.53s/it]Running generate_until requests:  94%|█████████▍| 78/83 [12:29<00:42,  8.56s/it]Running generate_until requests:  95%|█████████▌| 79/83 [12:41<00:38,  9.69s/it]Running generate_until requests:  96%|█████████▋| 80/83 [12:48<00:26,  8.93s/it]Running generate_until requests:  98%|█████████▊| 81/83 [12:58<00:18,  9.15s/it]Running generate_until requests:  99%|█████████▉| 82/83 [13:03<00:08,  8.02s/it]Running generate_until requests: 100%|██████████| 83/83 [13:07<00:00,  6.86s/it]Running generate_until requests: 100%|██████████| 83/83 [13:07<00:00,  9.49s/it]
The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2024-06-04:03:33:56,549 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:33:56,549 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:33:56,599 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:33:56,622 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:33:56,650 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:33:56,691 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:33:56,711 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:33:56,717 INFO     [main.py:288] Verbosity set to INFO
2024-06-04:03:34:01,818 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:01,820 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:01,826 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:01,826 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-04:03:34:02,016 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:02,018 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:02,022 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:02,022 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-04:03:34:02,677 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:02,678 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:02,681 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:02,681 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-04:03:34:02,702 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:02,703 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:02,710 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:02,710 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-04:03:34:02,842 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:02,842 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:02,845 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:02,845 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-04:03:34:02,855 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:02,856 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:02,859 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:02,859 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-04:03:34:02,875 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:02,876 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:02,880 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:02,880 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
2024-06-04:03:34:02,933 WARNING  [main.py:303]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2024-06-04:03:34:02,934 INFO     [main.py:378] Selected Tasks: ['gsm8k']
2024-06-04:03:34:02,937 INFO     [xevaluator.py:137] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2024-06-04:03:34:02,937 INFO     [xevaluator.py:184] Initializing xhf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'cats': True, 'spr': 0.5, 'check': True, 'kernel_size': 16, 'thr': 0.4}
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.85s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.87s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.68s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.55s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.88s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.63s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.49s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.50s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:08,  2.77s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.53s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.54s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.50s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.11s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.06s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:08<00:02,  2.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.02s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.01s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.16s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:34:45,062 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:34:45,064 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:34:45,136 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:34:45,138 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:34:45,241 INFO     [task.py:395] Building contexts for gsm8k on rank 7...
  0%|          | 0/82 [00:00<?, ?it/s]2024-06-04:03:34:45,290 INFO     [task.py:395] Building contexts for gsm8k on rank 1...
  0%|          | 0/83 [00:00<?, ?it/s] 27%|██▋       | 22/82 [00:00<00:00, 211.80it/s] 27%|██▋       | 22/83 [00:00<00:00, 212.21it/s] 54%|█████▎    | 44/82 [00:00<00:00, 212.41it/s] 53%|█████▎    | 44/83 [00:00<00:00, 212.45it/s] 80%|████████  | 66/82 [00:00<00:00, 213.03it/s] 80%|███████▉  | 66/83 [00:00<00:00, 212.99it/s]100%|██████████| 82/82 [00:00<00:00, 213.06it/s]
100%|██████████| 83/83 [00:00<00:00, 213.41it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:35:03,446 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:35:03,449 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:35:03,632 INFO     [task.py:395] Building contexts for gsm8k on rank 3...
  0%|          | 0/83 [00:00<?, ?it/s] 27%|██▋       | 22/83 [00:00<00:00, 213.45it/s] 53%|█████▎    | 44/83 [00:00<00:00, 216.13it/s] 80%|███████▉  | 66/83 [00:00<00:00, 215.17it/s]100%|██████████| 83/83 [00:00<00:00, 215.03it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:35:04,354 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:35:04,356 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:35:04,553 INFO     [task.py:395] Building contexts for gsm8k on rank 5...
  0%|          | 0/82 [00:00<?, ?it/s] 26%|██▌       | 21/82 [00:00<00:00, 204.61it/s] 51%|█████     | 42/82 [00:00<00:00, 204.87it/s] 77%|███████▋  | 63/82 [00:00<00:00, 205.06it/s]100%|██████████| 82/82 [00:00<00:00, 205.26it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:35:11,435 INFO     [xhuggingface.py:314] Using 8 devices with data parallelism
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:35:11,506 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:35:11,508 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-04:03:35:11,816 INFO     [task.py:395] Building contexts for gsm8k on rank 0...
  0%|          | 0/83 [00:00<?, ?it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:35:11,855 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:35:11,857 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 18%|█▊        | 15/83 [00:00<00:00, 147.70it/s]2024-06-04:03:35:12,015 INFO     [task.py:395] Building contexts for gsm8k on rank 6...
  0%|          | 0/82 [00:00<?, ?it/s] 36%|███▌      | 30/83 [00:00<00:00, 147.81it/s] 26%|██▌       | 21/82 [00:00<00:00, 207.34it/s] 54%|█████▍    | 45/83 [00:00<00:00, 146.73it/s] 51%|█████     | 42/82 [00:00<00:00, 207.76it/s] 72%|███████▏  | 60/83 [00:00<00:00, 147.18it/s] 77%|███████▋  | 63/82 [00:00<00:00, 208.01it/s] 90%|█████████ | 75/83 [00:00<00:00, 147.32it/s]100%|██████████| 83/83 [00:00<00:00, 147.36it/s]
100%|██████████| 82/82 [00:00<00:00, 207.67it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:35:14,568 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:35:14,570 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:35:14,819 INFO     [task.py:395] Building contexts for gsm8k on rank 4...
  0%|          | 0/82 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
 18%|█▊        | 15/82 [00:00<00:00, 146.29it/s]Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
2024-06-04:03:35:14,941 WARNING  [load.py:1631] Using the latest cached version of the dataset since gsm8k couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
2024-06-04:03:35:14,943 WARNING  [cache.py:95] Found the latest cached dataset configuration 'main' at /private/home/beidic/.cache/huggingface/datasets/gsm8k/main/0.0.0/e53f048856ff4f594e959d75785d2c2d37b678ee (last modified on Thu May 16 07:01:08 2024).
 37%|███▋      | 30/82 [00:00<00:00, 146.93it/s] 55%|█████▍    | 45/82 [00:00<00:00, 146.66it/s]2024-06-04:03:35:15,165 INFO     [task.py:395] Building contexts for gsm8k on rank 2...
  0%|          | 0/83 [00:00<?, ?it/s] 73%|███████▎  | 60/82 [00:00<00:00, 146.47it/s] 22%|██▏       | 18/83 [00:00<00:00, 175.85it/s] 91%|█████████▏| 75/82 [00:00<00:00, 146.20it/s] 43%|████▎     | 36/83 [00:00<00:00, 176.34it/s]100%|██████████| 82/82 [00:00<00:00, 146.32it/s]
 65%|██████▌   | 54/83 [00:00<00:00, 157.49it/s] 84%|████████▍ | 70/83 [00:00<00:00, 145.10it/s]100%|██████████| 83/83 [00:00<00:00, 155.06it/s]
2024-06-04:03:35:19,314 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:35:19,314 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:35:19,314 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:35:19,314 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:35:19,314 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:35:19,314 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:35:19,314 INFO     [xevaluator.py:395] Running generate_until requests
2024-06-04:03:35:19,315 INFO     [xevaluator.py:395] Running generate_until requests
Running generate_until requests:   0%|          | 0/83 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/83 [00:12<17:22, 12.71s/it]Running generate_until requests:   2%|▏         | 2/83 [00:30<20:57, 15.52s/it]Running generate_until requests:   4%|▎         | 3/83 [00:38<16:08, 12.11s/it]Running generate_until requests:   5%|▍         | 4/83 [00:43<12:22,  9.40s/it]Running generate_until requests:   6%|▌         | 5/83 [00:51<11:33,  8.89s/it]Running generate_until requests:   7%|▋         | 6/83 [00:59<11:05,  8.64s/it]Running generate_until requests:   8%|▊         | 7/83 [01:19<15:36, 12.32s/it]Running generate_until requests:  10%|▉         | 8/83 [01:27<13:41, 10.96s/it]Running generate_until requests:  11%|█         | 9/83 [01:38<13:36, 11.03s/it]Running generate_until requests:  12%|█▏        | 10/83 [01:46<12:18, 10.11s/it]Running generate_until requests:  13%|█▎        | 11/83 [02:00<13:22, 11.15s/it]Running generate_until requests:  14%|█▍        | 12/83 [02:12<13:37, 11.52s/it]Running generate_until requests:  16%|█▌        | 13/83 [02:22<12:50, 11.00s/it]Running generate_until requests:  17%|█▋        | 14/83 [02:32<12:17, 10.69s/it]Running generate_until requests:  18%|█▊        | 15/83 [03:02<18:52, 16.66s/it]Running generate_until requests:  19%|█▉        | 16/83 [03:20<18:57, 16.97s/it]Running generate_until requests:  20%|██        | 17/83 [03:32<17:04, 15.52s/it]Running generate_until requests:  22%|██▏       | 18/83 [03:41<14:40, 13.54s/it]Running generate_until requests:  23%|██▎       | 19/83 [03:51<13:13, 12.40s/it]Running generate_until requests:  24%|██▍       | 20/83 [04:08<14:20, 13.66s/it]Running generate_until requests:  25%|██▌       | 21/83 [04:18<13:06, 12.69s/it]Running generate_until requests:  27%|██▋       | 22/83 [04:25<11:06, 10.92s/it]Running generate_until requests:  28%|██▊       | 23/83 [04:35<10:37, 10.62s/it]Running generate_until requests:  29%|██▉       | 24/83 [04:42<09:33,  9.72s/it]Running generate_until requests:  30%|███       | 25/83 [04:57<10:51, 11.23s/it]Running generate_until requests:  31%|███▏      | 26/83 [05:06<09:57, 10.48s/it]Running generate_until requests:  33%|███▎      | 27/83 [05:24<11:51, 12.70s/it]Running generate_until requests:  34%|███▎      | 28/83 [05:31<10:10, 11.10s/it]Running generate_until requests:  35%|███▍      | 29/83 [05:46<11:04, 12.30s/it]Running generate_until requests:  36%|███▌      | 30/83 [05:58<10:51, 12.30s/it]Running generate_until requests:  37%|███▋      | 31/83 [06:08<10:01, 11.56s/it]Running generate_until requests:  39%|███▊      | 32/83 [06:19<09:38, 11.34s/it]Running generate_until requests:  40%|███▉      | 33/83 [06:28<08:45, 10.52s/it]Running generate_until requests:  41%|████      | 34/83 [06:46<10:25, 12.77s/it]Running generate_until requests:  42%|████▏     | 35/83 [06:54<09:08, 11.43s/it]Running generate_until requests:  43%|████▎     | 36/83 [07:08<09:26, 12.04s/it]Running generate_until requests:  45%|████▍     | 37/83 [07:17<08:34, 11.18s/it]Running generate_until requests:  46%|████▌     | 38/83 [07:31<09:00, 12.01s/it]Running generate_until requests:  47%|████▋     | 39/83 [07:39<07:56, 10.83s/it]Running generate_until requests:  48%|████▊     | 40/83 [07:52<08:16, 11.55s/it]Running generate_until requests:  49%|████▉     | 41/83 [08:00<07:14, 10.36s/it]Running generate_until requests:  51%|█████     | 42/83 [08:13<07:37, 11.15s/it]Running generate_until requests:  52%|█████▏    | 43/83 [08:23<07:16, 10.91s/it]Running generate_until requests:  53%|█████▎    | 44/83 [08:28<06:02,  9.28s/it]Running generate_until requests:  54%|█████▍    | 45/83 [08:36<05:30,  8.69s/it]Running generate_until requests:  55%|█████▌    | 46/83 [08:46<05:34,  9.05s/it]Running generate_until requests:  57%|█████▋    | 47/83 [08:52<05:01,  8.37s/it]Running generate_until requests:  58%|█████▊    | 48/83 [09:02<05:10,  8.87s/it]Running generate_until requests:  59%|█████▉    | 49/83 [09:09<04:35,  8.10s/it]Running generate_until requests:  60%|██████    | 50/83 [09:22<05:20,  9.72s/it]Running generate_until requests:  61%|██████▏   | 51/83 [09:35<05:39, 10.62s/it]Running generate_until requests:  63%|██████▎   | 52/83 [09:46<05:30, 10.67s/it]Running generate_until requests:  64%|██████▍   | 53/83 [09:56<05:21, 10.70s/it]Running generate_until requests:  65%|██████▌   | 54/83 [10:06<05:00, 10.35s/it]Running generate_until requests:  66%|██████▋   | 55/83 [10:15<04:41, 10.06s/it]Running generate_until requests:  67%|██████▋   | 56/83 [10:42<06:42, 14.91s/it]Running generate_until requests:  69%|██████▊   | 57/83 [11:02<07:13, 16.67s/it]Running generate_until requests:  70%|██████▉   | 58/83 [11:13<06:15, 15.00s/it]Running generate_until requests:  71%|███████   | 59/83 [11:19<04:52, 12.20s/it]Running generate_until requests:  72%|███████▏  | 60/83 [11:26<04:05, 10.68s/it]Running generate_until requests:  73%|███████▎  | 61/83 [11:37<03:56, 10.76s/it]Running generate_until requests:  75%|███████▍  | 62/83 [11:46<03:32, 10.12s/it]Running generate_until requests:  76%|███████▌  | 63/83 [11:52<03:01,  9.08s/it]Running generate_until requests:  77%|███████▋  | 64/83 [12:05<03:11, 10.08s/it]Running generate_until requests:  78%|███████▊  | 65/83 [12:11<02:39,  8.84s/it]Running generate_until requests:  80%|███████▉  | 66/83 [12:16<02:13,  7.85s/it]Running generate_until requests:  81%|████████  | 67/83 [12:22<01:53,  7.08s/it]Running generate_until requests:  82%|████████▏ | 68/83 [12:28<01:44,  6.95s/it]Running generate_until requests:  83%|████████▎ | 69/83 [12:48<02:30, 10.77s/it]Running generate_until requests:  84%|████████▍ | 70/83 [13:01<02:29, 11.49s/it]Running generate_until requests:  86%|████████▌ | 71/83 [13:06<01:54,  9.54s/it]Running generate_until requests:  87%|████████▋ | 72/83 [13:19<01:57, 10.65s/it]Running generate_until requests:  88%|████████▊ | 73/83 [13:28<01:40, 10.09s/it]Running generate_until requests:  89%|████████▉ | 74/83 [13:38<01:29,  9.89s/it]Running generate_until requests:  90%|█████████ | 75/83 [13:52<01:29, 11.24s/it]Running generate_until requests:  92%|█████████▏| 76/83 [14:08<01:28, 12.70s/it]Running generate_until requests:  93%|█████████▎| 77/83 [14:15<01:06, 11.05s/it]Running generate_until requests:  94%|█████████▍| 78/83 [14:25<00:53, 10.68s/it]Running generate_until requests:  95%|█████████▌| 79/83 [14:41<00:48, 12.13s/it]Running generate_until requests:  96%|█████████▋| 80/83 [14:48<00:32, 10.70s/it]Running generate_until requests:  98%|█████████▊| 81/83 [14:59<00:21, 10.70s/it]Running generate_until requests:  99%|█████████▉| 82/83 [15:05<00:09,  9.29s/it]Running generate_until requests: 100%|██████████| 83/83 [15:09<00:00,  7.75s/it]Running generate_until requests: 100%|██████████| 83/83 [15:09<00:00, 10.96s/it]
